{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b4fbb5b",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f3b62c",
   "metadata": {},
   "source": [
    "# Task 2: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d636646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I747530/Documents/NLP_project/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# --- Optional: nice progress bars ---\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "# --- NLP helpers (NLTK) ---\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK resources (safe to run multiple times)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"omw-1.4\", quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2278de5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 50000\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 1) Load CSV\n",
    "# -------------------------\n",
    "# Update this path to where you saved the Kaggle CSV\n",
    "CSV_PATH = \"imdb.csv\"  # e.g. \"/mnt/data/IMDB Dataset.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert {\"review\", \"sentiment\"}.issubset(df.columns), \"CSV must have columns: review, sentiment\"\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f8f3190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values:\n",
      " review       0\n",
      "sentiment    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicates: 418\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 2) Basic dataset checks\n",
    "# -------------------------\n",
    "print(\"\\nMissing values:\\n\", df[[\"review\", \"sentiment\"]].isna().sum())\n",
    "print(\"\\nDuplicates:\", df.duplicated(subset=[\"review\", \"sentiment\"]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7671675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 3) Cleaning utilities\n",
    "# -------------------------\n",
    "\n",
    "_HTML_RE = re.compile(r\"<[^>]+>\")                      # any HTML tag\n",
    "_WHITESPACE_RE = re.compile(r\"\\s+\")\n",
    "_NON_WORD_RE = re.compile(r\"[^a-z0-9\\s']\")             # keep letters, digits, spaces, and apostrophes\n",
    "_APOSTROPHE_RE = re.compile(r\"[’`´]\")                  # normalize fancy apostrophes\n",
    "\n",
    "def remove_html(text: str) -> str:\n",
    "    \"\"\"Remove HTML tags like <br />.\"\"\"\n",
    "    return _HTML_RE.sub(\" \", text)\n",
    "\n",
    "def normalize_whitespace(text: str) -> str:\n",
    "    \"\"\"Collapse whitespace and strip edges.\"\"\"\n",
    "    return _WHITESPACE_RE.sub(\" \", text).strip()\n",
    "\n",
    "def basic_clean(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Light cleaning shared across pipelines:\n",
    "    - remove HTML tags\n",
    "    - normalize apostrophes\n",
    "    - normalize whitespace\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\" if text is None else str(text)\n",
    "    text = remove_html(text)\n",
    "    text = _APOSTROPHE_RE.sub(\"'\", text)\n",
    "    text = normalize_whitespace(text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def classical_clean_for_tokenization(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Classical pipeline cleaning:\n",
    "    - basic_clean\n",
    "    - lowercase\n",
    "    - remove punctuation/symbols (keep letters, digits, spaces)\n",
    "    \"\"\"\n",
    "    text = basic_clean(text)\n",
    "    text = _NON_WORD_RE.sub(\" \", text)\n",
    "    text = normalize_whitespace(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b182ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4) Classical preprocessing pipeline\n",
    "#    (for n-grams, LSTM/GRU)\n",
    "# -------------------------\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "NEGATION_KEEP = {\"no\", \"not\", \"nor\", \"never\", \"none\", \"n't\"}  # keep negation cues\n",
    "\n",
    "# Remove stopwords but KEEP negations\n",
    "CLASSICAL_STOPWORDS = STOPWORDS - (NEGATION_KEEP & STOPWORDS)\n",
    "\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "def transformer_preprocess(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns cleaned text for transformer models:\n",
    "    - HTML removed\n",
    "    - normalize whitespace\n",
    "    - normalize apostrophes\n",
    "    - lowercase\n",
    "    \"\"\"\n",
    "    return basic_clean(text)\n",
    "\n",
    "\n",
    "def classical_preprocess(text: str,\n",
    "                         remove_stopwords: bool = False,\n",
    "                         lemmatize: bool = True,\n",
    "                         min_token_len: int | None = 2,\n",
    "                         keep_numbers: bool = True) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of tokens:\n",
    "    - HTML removed, lowercase, punctuation removed\n",
    "    - tokenized\n",
    "    - optional stopword removal (negations retained)\n",
    "    - optional lemmatization\n",
    "    - optional filtering by token length\n",
    "    \"\"\"\n",
    "    # Upstream cleaning (HTML, casing, punctuation) before tokenizing\n",
    "    text = classical_clean_for_tokenization(text)\n",
    "\n",
    "    # Tokenize (keep NLTK's tokenizer behavior)\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Bind locals for speed inside loop\n",
    "    neg_keep = NEGATION_KEEP\n",
    "    stop_set = CLASSICAL_STOPWORDS if remove_stopwords else None\n",
    "    min_len = min_token_len\n",
    "    keep_nums = keep_numbers\n",
    "    if lemmatize:\n",
    "        lem = LEMMATIZER.lemmatize\n",
    "    \n",
    "    result: list[str] = []\n",
    "    for t in tokens:\n",
    "        # Optionally drop purely numeric tokens\n",
    "        if not keep_nums and t.isdigit():\n",
    "            continue\n",
    "\n",
    "        # Filter short tokens (e.g., 'a', 'i') BUT keep negations\n",
    "        if min_len is not None and len(t) < min_len and t not in neg_keep:\n",
    "            continue\n",
    "\n",
    "        # Stopword removal (negations kept by design)\n",
    "        if stop_set is not None and t in stop_set:\n",
    "            continue\n",
    "\n",
    "        # skip orphan apostrophes\n",
    "        if t == \"'\" or t == \"''\":\n",
    "            continue\n",
    "\n",
    "        # Lemmatization (simple noun lemma by default; good enough for this project)\n",
    "        if lemmatize:\n",
    "            t = lem(t)\n",
    "\n",
    "        result.append(t)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea8c29a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:01<00:00, 28263.81it/s]\n",
      "100%|██████████| 50000/50000 [00:33<00:00, 1471.43it/s]\n",
      "100%|██████████| 50000/50000 [00:17<00:00, 2833.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created columns: review_clean_transformer, tokens_ngram, tokens_neural\n",
      "  sentiment                           review_clean_transformer  \\\n",
      "0  positive  one of the other reviewers has mentioned that ...   \n",
      "1  positive  a wonderful little production. the filming tec...   \n",
      "\n",
      "                                        tokens_ngram  \\\n",
      "0  [one, of, the, other, reviewer, ha, mentioned,...   \n",
      "1  [wonderful, little, production, the, filming, ...   \n",
      "\n",
      "                                       tokens_neural  \n",
      "0  [one, of, the, other, reviewers, has, mentione...  \n",
      "1  [a, wonderful, little, production, the, filmin...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 6) Apply pipelines\n",
    "# -------------------------\n",
    "\n",
    "apply_fn = df[\"review\"].progress_apply\n",
    "\n",
    "df[\"review_clean_transformer\"] = apply_fn(transformer_preprocess)\n",
    "df[\"tokens_ngram\"] = df[\"review\"].progress_apply(\n",
    "    lambda t: classical_preprocess(\n",
    "        t,\n",
    "        remove_stopwords=False,\n",
    "        lemmatize=True,\n",
    "        min_token_len=2,\n",
    "        keep_numbers=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "df[\"tokens_neural\"] = df[\"review\"].progress_apply(\n",
    "    lambda t: classical_preprocess(\n",
    "        t,\n",
    "        remove_stopwords=False,\n",
    "        lemmatize=False,   \n",
    "        min_token_len=1,   \n",
    "        keep_numbers=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nCreated columns: review_clean_transformer, tokens_ngram, tokens_neural\")\n",
    "print(df[[\"sentiment\", \"review_clean_transformer\", \"tokens_ngram\", \"tokens_neural\"]].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8386a13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Example 1 | sentiment=positive\n",
      "\n",
      "RAW:\n",
      " I really liked this Summerslam due to the look of the arena, the curtains and just the look overall was interesting to me for some reason. Anyways, this could have been one of the best Summerslam's ever if the WWF didn't have Lex Luger in the main event against Yokozuna, now for it's time it was ok to have a huge fat man vs a strong man but I'm glad times have changed. It was a terrible main event just like every match Luger is in is terrible. Other matches on the card were Razor Ramon vs Ted Dibiase, Steiner Brothers vs Heavenly Bodies, Shawn Michaels vs Curt Hening, this was the event where  ...\n",
      "\n",
      "TRANSFORMER (minimal cleaned):\n",
      " i really liked this summerslam due to the look of the arena, the curtains and just the look overall was interesting to me for some reason. anyways, this could have been one of the best summerslam's ever if the wwf didn't have lex luger in the main event against yokozuna, now for it's time it was ok to have a huge fat man vs a strong man but i'm glad times have changed. it was a terrible main event just like every match luger is in is terrible. other matches on the card were razor ramon vs ted dibiase, steiner brothers vs heavenly bodies, shawn michaels vs curt hening, this was the event where  ...\n",
      "\n",
      "NGRAM TOKENS (first 80):\n",
      " ['really', 'liked', 'this', 'summerslam', 'due', 'to', 'the', 'look', 'of', 'the', 'arena', 'the', 'curtain', 'and', 'just', 'the', 'look', 'overall', 'wa', 'interesting', 'to', 'me', 'for', 'some', 'reason', 'anyways', 'this', 'could', 'have', 'been', 'one', 'of', 'the', 'best', 'summerslam', \"'s\", 'ever', 'if', 'the', 'wwf', 'did', \"n't\", 'have', 'lex', 'luger', 'in', 'the', 'main', 'event', 'against', 'yokozuna', 'now', 'for', 'it', \"'s\", 'time', 'it', 'wa', 'ok', 'to', 'have', 'huge', 'fat', 'man', 'v', 'strong', 'man', 'but', \"'m\", 'glad', 'time', 'have', 'changed', 'it', 'wa', 'terrible', 'main', 'event', 'just', 'like']\n",
      "\n",
      "RNN LM TOKENS (first 80):\n",
      " ['i', 'really', 'liked', 'this', 'summerslam', 'due', 'to', 'the', 'look', 'of', 'the', 'arena', 'the', 'curtains', 'and', 'just', 'the', 'look', 'overall', 'was', 'interesting', 'to', 'me', 'for', 'some', 'reason', 'anyways', 'this', 'could', 'have', 'been', 'one', 'of', 'the', 'best', 'summerslam', \"'s\", 'ever', 'if', 'the', 'wwf', 'did', \"n't\", 'have', 'lex', 'luger', 'in', 'the', 'main', 'event', 'against', 'yokozuna', 'now', 'for', 'it', \"'s\", 'time', 'it', 'was', 'ok', 'to', 'have', 'a', 'huge', 'fat', 'man', 'vs', 'a', 'strong', 'man', 'but', 'i', \"'m\", 'glad', 'times', 'have', 'changed', 'it', 'was', 'a']\n",
      "Total tokens (ngram): 197\n",
      "Total tokens (rnn lm): 208\n",
      "==========================================================================================\n",
      "Example 2 | sentiment=positive\n",
      "\n",
      "RAW:\n",
      " Not many television shows appeal to quite as many different kinds of fans like Farscape does...I know youngsters and 30/40+ years old;fans both Male and Female in as many different countries as you can think of that just adore this T.V miniseries. It has elements that can be found in almost every other show on T.V, character driven drama that could be from an Australian soap opera; yet in the same episode it has science fact & fiction that would give even the hardiest \"Trekkie\" a run for his money in the brainbender stakes! Wormhole theory, Time Travel in true equational form...Magnificent. It ...\n",
      "\n",
      "TRANSFORMER (minimal cleaned):\n",
      " not many television shows appeal to quite as many different kinds of fans like farscape does...i know youngsters and 30/40+ years old;fans both male and female in as many different countries as you can think of that just adore this t.v miniseries. it has elements that can be found in almost every other show on t.v, character driven drama that could be from an australian soap opera; yet in the same episode it has science fact & fiction that would give even the hardiest \"trekkie\" a run for his money in the brainbender stakes! wormhole theory, time travel in true equational form...magnificent. it ...\n",
      "\n",
      "NGRAM TOKENS (first 80):\n",
      " ['not', 'many', 'television', 'show', 'appeal', 'to', 'quite', 'a', 'many', 'different', 'kind', 'of', 'fan', 'like', 'farscape', 'doe', 'know', 'youngster', 'and', '30', '40', 'year', 'old', 'fan', 'both', 'male', 'and', 'female', 'in', 'a', 'many', 'different', 'country', 'a', 'you', 'can', 'think', 'of', 'that', 'just', 'adore', 'this', 'miniseries', 'it', 'ha', 'element', 'that', 'can', 'be', 'found', 'in', 'almost', 'every', 'other', 'show', 'on', 'character', 'driven', 'drama', 'that', 'could', 'be', 'from', 'an', 'australian', 'soap', 'opera', 'yet', 'in', 'the', 'same', 'episode', 'it', 'ha', 'science', 'fact', 'fiction', 'that', 'would', 'give']\n",
      "\n",
      "RNN LM TOKENS (first 80):\n",
      " ['not', 'many', 'television', 'shows', 'appeal', 'to', 'quite', 'as', 'many', 'different', 'kinds', 'of', 'fans', 'like', 'farscape', 'does', 'i', 'know', 'youngsters', 'and', '30', '40', 'years', 'old', 'fans', 'both', 'male', 'and', 'female', 'in', 'as', 'many', 'different', 'countries', 'as', 'you', 'can', 'think', 'of', 'that', 'just', 'adore', 'this', 't', 'v', 'miniseries', 'it', 'has', 'elements', 'that', 'can', 'be', 'found', 'in', 'almost', 'every', 'other', 'show', 'on', 't', 'v', 'character', 'driven', 'drama', 'that', 'could', 'be', 'from', 'an', 'australian', 'soap', 'opera', 'yet', 'in', 'the', 'same', 'episode', 'it', 'has', 'science']\n",
      "Total tokens (ngram): 342\n",
      "Total tokens (rnn lm): 358\n",
      "==========================================================================================\n",
      "Example 3 | sentiment=negative\n",
      "\n",
      "RAW:\n",
      " The film quickly gets to a major chase scene with ever increasing destruction. The first really bad thing is the guy hijacking Steven Seagal would have been beaten to pulp by Seagal's driving, but that probably would have ended the whole premise for the movie.<br /><br />It seems like they decided to make all kinds of changes in the movie plot, so just plan to enjoy the action, and do not expect a coherent plot. Turn any sense of logic you may have, it will reduce your chance of getting a headache.<br /><br />I does give me some hope that Steven Seagal is trying to move back towards the type o ...\n",
      "\n",
      "TRANSFORMER (minimal cleaned):\n",
      " the film quickly gets to a major chase scene with ever increasing destruction. the first really bad thing is the guy hijacking steven seagal would have been beaten to pulp by seagal's driving, but that probably would have ended the whole premise for the movie. it seems like they decided to make all kinds of changes in the movie plot, so just plan to enjoy the action, and do not expect a coherent plot. turn any sense of logic you may have, it will reduce your chance of getting a headache. i does give me some hope that steven seagal is trying to move back towards the type of characters he portra ...\n",
      "\n",
      "NGRAM TOKENS (first 80):\n",
      " ['the', 'film', 'quickly', 'get', 'to', 'major', 'chase', 'scene', 'with', 'ever', 'increasing', 'destruction', 'the', 'first', 'really', 'bad', 'thing', 'is', 'the', 'guy', 'hijacking', 'steven', 'seagal', 'would', 'have', 'been', 'beaten', 'to', 'pulp', 'by', 'seagal', \"'s\", 'driving', 'but', 'that', 'probably', 'would', 'have', 'ended', 'the', 'whole', 'premise', 'for', 'the', 'movie', 'it', 'seems', 'like', 'they', 'decided', 'to', 'make', 'all', 'kind', 'of', 'change', 'in', 'the', 'movie', 'plot', 'so', 'just', 'plan', 'to', 'enjoy', 'the', 'action', 'and', 'do', 'not', 'expect', 'coherent', 'plot', 'turn', 'any', 'sense', 'of', 'logic', 'you', 'may']\n",
      "\n",
      "RNN LM TOKENS (first 80):\n",
      " ['the', 'film', 'quickly', 'gets', 'to', 'a', 'major', 'chase', 'scene', 'with', 'ever', 'increasing', 'destruction', 'the', 'first', 'really', 'bad', 'thing', 'is', 'the', 'guy', 'hijacking', 'steven', 'seagal', 'would', 'have', 'been', 'beaten', 'to', 'pulp', 'by', 'seagal', \"'s\", 'driving', 'but', 'that', 'probably', 'would', 'have', 'ended', 'the', 'whole', 'premise', 'for', 'the', 'movie', 'it', 'seems', 'like', 'they', 'decided', 'to', 'make', 'all', 'kinds', 'of', 'changes', 'in', 'the', 'movie', 'plot', 'so', 'just', 'plan', 'to', 'enjoy', 'the', 'action', 'and', 'do', 'not', 'expect', 'a', 'coherent', 'plot', 'turn', 'any', 'sense', 'of', 'logic']\n",
      "Total tokens (ngram): 114\n",
      "Total tokens (rnn lm): 118\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 7) Before / After examples (recommended for report)\n",
    "# -------------------------\n",
    "def show_examples(n: int = 3, random_state: int = 42):\n",
    "    sample = df.sample(n=n, random_state=random_state)[[\"review\", \"sentiment\", \"review_clean_transformer\", \"tokens_ngram\", \"tokens_neural\"]]\n",
    "    for i, row in enumerate(sample.itertuples(index=False), 1):\n",
    "        raw, sent, clean_t, tokens_ngram, tokens_neural = row\n",
    "        print(\"=\" * 90)\n",
    "        print(f\"Example {i} | sentiment={sent}\")\n",
    "        print(\"\\nRAW:\\n\", raw[:600], \"...\" if len(raw) > 600 else \"\")\n",
    "        print(\"\\nTRANSFORMER (minimal cleaned):\\n\", clean_t[:600], \"...\" if len(clean_t) > 600 else \"\")\n",
    "        print(\"\\nNGRAM TOKENS (first 80):\\n\", tokens_ngram[:80])\n",
    "        print(\"\\nRNN LM TOKENS (first 80):\\n\", tokens_neural[:80])\n",
    "        print(\"Total tokens (ngram):\", len(tokens_ngram))\n",
    "        print(\"Total tokens (rnn lm):\", len(tokens_neural))\n",
    "\n",
    "show_examples(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80eaa907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token length stats (classical):\n",
      "count    50000.000000\n",
      "mean       224.637820\n",
      "std        167.841772\n",
      "min          6.000000\n",
      "25%        122.000000\n",
      "50%        168.000000\n",
      "75%        272.000000\n",
      "max       2399.000000\n",
      "Name: len_tokens_classical, dtype: float64\n",
      "\n",
      "Token length stats by sentiment (classical):\n",
      "             count       mean         std   min    25%    50%    75%     max\n",
      "sentiment                                                                   \n",
      "negative   25000.0  222.94208  161.403126   6.0  123.0  168.0  270.0  1497.0\n",
      "positive   25000.0  226.33356  174.029099  10.0  121.0  167.0  275.0  2399.0\n",
      "\n",
      "Approx vocab size from 5000 samples: 35700\n",
      "Top 20 tokens: [('the', 66846), ('and', 32733), ('of', 28739), ('to', 27159), ('is', 21865), ('it', 20806), ('in', 18824), ('this', 15167), ('that', 14784), (\"'s\", 12279), ('wa', 10327), ('movie', 10306), ('film', 9083), ('a', 8996), ('for', 8972), ('with', 8650), ('but', 8251), ('you', 6998), ('on', 6827), (\"n't\", 6780)]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 8) Quick stats to validate preprocessing (optional but useful)\n",
    "# -------------------------\n",
    "df[\"len_tokens_classical\"] = df[\"tokens_ngram\"].apply(len)\n",
    "print(\"\\nToken length stats (classical):\")\n",
    "print(df[\"len_tokens_classical\"].describe())\n",
    "\n",
    "print(\"\\nToken length stats by sentiment (classical):\")\n",
    "print(df.groupby(\"sentiment\")[\"len_tokens_classical\"].describe())\n",
    "\n",
    "# Quick vocabulary size estimate (classical)\n",
    "# (This can be heavy; sample if needed)\n",
    "SAMPLE_FOR_VOCAB = 5000\n",
    "tokens_flat = [t for toks in df[\"tokens_ngram\"].sample(SAMPLE_FOR_VOCAB, random_state=0) for t in toks]\n",
    "vocabulary = Counter(tokens_flat)\n",
    "print(f\"\\nApprox vocab size from {SAMPLE_FOR_VOCAB} samples:\", len(vocabulary))\n",
    "print(\"Top 20 tokens:\", vocabulary.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e610f9",
   "metadata": {},
   "source": [
    "# Task 3: Classical Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5d8808",
   "metadata": {},
   "source": [
    "## Task 3.1: N-gram Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d56345a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reviews: 40000, Test reviews: 10000\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Task 3.1 — N-gram Language Models (Bigram & Trigram)\n",
    "# ============================================\n",
    "\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 0) Sanity check: df and tokens_classical\n",
    "# ---------------------------------------------------\n",
    "assert \"tokens_ngram\" in df.columns, \"Run Task 2 preprocessing first to create 'tokens_classical'.\"\n",
    "\n",
    "# Split reviews into train / test for language modeling\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "train_tokens_list_ngram: List[List[str]] = train_df[\"tokens_ngram\"].tolist()\n",
    "test_tokens_list_ngram: List[List[str]] = test_df[\"tokens_ngram\"].tolist()\n",
    "\n",
    "print(f\"Train reviews: {len(train_tokens_list_ngram)}, Test reviews: {len(test_tokens_list_ngram)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8caebc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size (with min_freq=2): 52962\n",
      "First 20 vocab items: ['<unk>', '<s>', '</s>', \"'00s\", \"'01\", \"'02\", \"'03\", \"'04\", \"'05\", \"'06\", \"'07\", \"'08\", \"'10\", \"'12\", \"'15\", \"'1947\", \"'2001\", \"'20s\", \"'24\", \"'28\"]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------\n",
    "# 1) Build vocabulary with <unk>, <s>, </s>\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def build_vocab(token_lists: List[List[str]], min_freq: int = 2) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Build a vocabulary from training tokens with frequency >= min_freq.\n",
    "    Always include special tokens: <unk>, <s>, </s>.\n",
    "    Return a dict {token: index} (index not used much, but handy).\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for tokens in token_lists:\n",
    "        counter.update(tokens)\n",
    "\n",
    "    vocab_tokens = [t for t, c in counter.items() if c >= min_freq]\n",
    "    vocab_tokens = sorted(vocab_tokens)\n",
    "\n",
    "    # Add special tokens\n",
    "    specials = [\"<unk>\", \"<s>\", \"</s>\"]\n",
    "    vocab_tokens = specials + [t for t in vocab_tokens if t not in specials]\n",
    "\n",
    "    vocab = {t: i for i, t in enumerate(vocab_tokens)}\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(train_tokens_list_ngram, min_freq=2)\n",
    "print(f\"Vocab size (with min_freq=2): {len(vocab)}\")\n",
    "print(\"First 20 vocab items:\", list(vocab.keys())[:20])\n",
    "\n",
    "\n",
    "def map_tokens_to_vocab(token_lists: List[List[str]], vocab: Dict[str, int]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Replace out-of-vocabulary tokens with <unk>.\n",
    "    \"\"\"\n",
    "    vset = set(vocab.keys())\n",
    "    mapped = []\n",
    "    for tokens in token_lists:\n",
    "        mapped_tokens = [t if t in vset else \"<unk>\" for t in tokens]\n",
    "        mapped.append(mapped_tokens)\n",
    "    return mapped\n",
    "\n",
    "train_tokens_mapped = map_tokens_to_vocab(train_tokens_list_ngram, vocab)\n",
    "test_tokens_mapped = map_tokens_to_vocab(test_tokens_list_ngram, vocab)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1da53733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example bigram sentence: ['<s>', 'that', \"'s\", 'what', 'kept', 'asking', 'myself', 'during', 'the', 'many', 'fight', 'screaming', 'match', 'swearing', 'and', 'general', 'mayhem', 'that', 'permeate', 'the', '84', 'minute', 'the', 'comparison', 'also', 'stand', 'up', 'when', 'you', 'think']\n",
      "Example trigram sentence: ['<s>', '<s>', 'that', \"'s\", 'what', 'kept', 'asking', 'myself', 'during', 'the', 'many', 'fight', 'screaming', 'match', 'swearing', 'and', 'general', 'mayhem', 'that', 'permeate', 'the', '84', 'minute', 'the', 'comparison', 'also', 'stand', 'up', 'when', 'you']\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------\n",
    "# 2) Build sentences with start/end symbols\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def build_ngram_sentences(\n",
    "    token_lists: List[List[str]],\n",
    "    n: int,\n",
    "    add_start_tokens: bool = True\n",
    ") -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Prepare sentences for n-gram modeling:\n",
    "    - pad with (n-1) <s> tokens at the start (if add_start_tokens)\n",
    "    - add </s> at the end\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    for tokens in token_lists:\n",
    "        sent = []\n",
    "        if add_start_tokens:\n",
    "            sent.extend([\"<s>\"] * (n - 1))\n",
    "        sent.extend(tokens)\n",
    "        sent.append(\"</s>\")\n",
    "        sentences.append(sent)\n",
    "    return sentences\n",
    "\n",
    "train_sents_bigram = build_ngram_sentences(train_tokens_mapped, n=2)\n",
    "train_sents_trigram = build_ngram_sentences(train_tokens_mapped, n=3)\n",
    "\n",
    "test_sents_bigram = build_ngram_sentences(test_tokens_mapped, n=2)\n",
    "test_sents_trigram = build_ngram_sentences(test_tokens_mapped, n=3)\n",
    "\n",
    "print(\"Example bigram sentence:\", train_sents_bigram[0][:30])\n",
    "print(\"Example trigram sentence:\", train_sents_trigram[0][:30])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b92659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 3) N-gram Language Model with Laplace smoothing\n",
    "# ---------------------------------------------------\n",
    "\n",
    "class NGramLM:\n",
    "    def __init__(self, n: int, vocab: Dict[str, int], alpha: float = 1.0):\n",
    "        \"\"\"\n",
    "        n: order of the n-gram (e.g., 2 = bigram, 3 = trigram)\n",
    "        vocab: mapping token -> index\n",
    "        alpha: Laplace smoothing parameter\n",
    "        \"\"\"\n",
    "        assert n >= 1\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.vocab = vocab\n",
    "        self.V = len(vocab)\n",
    "\n",
    "        # counts[context][word] = count\n",
    "        self.counts: Dict[Tuple[str, ...], Dict[str, int]] = defaultdict(lambda: defaultdict(int))\n",
    "        # total number of times each context appears (for normalization)\n",
    "        self.context_totals: Dict[Tuple[str, ...], int] = defaultdict(int)\n",
    "\n",
    "    def fit(self, sentences: List[List[str]]):\n",
    "        \"\"\"\n",
    "        Estimate counts from training sentences.\n",
    "        Each sentence is a list of tokens (already padded with <s>, </s>).\n",
    "        \"\"\"\n",
    "        for sent in sentences:\n",
    "            if len(sent) < self.n:\n",
    "                continue\n",
    "            for i in range(self.n - 1, len(sent)):\n",
    "                context = tuple(sent[i - self.n + 1 : i])  # length n-1\n",
    "                word = sent[i]\n",
    "                self.counts[context][word] += 1\n",
    "                self.context_totals[context] += 1\n",
    "\n",
    "    def prob(self, word: str, context: Tuple[str, ...]) -> float:\n",
    "        \"\"\"\n",
    "        Compute P(word | context) with Laplace smoothing.\n",
    "        context: tuple of length n-1.\n",
    "        \"\"\"\n",
    "        # Handle unseen word by mapping to <unk> if needed\n",
    "        if word not in self.vocab:\n",
    "            word = \"<unk>\"\n",
    "\n",
    "        word_count = self.counts[context].get(word, 0)\n",
    "        total_count = self.context_totals.get(context, 0)\n",
    "        # Laplace smoothing\n",
    "        num = word_count + self.alpha\n",
    "        den = total_count + self.alpha * self.V\n",
    "        return num / den\n",
    "\n",
    "    def sentence_log_prob(self, sentence: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Log probability of a single sentence:\n",
    "        sentence already padded with (n-1) <s> ... </s>\n",
    "        \"\"\"\n",
    "        log_p = 0.0\n",
    "        for i in range(self.n - 1, len(sentence)):\n",
    "            context = tuple(sentence[i - self.n + 1 : i])\n",
    "            word = sentence[i]\n",
    "            p = self.prob(word, context)\n",
    "            log_p += math.log(p + 1e-12)  # avoid log(0)\n",
    "        return log_p\n",
    "\n",
    "    def perplexity(self, sentences: List[List[str]]) -> float:\n",
    "        \"\"\"\n",
    "        Perplexity over a list of sentences.\n",
    "        \"\"\"\n",
    "        total_log_prob = 0.0\n",
    "        total_tokens = 0\n",
    "\n",
    "        for sent in sentences:\n",
    "            if len(sent) < self.n:\n",
    "                continue\n",
    "            total_log_prob += self.sentence_log_prob(sent)\n",
    "            total_tokens += len(sent) - (self.n - 1)  # tokens predicted\n",
    "\n",
    "        # perplexity = exp( - (1/N) * sum log p )\n",
    "        return math.exp(-total_log_prob / max(total_tokens, 1))\n",
    "\n",
    "    def generate(self, max_len: int = 30) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate a text sequence using the model.\n",
    "        Stops when </s> is generated or max_len is reached.\n",
    "        \"\"\"\n",
    "        vocab_list = list(self.vocab.keys())\n",
    "\n",
    "        # build initial context of (n-1) <s> tokens\n",
    "        if self.n == 1:\n",
    "            context = tuple()\n",
    "        else:\n",
    "            context = tuple([\"<s>\"] * (self.n - 1))\n",
    "\n",
    "        generated = list(context)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            # compute probability distribution over vocab\n",
    "            probs = []\n",
    "            for w in vocab_list:\n",
    "                p = self.prob(w, context)\n",
    "                probs.append(p)\n",
    "\n",
    "            # sample next token\n",
    "            next_token = random.choices(vocab_list, weights=probs, k=1)[0]\n",
    "            generated.append(next_token)\n",
    "\n",
    "            if next_token == \"</s>\":\n",
    "                break\n",
    "\n",
    "            # update context\n",
    "            if self.n > 1:\n",
    "                context = tuple(generated[-(self.n - 1) :])\n",
    "\n",
    "        # remove leading <s> and trailing </s>\n",
    "        out_tokens = [t for t in generated if t not in {\"<s>\", \"</s>\"}]\n",
    "        return out_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51a3a5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram and Trigram models trained.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------\n",
    "# 4) Train bigram and trigram models\n",
    "# ---------------------------------------------------\n",
    "\n",
    "bigram_lm = NGramLM(n=2, vocab=vocab, alpha=1.0)\n",
    "bigram_lm.fit(train_sents_bigram)\n",
    "\n",
    "trigram_lm = NGramLM(n=3, vocab=vocab, alpha=1.0)\n",
    "trigram_lm.fit(train_sents_trigram)\n",
    "\n",
    "print(\"Bigram and Trigram models trained.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f00dc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity on test set:\n",
      "  Bigram  (n=2): 1577.68\n",
      "  Trigram (n=3): 14053.90\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------\n",
    "# 5) Compute perplexity on test set\n",
    "# ---------------------------------------------------\n",
    "\n",
    "bigram_pp = bigram_lm.perplexity(test_sents_bigram)\n",
    "trigram_pp = trigram_lm.perplexity(test_sents_trigram)\n",
    "\n",
    "print(f\"\\nPerplexity on test set:\")\n",
    "print(f\"  Bigram  (n=2): {bigram_pp:.2f}\")\n",
    "print(f\"  Trigram (n=3): {trigram_pp:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb9fe5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample generations: Bigram model ===\n",
      "[Bigram sample 1]: bricusse modernist ziti faith lapd panzer beef legwarmers rivalled glitch dmv azuma wierd claud yada 'salem motivated popeil exacting jon tonal bronstein aspiration petersburg rothschild cuthbert 119 feck inertia imzadi\n",
      "[Bigram sample 2]: now maxx valli fulfilled hytner schindler violates seto institute yesilcam telescoping interconnected diferent grope knifed kangwon brogue inhibiting mekum laboeuf trannie handful zd integrates roch barmaid kyun gipdac ringside karloff\n",
      "[Bigram sample 3]: cavern shakingly ever 'spoil suavity fleetingly beane d'onofrio gurantee squeaking seaquarium hotheaded relive anding aparently quimby 'unknown fightrunner melodramatically pressurized persuasively creation hoisted seijun 'tulip argentine letch shopworn unexpressed flouncy\n",
      "\n",
      "=== Sample generations: Trigram model ===\n",
      "[Trigram sample 1]: boaz miming picket zag superimpose decorum technology dyan should't boyce cram 09 breakin hardest unedited dukakis stampede aesop 'legitimate blankfield bolton stewie accusing snowballed condo mahmut murdocco commodore inquire enchanted\n",
      "[Trigram sample 2]: am lankford beckon project cassell choosing nero sicily sartana carnacina incredible originated lemmya socio 70mm averill toyland clashing woefully analyzing botched weirdness murderously satirized cheapens wettest ejaculate joslyn pauly bricklayer\n",
      "[Trigram sample 3]: sammy n'dour splint chodorov vivre embittered corsair alaskan plinky smeared sicilian whirling jemma mccarthyism 'pache wifey unashamedly shiro ii minelli fyi tukur fiddle scientologists aaaaah kowalski cronica governing ravell allen\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------\n",
    "# 6) Generate sample texts from each model\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def join_tokens(tokens: List[str]) -> str:\n",
    "    \"\"\"Simple join with spaces; you can improve spacing if you like.\"\"\"\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "print(\"\\n=== Sample generations: Bigram model ===\")\n",
    "for i in range(3):\n",
    "    gen_tokens = bigram_lm.generate(max_len=30)\n",
    "    print(f\"[Bigram sample {i+1}]:\", join_tokens(gen_tokens))\n",
    "\n",
    "print(\"\\n=== Sample generations: Trigram model ===\")\n",
    "for i in range(3):\n",
    "    gen_tokens = trigram_lm.generate(max_len=30)\n",
    "    print(f\"[Trigram sample {i+1}]:\", join_tokens(gen_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510cc05b",
   "metadata": {},
   "source": [
    "## Task 3.2: Comparison with a Neural Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa0c316c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Task 3.2 — Neural Language Model (Embedding + LSTM + Softmax)\n",
    "# ============================================\n",
    "\n",
    "import time\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 0) Sanity checks and device\n",
    "# ---------------------------------------------------\n",
    "assert \"tokens_neural\" in df.columns, \"Run Task 2 preprocessing first.\"\n",
    "assert 'vocab' in globals(), \"Run Task 3.1 setup to create 'vocab'.\"\n",
    "assert 'train_tokens_mapped' in globals() and 'test_tokens_mapped' in globals(), \\\n",
    "    \"Run Task 3.1 mapping (train_tokens_mapped, test_tokens_mapped).\"\n",
    "\n",
    "train_tokens_list_rnn_lm: List[List[str]] = train_df[\"tokens_neural\"].tolist()\n",
    "test_tokens_list_rnn_lm: List[List[str]] = test_df[\"tokens_neural\"].tolist()\n",
    "vocab_neural = build_vocab(train_tokens_list_rnn_lm, min_freq=2)\n",
    "train_tokens_mapped_neural = map_tokens_to_vocab(train_tokens_list_rnn_lm, vocab_neural)\n",
    "test_tokens_mapped_neural = map_tokens_to_vocab(test_tokens_list_rnn_lm, vocab_neural)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "721b3799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural LM vocab size: 59281\n",
      "pad_id: 59280 unk_id: 0 bos_id: 1 eos_id: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1) Build neural-LM-specific vocab (add <pad>)\n",
    "# ---------------------------------------------------\n",
    "\n",
    "if \"<pad>\" not in vocab_neural:\n",
    "    vocab_neural[\"<pad>\"] = len(vocab_neural)\n",
    "\n",
    "# In case <s>, </s>, <unk> weren't included before (they should be, but just to be safe)\n",
    "for tok in [\"<s>\", \"</s>\", \"<unk>\"]:\n",
    "    if tok not in vocab_neural:\n",
    "        vocab_neural[tok] = len(vocab_neural)\n",
    "\n",
    "id2token_neural = {i: t for t, i in vocab_neural.items()}\n",
    "\n",
    "vocab_size = len(vocab_neural)\n",
    "pad_id = vocab_neural[\"<pad>\"]\n",
    "unk_id = vocab_neural[\"<unk>\"]\n",
    "bos_id = vocab_neural[\"<s>\"]\n",
    "eos_id = vocab_neural[\"</s>\"]\n",
    "\n",
    "print(\"Neural LM vocab size:\", vocab_size)\n",
    "print(\"pad_id:\", pad_id, \"unk_id:\", unk_id, \"bos_id:\", bos_id, \"eos_id:\", eos_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df7b2fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example train_ids: [1, 52824, 1292, 57668, 29661, 4957, 36009, 17455, 52836, 33083, 20487, 46537, 33473, 51672, 4009, 22512, 33651, 52824, 39614, 52836]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2) Convert token lists to ID sequences + add BOS/EOS\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def tokens_to_ids(tokens_list: List[List[str]]) -> List[List[int]]:\n",
    "    all_ids = []\n",
    "    for toks in tokens_list:\n",
    "        seq = [bos_id]  # start token\n",
    "        for t in toks:\n",
    "            seq.append(vocab_neural.get(t, unk_id))\n",
    "        seq.append(eos_id)  # end token\n",
    "        all_ids.append(seq)\n",
    "    return all_ids\n",
    "\n",
    "train_ids = tokens_to_ids(train_tokens_mapped)\n",
    "test_ids = tokens_to_ids(test_tokens_mapped)\n",
    "\n",
    "print(\"Example train_ids:\", train_ids[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c69f56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 245143 samples (max_seq_len=40).\n",
      "Created 61460 samples (max_seq_len=40).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3) Dataset for LM: (input_seq, target_seq) pairs\n",
    "# ---------------------------------------------------\n",
    "\n",
    "class LMSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Turn a list of token-id sequences into fixed-length\n",
    "    (input, target) pairs for language modeling.\n",
    "    - Each sample has length max_seq_len.\n",
    "    - Input is sequence[:-1], target is sequence[1:].\n",
    "    - Sequences are split into chunks when longer than max_seq_len+1.\n",
    "    - Short chunks are padded with <pad>.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seqs: List[List[int]], max_seq_len: int = 50, pad_id: int = 0):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.pad_id = pad_id\n",
    "        self.samples = []  # list of (input_ids, target_ids)\n",
    "\n",
    "        for ids in seqs:\n",
    "            if len(ids) < 2:\n",
    "                continue\n",
    "            # Slide over the sequence in chunks of max_seq_len+1 (for input+target)\n",
    "            start = 0\n",
    "            while start < len(ids) - 1:\n",
    "                end = min(start + max_seq_len + 1, len(ids))\n",
    "                chunk = ids[start:end]\n",
    "                if len(chunk) < 2:\n",
    "                    break\n",
    "                inp = chunk[:-1]\n",
    "                tgt = chunk[1:]\n",
    "\n",
    "                # Pad to fixed length\n",
    "                pad_len = max_seq_len - len(inp)\n",
    "                if pad_len > 0:\n",
    "                    inp = inp + [pad_id] * pad_len\n",
    "                    tgt = tgt + [pad_id] * pad_len\n",
    "\n",
    "                self.samples.append((inp, tgt))\n",
    "                start += max_seq_len  # non-overlapping chunks\n",
    "\n",
    "        print(f\"Created {len(self.samples)} samples (max_seq_len={max_seq_len}).\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        inp, tgt = self.samples[idx]\n",
    "        return torch.tensor(inp, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Hyperparameters (you can tune these)\n",
    "MAX_SEQ_LEN = 40\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = LMSequenceDataset(train_ids, max_seq_len=MAX_SEQ_LEN, pad_id=pad_id)\n",
    "test_dataset = LMSequenceDataset(test_ids, max_seq_len=MAX_SEQ_LEN, pad_id=pad_id)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "679187cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMLanguageModel(\n",
      "  (embedding): Embedding(59281, 128, padding_idx=59280)\n",
      "  (lstm): LSTM(128, 256, batch_first=True)\n",
      "  (fc_out): Linear(in_features=256, out_features=59281, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4) LSTM Language Model definition\n",
    "# ---------------------------------------------------\n",
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 emb_dim: int = 128,\n",
    "                 hidden_dim: int = 256,\n",
    "                 num_layers: int = 1,\n",
    "                 pad_idx: int = 0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(emb_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, hidden=None):\n",
    "        \"\"\"\n",
    "        input_ids: [batch_size, seq_len]\n",
    "        returns logits: [batch_size, seq_len, vocab_size]\n",
    "        \"\"\"\n",
    "        emb = self.embedding(input_ids)      # [B, L, E]\n",
    "        outputs, hidden = self.lstm(emb, hidden)  # outputs: [B, L, H]\n",
    "        logits = self.fc_out(outputs)        # [B, L, V]\n",
    "        return logits, hidden\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "EMB_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 1\n",
    "\n",
    "model = LSTMLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=EMB_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    pad_idx=pad_id,\n",
    ").to(device)\n",
    "\n",
    "# Loss ignores padding positions\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5dabd363",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5) Training & evaluation loops (perplexity)\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_idx, (inp, tgt) in enumerate(dataloader):\n",
    "        inp = inp.to(device)\n",
    "        tgt = tgt.to(device)  # [B, L]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(inp)  # logits: [B, L, V]\n",
    "\n",
    "        # Reshape for CrossEntropy: [B*L, V] vs [B*L]\n",
    "        B, L, V = logits.size()\n",
    "        loss = criterion(logits.view(B * L, V), tgt.view(B * L))\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # count non-pad tokens\n",
    "            non_pad = (tgt != pad_id).sum().item()\n",
    "            total_loss += loss.item() * non_pad\n",
    "            total_tokens += non_pad\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    avg_loss = total_loss / max(total_tokens, 1)\n",
    "    ppl = math.exp(avg_loss)\n",
    "    return avg_loss, ppl, elapsed\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inp, tgt in dataloader:\n",
    "            inp = inp.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "\n",
    "            logits, _ = model(inp)\n",
    "            B, L, V = logits.size()\n",
    "            loss = criterion(logits.view(B * L, V), tgt.view(B * L))\n",
    "\n",
    "            non_pad = (tgt != pad_id).sum().item()\n",
    "            total_loss += loss.item() * non_pad\n",
    "            total_tokens += non_pad\n",
    "\n",
    "    avg_loss = total_loss / max(total_tokens, 1)\n",
    "    ppl = math.exp(avg_loss)\n",
    "    return avg_loss, ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3d34eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n",
      "  Train loss: 6.1157,  Train perplexity: 452.91, Time: 1675.2s\n",
      "  Test  loss: 5.7345,  Test  perplexity: 309.37\n",
      "\n",
      "Epoch 2/3\n",
      "  Train loss: 5.5767,  Train perplexity: 264.20, Time: 1677.6s\n",
      "  Test  loss: 5.5338,  Test  perplexity: 253.11\n",
      "\n",
      "Epoch 3/3\n",
      "  Train loss: 5.3667,  Train perplexity: 214.16, Time: 1678.2s\n",
      "  Test  loss: 5.4541,  Test  perplexity: 233.72\n",
      "\n",
      "Average training time per epoch: 1677.0 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6) Train for a few epochs and monitor perplexity\n",
    "# ---------------------------------------------------\n",
    "\n",
    "EPOCHS = 3  # you can increase to 5–10 if training is fast enough\n",
    "\n",
    "train_times = []\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_ppl, train_time_sec = train_one_epoch(\n",
    "        model, train_loader, optimizer, criterion, device\n",
    "    )\n",
    "    val_loss, val_ppl = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    train_times.append(train_time_sec)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "    print(f\"  Train loss: {train_loss:.4f},  Train perplexity: {train_ppl:.2f}, \"\n",
    "          f\"Time: {train_time_sec:.1f}s\")\n",
    "    print(f\"  Test  loss: {val_loss:.4f},  Test  perplexity: {val_ppl:.2f}\")\n",
    "\n",
    "avg_train_time = sum(train_times) / len(train_times)\n",
    "print(f\"\\nAverage training time per epoch: {avg_train_time:.1f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f0ac4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample generations: LSTM neural language model ===\n",
      "[LSTM sample 1]: the screenplay is old dutch wa fine ken donald coates the romantic duo have touched me in her assignment his work that if this is written by one of the\n",
      "[LSTM sample 2]: this horrible movie rated the novelty of the wwi premiere of the group from germany with margaret croc trying to keep my attention title song both of course in this\n",
      "[LSTM sample 3]: im annoying by such trash by glen the case here is story that possibly supposed to happen dirt film and reality it is so exciting say something we venerate already\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------\n",
    "# 7) Text generation with the LSTM LM\n",
    "# ---------------------------------------------------\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def generate_with_lstm(model,\n",
    "                       max_len: int = 30,\n",
    "                       temperature: float = 1.0,\n",
    "                       start_token: str = \"<s>\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate text from the LSTM LM.\n",
    "    - start_token: starting token (usually <s>)\n",
    "    - temperature: >1.0 -> more random, <1.0 -> more greedy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Initialize input sequence with BOS\n",
    "        start_id = vocab_neural.get(start_token, bos_id)\n",
    "        input_ids = torch.tensor([[start_id]], dtype=torch.long).to(device)\n",
    "        generated_ids = [start_id]\n",
    "        hidden = None\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            logits, hidden = model(input_ids, hidden)       # logits: [1, 1, V]\n",
    "            logits = logits[:, -1, :] / max(temperature, 1e-6)  # last time step, temp scaling\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)               # [1, V]\n",
    "            next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if next_id == eos_id:\n",
    "                break\n",
    "\n",
    "            generated_ids.append(next_id)\n",
    "\n",
    "            # Next input is the newly generated token\n",
    "            input_ids = torch.tensor([[next_id]], dtype=torch.long).to(device)\n",
    "\n",
    "        # Convert IDs back to tokens (skip <s>, </s>, <pad>)\n",
    "        tokens = [\n",
    "            id2token_neural[i]\n",
    "            for i in generated_ids\n",
    "            if id2token_neural[i] not in (\"<s>\", \"</s>\", \"<pad>\")\n",
    "        ]\n",
    "        return tokens\n",
    "\n",
    "def join_tokens(tokens: List[str]) -> str:\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "print(\"\\n=== Sample generations: LSTM neural language model ===\")\n",
    "for i in range(3):\n",
    "    gen_tokens = generate_with_lstm(model, max_len=30, temperature=1.0)\n",
    "    print(f\"[LSTM sample {i+1}]:\", join_tokens(gen_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd083e",
   "metadata": {},
   "source": [
    "# Task 4: Transformer Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6489ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import evaluate\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c39ff809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_clean_transformer', 'label', '__index_level_0__'],\n",
       "        num_rows: 40000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['review_clean_transformer', 'label', '__index_level_0__'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_clean_transformer', 'label', '__index_level_0__'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert \"review_clean_transformer\" in df.columns, \"Run Task 2 first\"\n",
    "\n",
    "# Map labels to 0/1\n",
    "df_hf = df.copy()\n",
    "df_hf[\"label\"] = df_hf[\"sentiment\"].map({\"negative\": 0, \"positive\": 1})\n",
    "\n",
    "train_df, temp_df = train_test_split(df_hf, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "hf_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df[[\"review_clean_transformer\", \"label\"]]),\n",
    "    \"validation\": Dataset.from_pandas(val_df[[\"review_clean_transformer\", \"label\"]]),\n",
    "    \"test\": Dataset.from_pandas(test_df[[\"review_clean_transformer\", \"label\"]]),\n",
    "})\n",
    "\n",
    "hf_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec9453d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": metric_acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1\": metric_f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a808c068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40000/40000 [00:03<00:00, 10479.19 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 10136.23 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 10633.80 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_clean_transformer', 'label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 40000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['review_clean_transformer', 'label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_clean_transformer', 'label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MiniLM: small BERT-like model with WordPiece tokenization\n",
    "model_name_minilm = \"microsoft/MiniLM-L12-H384-uncased\"\n",
    "\n",
    "tokenizer_minilm = AutoTokenizer.from_pretrained(model_name_minilm)\n",
    "\n",
    "MAX_LENGTH = 256  \n",
    "\n",
    "def tokenize_minilm(batch):\n",
    "    return tokenizer_minilm(\n",
    "        batch[\"review_clean_transformer\"],\n",
    "        truncation=True,        # cut off longer sequences\n",
    "        max_length=MAX_LENGTH,  # hard cap at 256\n",
    "        padding=False           # let DataCollatorWithPadding handle dynamic padding\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_minilm = hf_dataset.map(tokenize_minilm, batched=True)\n",
    "tokenized_minilm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16aa9813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "data_collator_minilm = DataCollatorWithPadding(tokenizer=tokenizer_minilm)\n",
    "\n",
    "model_minilm = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name_minilm,\n",
    "    num_labels=2\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99f5d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only 8k for training, 2k for validation\n",
    "train_small_minilm = tokenized_minilm[\"train\"].shuffle(seed=42).select(range(8000))\n",
    "val_small_minilm   = tokenized_minilm[\"validation\"].shuffle(seed=42).select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84f07d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nv/z73q9gjn4cj9r9b660mdfdf40000gn/T/ipykernel_91026/3753710486.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_minilm = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args_minilm = TrainingArguments(\n",
    "    output_dir=\"./minilm_sentiment\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,        # you can increase to 3–4 if you have GPU\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=False,  # simple setup\n",
    ")\n",
    "\n",
    "trainer_minilm = Trainer(\n",
    "    model=model_minilm,\n",
    "    args=training_args_minilm,\n",
    "    train_dataset=train_small_minilm,\n",
    "    eval_dataset=val_small_minilm,\n",
    "    tokenizer=tokenizer_minilm,\n",
    "    data_collator=data_collator_minilm,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b56b68d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I747530/Documents/NLP_project/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 10:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.251700</td>\n",
       "      <td>0.314959</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.879168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.178400</td>\n",
       "      <td>0.250029</td>\n",
       "      <td>0.913500</td>\n",
       "      <td>0.913486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.162300</td>\n",
       "      <td>0.266863</td>\n",
       "      <td>0.917000</td>\n",
       "      <td>0.916952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I747530/Documents/NLP_project/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/I747530/Documents/NLP_project/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/I747530/Documents/NLP_project/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniLM (validation) metrics:\n",
      "{'eval_loss': 0.2668634057044983, 'eval_accuracy': 0.917, 'eval_f1': 0.9169521644467213, 'eval_runtime': 13.198, 'eval_samples_per_second': 151.538, 'eval_steps_per_second': 4.773, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "train_result_minilm = trainer_minilm.train()\n",
    "eval_minilm_val = trainer_minilm.evaluate()\n",
    "\n",
    "print(\"MiniLM (validation) metrics:\")\n",
    "print(eval_minilm_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5361db84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I747530/Documents/NLP_project/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniLM Test Accuracy: {'accuracy': 0.9068}\n",
      "MiniLM Test Macro-F1: {'f1': 0.9067515257131267}\n",
      "\n",
      "=== Confusion Matrix (Text) ===\n",
      "                       Predicted Negative   Predicted Positive\n",
      "Actual Negative                      2210                  252\n",
      "Actual Positive                       214                 2324\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_test_minilm = trainer_minilm.predict(tokenized_minilm[\"test\"])\n",
    "logits_test_minilm = pred_test_minilm.predictions\n",
    "labels_test_minilm = pred_test_minilm.label_ids\n",
    "pred_labels_minilm = logits_test_minilm.argmax(axis=-1)\n",
    "\n",
    "acc_test_minilm = metric_acc.compute(predictions=pred_labels_minilm, references=labels_test_minilm)\n",
    "f1_test_minilm = metric_f1.compute(predictions=pred_labels_minilm, references=labels_test_minilm, average=\"macro\")\n",
    "\n",
    "print(\"MiniLM Test Accuracy:\", acc_test_minilm)\n",
    "print(\"MiniLM Test Macro-F1:\", f1_test_minilm)\n",
    "\n",
    "cm_minilm = confusion_matrix(labels_test_minilm, pred_labels_minilm)\n",
    "\n",
    "print(\"\\n=== Confusion Matrix (Text) ===\")\n",
    "print(f\"{'':20} {'Predicted Negative':>20} {'Predicted Positive':>20}\")\n",
    "print(f\"{'Actual Negative':20} {cm_minilm[0, 0]:>20} {cm_minilm[0, 1]:>20}\")\n",
    "print(f\"{'Actual Positive':20} {cm_minilm[1, 0]:>20} {cm_minilm[1, 1]:>20}\")\n",
    "\n",
    "disp_minilm = ConfusionMatrixDisplay(confusion_matrix=cm_minilm, display_labels=[\"negative\", \"positive\"])\n",
    "disp_minilm.plot(cmap=\"Blues\")\n",
    "plt.title(\"MiniLM Confusion Matrix (Test)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f2f70b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40000/40000 [00:04<00:00, 8017.45 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 8045.53 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 8169.71 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_clean_transformer', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 40000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['review_clean_transformer', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_clean_transformer', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_bpe = \"distilroberta-base\"\n",
    "\n",
    "tokenizer_bpe = AutoTokenizer.from_pretrained(model_name_bpe)\n",
    "\n",
    "\n",
    "def tokenize_bpe(batch):\n",
    "    return tokenizer_bpe(\n",
    "        batch[\"review_clean_transformer\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",   # or \"longest\" if you prefer dynamic padding\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "tokenized_bpe = hf_dataset.map(tokenize_bpe, batched=True)\n",
    "\n",
    "tokenized_bpe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4719b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the original text column(s) so Trainer only sees tensors + labels\n",
    "tokenized_bpe = tokenized_bpe.remove_columns([\"review_clean_transformer\"])\n",
    "\n",
    "# Make sure labels are called \"labels\" for Trainer\n",
    "tokenized_bpe = tokenized_bpe.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Set to torch format\n",
    "tokenized_bpe.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d8bd124",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_bpe = tokenized_bpe[\"train\"].shuffle(seed=42).select(range(8000))\n",
    "val_small_bpe   = tokenized_bpe[\"validation\"].shuffle(seed=42).select(range(2000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f063a0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/nv/z73q9gjn4cj9r9b660mdfdf40000gn/T/ipykernel_91026/1590910999.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_bpe = Trainer(\n"
     ]
    }
   ],
   "source": [
    "num_labels = 2  # binary sentiment\n",
    "id2label = {0: \"negative\", 1: \"positive\"}\n",
    "label2id = {\"negative\": 0, \"positive\": 1}\n",
    "\n",
    "model_bpe = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name_bpe,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "training_args_bpe = TrainingArguments(\n",
    "    output_dir=\"./distilroberta_sentiment\",\n",
    "    eval_strategy=\"epoch\",  \n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer_bpe = Trainer(\n",
    "    model=model_bpe,\n",
    "    args=training_args_bpe,\n",
    "    train_dataset=train_small_bpe,\n",
    "    eval_dataset=val_small_bpe,\n",
    "    tokenizer=tokenizer_bpe,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ec51312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I747530/Documents/NLP_project/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 11:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.235700</td>\n",
       "      <td>0.239735</td>\n",
       "      <td>0.909500</td>\n",
       "      <td>0.909278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.162100</td>\n",
       "      <td>0.265694</td>\n",
       "      <td>0.928500</td>\n",
       "      <td>0.928500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.129900</td>\n",
       "      <td>0.296287</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.927977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I747530/Documents/NLP_project/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/I747530/Documents/NLP_project/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/I747530/Documents/NLP_project/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilRoBERTa (validation) metrics:\n",
      "{'eval_loss': 0.26569420099258423, 'eval_accuracy': 0.9285, 'eval_f1': 0.928499553122207, 'eval_runtime': 14.6647, 'eval_samples_per_second': 136.382, 'eval_steps_per_second': 4.296, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "train_result_bpe = trainer_bpe.train()\n",
    "eval_bpe_val = trainer_bpe.evaluate()\n",
    "\n",
    "print(\"DistilRoBERTa (validation) metrics:\")\n",
    "print(eval_bpe_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ddb7362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I747530/Documents/NLP_project/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilRoBERTa Test Accuracy: {'accuracy': 0.916}\n",
      "DistilRoBERTa Test Macro-F1: {'f1': 0.9159977285785807}\n",
      "\n",
      "=== Confusion Matrix (Text) ===\n",
      "                       Predicted Negative   Predicted Positive\n",
      "Actual Negative                      2277                  185\n",
      "Actual Positive                       235                 2303\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAXkNJREFUeJzt3Qd4FNXXBvCThBZK6ITQe++I9CYQBKQoqHSQjvQOSq8KSBEVlCoIgvAXaQqELr1JR7oQpNdAICEh+z3v5Zt1N8kmu9lNmcn78xnD7szOzpZkz55z7h03k8lkEiIiIiKDcI/vAyAiIiJyJQY3REREZCgMboiIiMhQGNwQERGRoTC4ISIiIkNhcENERESGwuCGiIiIDIXBDRERERkKgxsiIiIyFAY35JSxY8eKm5uby57Fjh07Sp48eayuw/5xP6Rfly5dEl9fX0mbNq16PX/77TeX7v+ff/5R+12yZIlL96tntWrVUosr+fv7S4oUKWTfvn2SkFSqVEmGDh0a34dBCQiDGzLDBwM+ILQFf8SyZcsm9evXl6+//lqePXvmkmfr1q1bKlg5ceKES44zSZIkkj17dhUY/fvvvy7ZJ5YsWbJI7dq15Y8//oiwffhtLZcePXqYt8MxWa5Lnjy5FCpUSEaPHi1BQUFqGwRzUe1PW5z94L5y5Yp0795d8uXLp15bLy8vqVq1qsyePVtevnwpsalDhw5y+vRpmTRpkixbtkzeeustMQrtNcbzGdnziMBOew2nT58e578vrjR+/HipWLGiet/s2rXLrvetq778nDt3Tj0PCGTDGzZsmHz77bdy584dl9wX6V+S+D4ASnjwByxv3rwSEhKi/ljgj1j//v1lxowZsn79eilVqpR525EjR8rw4cMd/mM9btw49aFepkwZq3Xz58+XsLAwh44TQcLBgwfVh//evXvlzJkz6sM7JrR94pRrd+/eVfts2LChbNiwQd577z2rbevVqyft27ePsA8EL5YQ0CxYsED9++nTp7Ju3TqZMGGCCjaWL18us2bNkufPn5u3//333+Xnn3+WmTNnSqZMmczXV6lSRWJq06ZN8uGHH6pjwTGXKFFCXr16pZ6vIUOGyNmzZ+WHH36Q2IAP/AMHDsjnn38uvXv3jpX7yJ07t7qfpEmTSnxAgP3ixQv1Pvnoo4+s1uE1xvtRC2YdFdXvS1S2bt0qrnT//n358ccf1QJFixZVgaqlESNGSOrUqdVr7WoIbvA8IBsVPrvbtGlTFVx+99136neYCH/EiZTFixfjJKqmI0eORHhGtm/fbvL09DTlzp3b9OLFC6eeMewf94P7swe2HTNmTLTHOWzYMHX9qlWrHD4mW/t89OiRKWnSpKbWrVtHOKZevXpFu98OHTqYUqVKZXVdWFiYqVKlSiY3NzfTnTt3Itxm2rRpav/Xrl0zucLVq1dNqVOnNhUpUsR069atCOsvXbpkmjVrlim2XL9+XT0ePC4j0l5jX19fU7NmzSKsL1iwoKl58+Yxfg4c/X0JDAw0xYYZM2aovwHPnj2zuU3x4sVNNWvWjJX7X716tXoedu7cGen63r17q79P+P0iYlmK7PLOO+/IqFGj5Pr16/LTTz9F2XPj5+cn1apVk3Tp0qlvcYULF5bPPvtMrUMWqEKFCurfn3zySYSSS2Q9N/aqXr26+omMiKUdO3aodalSpVLHhG9558+ft2uf2N7T01N9M3cVPF48P4iRrl69atdtkO1p1KiRKhMi+5I/f36V/Xn9+nW0t506darKDC1cuFB8fHwirC9QoID069fPfDk0NFTtG/eB+8LrgdcvODjY6na4HtksZH/efvttlZ1AyWvp0qVW7w9kVQAZIjx27fW19Vo7+p6KqufGntdeu7/Lly+rY8J26A3C+xPZGHu1bt1alTCfPHlivu7IkSOqLIV14T169EgGDx4sJUuWVI8JmYcGDRrIyZMnzdtE9/uCLAaycMeOHZMaNWpIypQpzc9L+J4blAbxGoV//Cg7p0+fXmWIooI+KZSkcKyOwPOBzG/OnDnV+wnvty+//DJChnblypVSvnx5SZMmjXou8LygZAp4vMg8AkrF2vOA58cyk4q/TwmhfEfxj8EN2a1du3bRprtR3sAHHj4IkR7+6quvpEmTJuYGRKSytbRxt27dVFobC/4wO0urxeMPtWbbtm3qj/e9e/fUh9jAgQNl//79qmcgsto9ykYPHjxQKXg8lp49e6rAoG3bthG2RZkB24ZfUO6JybFGBX/c8aGC48cffHwIoG/HnpIgSiUIOuwta3Xp0kXtu1y5cqo0VrNmTZkyZYq0bNkywrYICFq0aKE+WPBa4/EgQMBzBx988IHaB7Rq1Uq91ijDOSK695Qtjr72KCehrwyPFf/Gc44yiL3wWPGB++uvv5qvW7FihRQpUkQ9l+EhsEXAgMeGki+CP/Ql4fnWAg17fl8ePnyogiKUrPDc4sM/MnjfZM6cWQU5WlD8/fffq9/nOXPmqMDZFpSoEahF9jiiguAQjwdfiFAORe8enn+Ur/B6WAaveH/g/YPA54svvlCBmfYa4/H27dtX/RvBm/Y84PnR4HcCElqzM8UTJq/InrKUJm3atKayZcuaL6NcZPk2mjlzprp8//79GKXZkeJHatmestS2bdvU/fj7+5vWrFljypw5syl58uTqsqZMmTKmLFmymB4+fGi+7uTJkyZ3d3dT+/btI+wz/IL9LVmyJMJxRrattvz8888RShY4TiyXL182TZ8+XZWkSpQoEWkKPbKyVGSlwO7du5tSpkxpCgoKsvlcP336VO2radOmJnucOHFCbd+lSxer6wcPHqyu37Fjh/k6vE64bs+ePebr7t27p56zQYMGma/D44isJBPZax3T95R2H5bvKXtfe+3+OnXqZLXP999/35QxY0ab9xlZ6bFFixamOnXqqH+/fv3alDVrVtO4ceMifQ7wumGb8I8Dz9/48ePt+n1BCQjr5s2bF+m68CWiLVu2qO0nTpxoLldGVkoLD+9b3G7OnDlRbhe+LDVhwgT13Fy8eNFqu+HDh5s8PDxMN27cUJf79etn8vLyMoWGhsa4LAXJkiUz9ezZM9rHQ8bHzA05BNmDqEZNIaWvlVHsbQyOqbp166pvokh3I3uA0gMannPkyKHW3759W6WokUnIkCGD+XZoiEamAY274WHEBb5FYsG3TXwLRibD8tu4BiUObVvLJfw358DAQHWcWJCSRykC317xHNk7kgSlMQ2ef2SIUG7BN+O///7b5u0CAgLUT6T67aE9J5bfqmHQoEHmxmRLxYoVM5cDAY8RJSN7y232iMl7KiavveUoN8DjQlZEew7tgfITSiVoxEdJDD8jK0kBSjTu7m/+BCOTgvvSSm7Hjx+3+z6xH5Ss7IHh+Bgxh2wQMk0oUyF7Ex0cmyOZRs3q1avV84jbWWY38buLx7xnzx7za4zfE/z+OEO7HyIGN+QQlGii+qD8+OOP1Qc3AgJvb29Vyvjll19iJdDRApE1a9aoEU34o4Y/9BrU3wEfFuEhnY3t8QfVEnpH8IcXS5s2bdSHOT7AMconfLkJQZS2reWCx20JHyBa4LN48WJ13yiVWAYs9pRm3n//fdULgn4EBBFaqQylNFuwLdg7jB/PGT5wEYRZypo1q/oA0p5TTa5cuSL9gHn8+LG4SkzeUzF57cM/Fu2D3JHHgvchfj9WrVqlRkmhXyb8c6nB8aNkV7BgQfW+xcg4vK6nTp2K8jUND9MgJEuWzO7tMRwdAR+CP5SJMOWBvd4kLe2HfqPNmzebg3ttwe8J4PcAPv30UzXKEOU1/F516tRJ3c5ROD5XzrtF+sWh4GS3mzdvqj+6tv5YAz6w8W1s586dKjDAHyj8oUdDMmr7Hh4eLnvGEYho86U0a9ZMNZziW/KFCxccbnq0BR/0yMSgXwF/qIsXL+7wPvCYtT/mgD4Q9GHgGzQyTfY0ZKJvAYEKvnGj0RcBE77dY36PqD7kcRv0UmB4vCPs/YCw9Xra8yFo6z7CN0nH1XvKmceiQZCCjAiGSyN7FdXkk5MnT1ZN+vggRwM3Ag6839B868iXAUeCZPjrr7/MQQV6fNDrEp2MGTOqn44GrXgcyJTZmmBPmzYBARaCrS1btqimbCz4IoA+HW3ouT3wu2I5fQIlXszckN20OS3w4Rzlm8rdXerUqaOaJDE3BSZuQ4oeH04QG9+s8MGERlA0Yn7zzTfqOm2UDoKd8FDKwR9BlLKig9FDYDkXjTMwYmnAgAGq0Rfz80QHZQ6UBdDgilFNaEBFsGRviQDbYwQZ5pqJDp4zfCAhkLOEOX/wwaE9p66A47ccWaQJnx2y5z0V2eNwxWsfEwiwEUAgWxZZE7YGGUcEzhjFhu1QMsLrGv45ceXvC7JVKGEhG4kGZYykQ6NwdJDVQhB17do1h+4PgTh+byLLcGKxzJYh+9S4cWM1V4024SRG3qFp3Z7nARN4Irtq2WRMiReDG7ILPkjw7RIT3KFcYwuGt4anTTymDSXWPlQi+2BzBkZXIJuDESMYyYQgAveNb36W94UsBr7xo4QQHYwSwbb4w+vKP5p9+vRRw3YxKsTejIJlBgF/xPEhYA98a8ZzjrIOgpTw8EGiDbnVnpPwI5oQVACGo7sKPviQCUQZxrJXZu3atQ6/p8JzxWsfUwhY8LuCIBvlvKhe1/BZIfSohJ9l25W/L8j03bhxQz0veE0xFB+jp2w9jxpMjogs6dGjRx26P4w6Q1CNjEx4eDzaFwetp8cymNUmC7X37waGwzs72SUZB8tSFAFSwvh2iz88+DBEYIN+EXwbRhklqtl/UTZBCQEfgtge6W98CKOOjrKR9qGG/o158+ap/gT80cL8GQicnIXhtJgPA1kONIhOmzZN1fErV64snTt3VrPYYtgrelciKxlojx1w7BjKiywGhlxr/SuaixcvWs35o0FfCFLx0aX58Q0azw3mHYkqcMIfa2Q58CGE4bD4Bossmr3lEjzfeBzoXcH9WM5QjKHR+EBF4y2ULl1a3Q9mK9bKYYcPH1Yfhij92RpmHBPIVuDDFr1EeFxojp47d64qVVg21NrznoqMo6+9q+CDGTN325NRw2PD+wCvMUpE6NPBsH1Lrvp9we8xnrcxY8aYh3Sj9IMvBSiPIYsTFTTQY+ZhNFiH/12I6vcRfzPwWPEew3BtZI/wWJG5wpB8ZNEQeCOIRakRryuyd3itEKBqvxv4NwJCDBVHUIwSILbXeobwNwqZoLJlyzr0vJBBxfdwLUo4wg+HxrBKDGWtV6+eafbs2aaAgIBoh+1iJmMMO86WLZu6PX62atUqwlDQdevWmYoVK2ZKkiSJ1TBXR4aCRzZkHUNr8+fPrxZtWCmGjFetWlXNrorhpo0bNzadO3cuyseOJUWKFGo48dy5cyMM2Y5qKLjlUNjIZijWXLlyRQ2HxTbRDQXft2+fmtUYjwHP6dChQ83DeqMaGmsJr0HXrl1NefLkUa9NmjRp1POC4b2Ww8lDQkLU8OW8efOq2Zlz5sxpGjFiRIQh53idGjVqFO0QZFtDwWHr1q1qSDyOp3DhwqaffvopRu+pyIaC2/vaa/cXfqi59p6IbqboqF7jqJ4DPJ8YMu/j46OOD8d54MCBSIdw2/p9wXYYfh0Zy/3gdxevV7ly5dTra2nAgAFqeDzuOyp3795V979s2TKHZijGjMZ4/xQoUEC9fpkyZTJVqVJFTYnw6tUrtQ2mcsAMzxi6j21y5cqlpjq4ffu21b7mz59vypcvn/q9sXzv4/cez+PIkSOjfAyUeLjhf/EdYBERUcKHDBgyln/++ackJJgMEb1OKLFGNgs3JT4MboiIyC7o10HZcPv27Wp4fkKB0iPm04mutEaJB4MbIiIiMhSOliIiIiJDYXBDREREhsLghoiIiAyFwQ0REREZCifxS0Aw7T1OH4CJunjyNyIi/cHsKjj1Bs7ppp31PTZgFvbwJ/ONiWTJkkU5MateMbhJQBDY5MyZM74Pg4iInOTv769mW46twMYzbSqRV/afYNUWnCIE5wwzWoDD4CYBQcZGqZlVJAkrhmRMd1e/OQcQkRE9C3gmBfIU+u/veSxQGRsENtXwWeHEiVVDTXJn7x21PwY3FGvMpSgENgxuyKDsPS8RkZ7FSWtBUic/K9ycz/wkVMzcEBER6RHiGmeS/O5iWAxuiIiI9AjZIWcyRG5xkF2KJwaO24iIiMhVpkyZIhUqVFD9RFmyZJFmzZrJhQsXzOsfPXokffr0kcKFC4unp6fkypVL+vbtK0+fPo1Qsgu/rFy50mqbXbt2Sbly5SR58uRSoEABWbJkiUPHyuCGiIhIr9ycWBy0e/du6dWrlxw8eFD8/PwkJCREfH19JTAw0DziF8v06dPlzJkzKiDZvHmzOpt8eIsXL5bbt2+bFwRKGozeatSokdSuXVtOnDgh/fv3ly5dusiWLVvsPlaeODMBCQgIkLRp04rUycaGYjKsl5v++6ZHZMS/494ZfFS2Iraa582fFfVyvGkqjqmQMBG/mzE+1vv376sMDoKeGjVqRLrN6tWrpW3btioASpLkTScMMjVr1661CmgsDRs2TDZt2qQCJE3Lli3lyZMnKliyBzM3REREiVhAQIDVEhwcbNfttHJThgwZotwGgZMW2GiQAcqUKZO8/fbbsmjRIjX5oebAgQNSt25dq+3r16+vrrcXgxsiIiI9j5ZyZhFRk8ciE6Qt6K2xZ0Z9lIuqVq0qJUqUiHSbBw8eyIQJE6Rbt25W148fP15++eUXVdpq3ry5fPrppzJnzhzz+jt37oi3t7fVbXAZgdfLly/temo4WoqIiCgRj5by9/e3KkuhiTc6yLygbLR3795I1yMQQd9MsWLFZOzYsVbrRo0aZf532bJlVclq2rRpqvnYVZi5ISIiSsS8vLysluiCm969e8vGjRtl586dkZ5iAufWevfdd9WoKvTWJE2aNMr9VaxYUW7evGkuh+GUEHfv3rXaBpdxbBiFZQ8GN0RERIltpJSb4yOm0BeDwAYBy44dOyRv3ryRZmwwggon5Fy/fr1dp3XAiKj06dObg6rKlSvL9u3brbZBCQvX24tlKSIiIj1yd3uzOHN7B6AUtWLFClm3bp3KyqA3BtCng4yKFti8ePFCfvrpJ3ODMmTOnFk8PDxkw4YNKgtTqVIlFfggaJk8ebIMHjzYfD89evSQb775RoYOHSqdOnVSgRR6dDCCyl4MboiIiChac+fOVT9r1aoVYc6ajh07yvHjx+XQoUPqOky8Zwlz1+TJk0eVqL799lsZMGCAygRhuxkzZkjXrl3N2yIjhEAG28yePVuVvhYsWKBGTNmL89wkIJznhhIDznNDRhan89y8l8v5eW423ojVY40vzNwQERHpEc8tZRODGyIiIj2K4WkUzIx73kyOliIiIiJjYeaGiIhIj+J4tJSeMLghIiLSI5albOIkfkRERGQozNwQERHpEUdL2cTghoiISI/Yc2MTy1JERERkKMzcEBER6REbim1icENERKTb4MaJ4dxuYlgsSxEREZGhMHNDRESkVwbOvjiDwQ0REZEecbSUTQxuiIiI9IgNxTax54aIiIgMhZkbIiIiPeIMxTYxuCEiItJr7cWZ+ou7GJaBHxoRERElRszcEBER6RHLUjYxuCEiItIjjpayiWUpIiIiMhRmboiIiPSIZSmbGNwQERHpEUdL2cSyFBERERkKMzdERER6xLKUTQxuiIiI9IijpWxicENERKRHPCu4Tey5ISIiIkNh5oaIiEiP2HNjE4MbIiIiPWLPjU0sSxEREZGhMHNDRESkS27ihtJUDJlU6seYmLkhIiLSIQQ2zi6OmDJlilSoUEHSpEkjWbJkkWbNmsmFCxestgkKCpJevXpJxowZJXXq1NK8eXO5e/eu1TY3btyQRo0aScqUKdV+hgwZIqGhoVbb7Nq1S8qVKyfJkyeXAgUKyJIlSxw6VgY3REREFK3du3erwOXgwYPi5+cnISEh4uvrK4GBgeZtBgwYIBs2bJDVq1er7W/duiUffPCBef3r169VYPPq1SvZv3+//PjjjypwGT16tHmba9euqW1q164tJ06ckP79+0uXLl1ky5YtYi83k8lksntrilUBAQGSNm1akTrZRJIw7iRjernJ+psekdH+jntn8JGnT5+Kl5dXrH5WuPcsLm7JPWK8H1PwawmbezbGx3r//n2VeUEQU6NGDbWfzJkzy4oVK6RFixZqm7///luKFi0qBw4ckEqVKskff/wh7733ngp6vL291Tbz5s2TYcOGqf0lS5ZM/XvTpk1y5swZ8321bNlSnjx5Ips3b7br2PgJSkREpEPubm5OL1qwZLkEBweLPRDMQIYMGdTPY8eOqWxO3bp1zdsUKVJEcuXKpYIbwM+SJUuaAxuoX7++ut+zZ8+at7Hch7aNtg97MLghIiJKxHLmzKkyQdqC3prohIWFqXJR1apVpUSJEuq6O3fuqMxLunTprLZFIIN12jaWgY22XlsX1TYIgF6+fGnXY+JoKSIiIh2KSVOwlf+/rb+/v1VZCk280UHvDcpGe/fulYSIwQ0REVEiDm68vLwc6rnp3bu3bNy4Ufbs2SM5cuQwX581a1bVKIzeGMvsDUZLYZ22zeHDh632p42mstwm/AgrXMYxenp62nWMLEsRERHpUFwPBTeZTCqwWbt2rezYsUPy5s1rtb58+fKSNGlS2b59u/k6DBXH0O/KlSury/h5+vRpuXfvnnkbjLxC4FKsWDHzNpb70LbR9mEPZm6IiIjIrlIURkKtW7dOzXWj9cigTwcZFfzs3LmzDBw4UDUZI2Dp06ePCkowUgowdBxBTLt27WTq1KlqHyNHjlT71sphPXr0kG+++UaGDh0qnTp1UoHUL7/8okZQ2YvBDRERUSI8b6Y4eNu5c+eqn7Vq1bK6fvHixdKxY0f175kzZ4q7u7uavA+jrjDK6bvvvjNv6+HhoUpaPXv2VEFPqlSppEOHDjJ+/HjzNsgIIZDBnDmzZ89Wpa8FCxaofdn90DjPTcLBeW4oMeA8N2RkcTnPjWe/Mk7Pc/Ny9olYPdb4wp4bIiIiMhSWpYiIiBLxaCkjYnBDRESkQ27//58zezAqlqWIiIjIUJi5ISIi0iGWpWxjcENERKRDcT0UXE9YliIiIiJDYeaGiIhIh9xV5ibm6ReTgTM3DG6IiIh0iD03tjG4ISIi0iEGN7ax54aIiIgMhZkbIiIiPXJytJSJPTdERERkpLKUm4FPv8CyFBERERkKy1JEREQ6xMyNbQxuiIiIdEidNtOZspSwLEVERESkC8zcEBER6RDLUrYxuCEiIkqEJ850M25ViqOliIiIyFiYuSEiItIhlqVsY3BDRESkQwxubGNwQ0REpEPubm5qiTE34zbdcIZiIiIiMhRmboiIiHSIo6VsY3BDRESkQ+y5sY1lKSIiIjIUZm5sGDt2rPz2229y4sSJuH1FyCGDP+wmzar4SqEc+eTlqyA5dP4v+XzxdLn07zW1Pn3qtDKqbR+pU7aa5MzsIw+ePpINB7fJuGWzJeDFc7VN27rvy/wBX0S6/1ytK8v9p4/khwFTpF3dDyKsP3f9kpT/9D2+ahSn9p4+IjPXLJDjl87KnUf3ZNXob6VJlXrm9c9fBsrIRdNlw4Ft8ijgieTJmkM+bdpeujZqZd7Gd0hb+fP0Yav9dmnYUub0HR+nj4WcPLeUE+eHcjPwuaUY3Px/am/t2rXSrFkz8xMzePBg6dOnT3y+NmSH6iXflnmblsuxi6cliYeHjOswUDZOXChlezSSF8EvxSdjFvHJkEVGLPxSzt+4LLmyZJc5vceq61pP6af2sWbP7+J37E+r/f4w4AtJkTSZCmxg8PeTZNSSr8zrk7h7yKFv1smvezfzdaI4Fxj0QkrmLSLtfZtLywm9I6wf9sMU2XXioCweMl1ye2eXbcf3Sr9vxqn3/XuV65i369TgIxnV7s3vAaRM7hlnj4Gcx7KUbQxubEidOrVaKGFrOrqL1eVuM4aL/88HpWyB4rLv7FGVWWk1ua95/bU7/jJ26SxZNHiaeLh7yOuw1xL0Klgtmkxe6aVWqYrSY/ZI83XI8miZHmhcqY7KCi3z+zXWHyNRePUr1FSLLQfP/aUykjVKV1SXOzdsKQt/XyVHL5yyCm48k3tK1gyZ+QST4cRrz02tWrWkb9++MnToUMmQIYNkzZpVlYM0T548kS5dukjmzJnFy8tL3nnnHTl58qTVPiZOnChZsmSRNGnSqG2HDx8uZcqUMa8/cuSI1KtXTzJlyiRp06aVmjVryvHjx83r8+TJo36+//77KgrWLuM4tP1s3bpVUqRIoY7HUr9+/dQxafbu3SvVq1cXT09PyZkzp3psgYGBLn/eyDavVGnUz8fPn9reJmVqFaggsIlMmzrN5EVwkKzdZzsr08G3hew4sV9u3L/Fl4MSnErFysrGg9vl3wd3xGQyye6TB+XSv/9I3fLVrLZbtXO95PjobSnfvZGMWjRdXgS9jLdjpphnbpxZjCreG4p//PFHSZUqlRw6dEimTp0q48ePFz8/P7Xuww8/lHv37skff/whx44dk3LlykmdOnXk0aM3pYLly5fLpEmT5Msvv1Trc+XKJXPnzrXa/7Nnz6RDhw4q8Dh48KAULFhQGjZsqK7Xgh9YvHix3L5923zZEu4zXbp08r///c983evXr2XVqlXSpk0bdfnKlSvy7rvvSvPmzeXUqVNqHe6zd++IKWOKHfhFndbtM9l/9pjK2EQmo1d6GdHqU1m0eVWUgcuq3RutsjmWkNqv/1YNWbJljcuOnciVZvQcLUVzF5ACbWuI13vFpcnIzjKr12ipVrKCeZuPa78ni4ZMl81fLpPBH3eXFTvWySdTB/OF0OFQcGcWo4r34KZUqVIyZswYFXS0b99e3nrrLdm+fbsKDA4fPiyrV69W12H99OnTVZCxZs2bD5U5c+ZI586d5ZNPPpFChQrJ6NGjpWTJklb7R2albdu2UqRIESlatKj88MMP8uLFC9m9e7daj6wQYL/IHGmXLXl4eEjLli1lxYoV5utwjMjkIJiBKVOmqECnf//+6lirVKkiX3/9tSxdulSCgoIifezBwcESEBBgtVDMzeo5RornLijtvxwQ6fo0nqlk7djv5fyNKzJx+TeRblOxSBkpmquA/LjVduCCzM6T589k/cFtfLkoQfpu/TI5fP6krBk7T/bP+VW+6Dpc+n87XnYc32feBqWqem9VlxJ5C0urd5rIwsFTZf1+P7l660a8HjslXHv27JHGjRtLtmzZ1JdJDLqxZCs7NG3aNPM2qI6EX//FF9YDOpAgQBUEFRNUQZD40GVwY8nHx0dla1B+ev78uWTMmNHc/4Ll2rVrKksCFy5ckLffftvq9uEv3717V7p27aoCDpSlUN7Cfm/ccOwXGIHLrl275NatW+asUaNGjVRQBDjeJUuWWB1r/fr1JSwsTB1zZBAQ4Zi0BS8ixczMHqOk4du1pP6IDvLvw7sR1qf2TCXrJyyQZy8D5eOJvST0dWik++lY/0M5ceWc/HX5rM376lCvufy8c52EhIbw5aIE52VwkIxZMkO+7DZcGlV6R0rmKyI9m7STFjUayKz/LbJ5uwpFSqufV25dj8OjJT2VpQIDA6V06dLy7bffRroe1Q/LZdGiReo+tCSABhUay+0sB+/gS76vr6/kzp1bVWQQGKFNBIkJXTUUJ02a1OoynggEBAhAEOggoAhPCyjsgZLUw4cPZfbs2erJSp48uVSuXFlevXrl0HFWqFBB8ufPLytXrpSePXuq0VUIZjQ43u7du6s+m/BQLovMiBEjZODAgVYvKgOcmAU2TSrXE98R7eT63ZuRZmw2TFgowSGvpMX4nupnZFKlSCnNqzWQ0T/+NyoqstFZBbLnkSWTWJKihCkkNFQF3u7u1t9d0UAfZgqzebuTV86rn2ww1o+4Hi3VoEEDtdiC6oeldevWSe3atSVfvnxW16NHNvy2GiQO8PmMwChZsmRSvHhxNSXLjBkzpFu3bvoJbmxBf82dO3ckSZIk5ibf8AoXLqx6ZFDO0oTvmdm3b5989913qs8G/P395cGDBxECLPTQ2JO9wROfI0cO9YcDmRvL4z137pwUKFDA7seIQAsLxdysT8fIxzXfkw8nfKrm9vBOn0ld/zTwmeqZQWCzceIiNSrkk+lDVDMxFsAwbwTSmhY1Gqrh5D/vXG/z/jr6tpDDf5+w2dNDFBfwXrfMsPxz56acvHJO0qdJJ7myZFNB+GcLpopnshSSyzub/HnqiCzf/pt82W2E2h6lp1U7N0j9t2tKxjTp5PS1CzL0h8mqJweZHtIJZ5uC3WKv6QZVk02bNqm+2vBQhpowYYL64t+6dWsZMGCA+qyHAwcOSI0aNVRgo0EVBL21jx8/lvTp0+s7uKlbt67KsGDuGdTb0FODkhCeLIxsQh8OUlkoOeHf6HFBEy9qdZZRIspRy5YtU9sgMzJkyBA1mskSgif00FStWlUFG7aePAQ3SI+hiblFixZWgcmwYcOkUqVKqoEYo7bQJI1gB83R33wTeX8HOa97o9bqp9+XP1ld33XmcPlp21opU6C4vF3kzai3cwute2QKf/KO3Lj3r/lyR9/msm6/nwqMIoOgCBMGDv5hEl86ilfHL56R+sPaWc1rY56QcvCXsnTETBm9+CvpOHWQPH72VAU8YzsMME/ihy90GO33zW8/qjlzcmT2kWZV68vwVp/G22Oi+BMQrt/TFV+8EdQgQ/PBB9aTn6K6gWQARkjv379fVTBQmkJmBpDUyJs3r9VtvL29zet0H9wgGv3999/l888/Vw3D9+/fV2ksRHTaA0WwcfXqVTXhHpp2P/roI+nYsaNqRNYsXLhQpbLwZKLkM3nyZLW9pa+++kqVh+bPny/Zs2eXf/75J9JjQlYGPT3Y/6xZsyL0DqFJGceLRigMv0QZ6+OPP46V54fe8GxUOMqnAjOwRreNpvbg/2ZvjQyGj2ds/t80A0TxBfPXvNx80eZ6lJZ+GBT5rNuA2br9pi2PpaOjuOKqE2fmDNfviUE+ltOyxATKSviMRlOwJctWDHxuIkODlg70oLqykuFmwqewgWBOGwRByNboMXpGY7HUyYYpcOP7cIhixctNF/jMkmHh77h3Bh95+vSpGsASm58VBabUFY8UMc9RvA4Klcsjtql2DctjtSdzE9nM/po///xTJSLQK4MG5KicPXtWSpQoIX///bdqNUGbCR6f5UisnTt3qpHPmAZG95kbe2BI97x581Q9DsO1f/75Z9m2bZt5nhwiIiKKGgIbVwZiqJiUL18+2sAGEAChhxWT8QLaUVABCQkJMQ84wmc6Ah97AxvdBzda6Qo9MChL4cFjoj306xARERm/LOXMaClxCEYFX7582XwZ05wgOEH/jDYqGFkXzE+Hdo/w0CyMCXsxggr9OLiMZmLMRacFLmgwHjdunJrDDr2sZ86cUaOdZ86c6dCx6jq4QWMwMjVERESJTVwPBT969KgKTML3z2DKFW1qFEyXgm6XVq0i9jCi1IX16OfBJLZoHEZwY9mHg3IbTnnUq1cvlf3BqZMwQa8jw8AN2XOjZ+y5ocSAPTdkZHHZc1Poy3rikcJ6rjhHvA4KkYvD/GL1WOOLrjM3REREiRXyLk6NlhLjYnBDRESkQ3FdltITjjcmIiIiQ2HmhoiISIeYubGNwQ0REZEOMbixjcENERFRIj79ghGx54aIiIgMhZkbIiIiHWJZyjYGN0RERHrEupRNLEsRERGRoTBzQ0REpEMsS9nG4IaIiEiHWJWyjWUpIiIiMhRmboiIiHSIZSnbGNwQERHpEIMb21iWIiIiIkNh5oaIiEiHmLmxjcENERGRDnG0lG0MboiIiHSImRvb2HNDREREhsLMDRERkR65uansjTO3NyoGN0RERDrEspRtLEsRERGRoTBzQ0REpEPM3NjG4IaIiEiHOBTcNpaliIiIyFCYuSEiItIhN/znxIgnN+FoKSIiIkpA2HNjG8tSREREZCgsSxEREekQMze2MbghIiLSIY6Wso3BDRERkQ4xc2Mbe26IiIjIUBjcEBER6ZGbRW0qRos4ZM+ePdK4cWPJli2byhr99ttvVus7duxoziZpy7vvvmu1zaNHj6RNmzbi5eUl6dKlk86dO8vz58+ttjl16pRUr15dUqRIITlz5pSpU6c6/NQwuCEiItKh8IFETBZHBAYGSunSpeXbb7+1uQ2Cmdu3b5uXn3/+2Wo9ApuzZ8+Kn5+fbNy4UQVM3bp1M68PCAgQX19fyZ07txw7dkymTZsmY8eOlR9++MGhY2XPDREREUWrQYMGaolK8uTJJWvWrJGuO3/+vGzevFmOHDkib731lrpuzpw50rBhQ5k+fbrKCC1fvlxevXolixYtkmTJkknx4sXlxIkTMmPGDKsgKDrM3BAREemQu5vzi5YtsVyCg4Mlpnbt2iVZsmSRwoULS8+ePeXhw4fmdQcOHFClKC2wgbp164q7u7scOnTIvE2NGjVUYKOpX7++XLhwQR4/fmz3cTC4ISIiSsRlqZw5c0ratGnNy5QpU2J0PChJLV26VLZv3y5ffvml7N69W2V6Xr9+rdbfuXNHBT6WkiRJIhkyZFDrtG28vb2tttEua9vYg2UpIiKiRMzf3181+FqWlmKiZcuW5n+XLFlSSpUqJfnz51fZnDp16khcYuaGiIhIh9zd3JxeAIGN5RLT4Ca8fPnySaZMmeTy5cvqMnpx7t27Z7VNaGioGkGl9eng5927d6220S7b6uWJDIMbIiIiHYrr0VKOunnzpuq58fHxUZcrV64sT548UaOgNDt27JCwsDCpWLGieRuMoAoJCTFvg5FV6OFJnz693ffN4IaIiEiH3F2wOALz0WDkEha4du2a+veNGzfUuiFDhsjBgwfln3/+UX03TZs2lQIFCqiGYChatKjqy+natascPnxY9u3bJ71791blLIyUgtatW6tmYsx/gyHjq1atktmzZ8vAgQMdOlYGN0RERBSto0ePStmyZdUCCDjw79GjR4uHh4eafK9JkyZSqFAhFZyUL19e/vzzT6syF4Z6FylSRPXgYAh4tWrVrOawQUPz1q1bVeCE2w8aNEjt35Fh4MCGYiIiIh1ys+ibientHVGrVi0xmUw212/ZsiXafWBk1IoVK6LcBo3ICIqcweCGiIhIh3jiTNtYliIiIiJDYeaGiIhIhyyHc8f09kbF4IaIiEiHWJayjWUpIiIiMhRmboiIiHQoJnPVJJbshl3Bzfr16+3eIca4ExERUexiz42TwU2zZs3srv9pZ/8kIiIiSrDBDc77QERERAkHG4pjqecmKChIUqRI4cwuiIiIKAZYlnJhPxHKThMmTJDs2bNL6tSp5erVq+r6UaNGycKFCx3dHREREcWAmwsWo3I4uJk0aZIsWbJEpk6dqs7cqSlRooQsWLDA1cdHREREFLvBzdKlS9UZPNu0aaPOAqopXbq0/P33347ujoiIiJwoSzmzGJXDPTf//vuvFChQINKm45CQEFcdFxEREUXBXZw8/YIYN7hxOHNTrFixSE9FvmbNGilbtqyrjouIiIgobjI3o0ePlg4dOqgMDrI1v/76q1y4cEGVqzZu3BizoyAiIiKHcCi4CzM3TZs2lQ0bNsi2bdskVapUKtg5f/68uq5evXqO7o6IiIhiGNw402/jxp4ba9WrVxc/Pz++GYmIiMg4k/gdPXpUZWy0Ppzy5cu78riIiIgoCs7OVeNm4GfX4eDm5s2b0qpVK9m3b5+kS5dOXffkyROpUqWKrFy5UnLkyBEbx0lEREQWOEOxC3tuunTpooZ8I2vz6NEjteDfaC7GOiIiIiJdZW52794t+/fvl8KFC5uvw7/nzJmjenGIiIgo9jFz48LgJmfOnJFO1odzTmXLls3R3REREVEMYLCTMyOe3AzcdONwWWratGnSp08f1VCswb/79esn06dPd/XxERERUSR4+gUnMzfp06e3ig4DAwOlYsWKkiTJm5uHhoaqf3fq1EmaNWtmzy6JiIiI4i+4mTVrVuzcOxEREcUIh4I7GdzgdAtERESUcLChOBYm8YOgoCB59eqV1XVeXl7O7JKIiIgoboMb9NsMGzZMfvnlF3n48GGko6aIiIgodjFz48LRUkOHDpUdO3bI3LlzJXny5LJgwQIZN26cGgaOM4MTERFR3J0V3JnFqBzO3ODs3whiatWqJZ988omauK9AgQKSO3duWb58ubRp0yZ2jpSIiIgoNjI3ON1Cvnz5zP01uAzVqlWTPXv2OLo7IiIiiuEHuLOLUTn82BDYXLt2Tf27SJEiqvdGy+hoJ9IkIiKiWOZsScrNuGUph4MblKJOnjyp/j18+HD59ttvJUWKFDJgwAAZMmRIbBwjERERxbM9e/ZI48aNVY8tgqPffvvNvA6nZcJgo5IlS0qqVKnUNu3bt5dbt25Z7SNPnjwRgqwvvvjCaptTp06plhfEFjjl09SpU2O/5wZBjKZu3bry999/y7Fjx1TfTalSpRw+ACIiIkr4o6UCAwOldOnS6mwEH3zwgdW6Fy9eyPHjx2XUqFFqm8ePH6vTMjVp0sTqdE0wfvx46dq1q/lymjRpzP8OCAgQX19fFV/MmzdPTp8+re4PlaFu3brFzTw3gEZiLERERGTc4KZBgwZqiUzatGnFz8/P6rpvvvlG3n77bblx44bkypXLKpjJmjVrpPvBwCTMn7do0SJJliyZFC9eXE6cOCEzZsxwfXDz9ddf273Dvn372r0tERERxYyzw7nd/v+2yJZYwjQvWJz19OlTdR/h+3FRhpowYYIKeFq3bq0qQtq5Kg8cOCA1atRQgY2mfv368uWXX6psEM516bLgZubMmXbtDA+CwQ0REZF+5MyZ0+rymDFjZOzYseLsGQzQg9OqVSurMxcgRihXrpxkyJBB9u/fLyNGjJDbt2+rzAzcuXNH8ubNa7Uvb29v8zqXBjfa6CiKG/+uOsTTWJBheb5bKL4PgSj2hIbF2bPrLm5qceb24O/vb/WZ42zWBs3FH330kZhMJjXhr6WBAwea/40+XWRounfvLlOmTHFJtshlPTdERESk37KUl5eXy75Qa4HN9evX1dkMottvxYoVJTQ0VP755x8pXLiw6sW5e/eu1TbaZVt9OpEx8hw+REREFEe0wObSpUuybds2yZgxY7S3QbOwu7u7ZMmSRV2uXLmyGnKOfWnQqIzAx96SFDBzQ0REpENxPVrq+fPncvnyZauWFQQn6J/x8fGRFi1aqOHgGzduVCfRRo8MYD3KT2gWPnTokNSuXVuNmMJlNBO3bdvWHLigwRjnq+zcubPq2Tlz5ozMnj3b7t5fDYMbIiIiHXL7//+cub0jMF8NApPw/TMdOnRQDcjr169Xl8uUKWN1u507d6rzUaKnZuXKlWrb4OBg1TiM4MayDwdDyrdu3Sq9evWS8uXLS6ZMmWT06NEODQMHBjdEREQULQQoaBK2Jap1gFFSBw8ejPZ+0Gj8559/ijNi1HODO0UaCbWxf//9V123bNky2bt3r1MHQ0RERPZx5rxSbk42Iyd0Dgc3//vf/9SEOp6envLXX3+p1JI2Wc/kyZNj4xiJiIjIRs+NM4tRORzcTJw4UZ3vYf78+ZI0aVLz9VWrVlWNRERERETxyeGemwsXLqipkcNDE9CTJ09cdVxEREQUhTdT+MV8Rhc3A88G4/AjwyQ6lkPBNOi3yZcvn6uOi4iIiKKgQhtnylLCspQZTlOO05hjrDqakW7duqXO4jl48GDp2bMn34hERERxwc25pmIxbmzjeFlq+PDhEhYWJnXq1JEXL16oEhXGriO46dOnT+wcJREREVFsBTeI9j7//HMZMmSIKk9hxsJixYpJ6tSpHd0VERER6WQSPz2J8SR+mEoZQQ0REREZ//QLhg5uMPVyVBP/4CygRERERLoJbsKfMwJn7sSJs3ByK5xfgoiIiGKfs7MMuzFz8x9bZ+bEibDQf0NERESxz/3//3Pm9kblskeGc00tWrTIVbsjIiIiihGXnRX8wIEDkiJFClftjoiIiKLAspQLg5sPPvggwinOb9++LUePHpVRo0Y5ujsiIiKKAQY3LgxucA4pS+7u7lK4cGEZP368+Pr6Oro7IiIiovgLbl6/fi2ffPKJlCxZUtKnT+/aIyEiIiK7vTltphPz3Ihx57lxqKHYw8NDZWd49m8iIqL45cx5pdycHEZuuNFSJUqUkKtXr8bO0RAREZFdnDojuJtzsxsbLriZOHGiOknmxo0bVSNxQECA1UJERESki54bNAwPGjRIGjZsqC43adLEKqWFUVO4jL4cIiIiil08caYLgptx48ZJjx49ZOfOnfbehIiIiGKJu5u7Wpy5vST24AaZGahZs2ZsHg8RERFR3A0FN3JnNRERkZ5wEj8XBTeFChWKNsB59OiRI7skIiIiJ7puYs7NsM+7Q8EN+m7Cz1BMREREpNvgpmXLlpIlS5bYOxoiIiKyi7Nz1bgbuNXE7uCG/TZEREQJB4eC2+bu6GgpIiIiIkNkbsLCwmL3SIiIiMhu7m7OlZbcjVuVcqznhoiIiBIGNzd3tThze6NicENERKRD7LmxzbhhGxEREbnMnj17pHHjxpItWzY1yOi3336L0Js7evRo8fHxEU9PT6lbt65cunQpwlx4bdq0ES8vL0mXLp107txZnj9/brXNqVOnpHr16pIiRQrJmTOnTJ061eFjZXBDRESk46HgziyOCAwMlNKlS8u3334b6XoEIV9//bXMmzdPDh06JKlSpZL69etLUFCQeRsENmfPnhU/Pz/ZuHGjCpi6detmXh8QECC+vr6SO3duOXbsmEybNk3Gjh0rP/zwg0PHyrIUERGRDsX16RcaNGiglsggazNr1iwZOXKkNG3aVF23dOlS8fb2VhkezJN3/vx52bx5sxw5ckTeeusttc2cOXOkYcOGMn36dJURWr58ubx69UoWLVokyZIlk+LFi8uJEydkxowZVkFQdJi5ISIiIqdcu3ZN7ty5o0pRGpzRoGLFinLgwAF1GT9RitICG8D27u7uKtOjbVOjRg0V2GiQ/blw4YI8fvzY7uNh5oaIiEiH3MVNLc7cXisFWUqePLlaHIHABpCpsYTL2jr8DH+WgyRJkkiGDBmstsmbN2+EfWjr0qdPb9fxMHNDRESk47KUMwugaRdZFm2ZMmWK6B0zN0RERImYv7+/Gr2kcTRrA1mzZlU/7969q0ZLaXC5TJky5m3u3btndbvQ0FA1gkq7PX7iNpa0y9o29mDmhoiISMeT+DmzAAIbyyUmwQ1KSQg+tm/fbr4O5S700lSuXFldxs8nT56oUVCaHTt2qDMgoDdH2wYjqEJCQszbYGRV4cKF7S5JAYMbIiIiHffcOLM4AvPRYOQSFq2JGP++ceOGKnH1799fJk6cKOvXr5fTp09L+/bt1QioZs2aqe2LFi0q7777rnTt2lUOHz4s+/btk969e6uRVNgOWrdurZqJMf8NhoyvWrVKZs+eLQMHDnToWFmWIiIiomgdPXpUateubb6sBRwdOnSQJUuWyNChQ9VcOBiyjQxNtWrV1NBvTManwVBvBDR16tRRo6SaN2+u5sbRoOdn69at0qtXLylfvrxkypRJTQzoyDBwcDPxdN8JBlJ4eGH/fWBd/yQykjQNi8X3IRDFntAwkV235enTp7H2d1z7rPjh+Dfimdozxvt5+fyldCvXO1aPNb4wc0NERKTjs0s5c3ujYnBDRESk19DGmRmKxbjBDRuKiYiIyFCYuSEiIkrEMxQbEYMbIiIiHbKcqyamtzcq4z4yIiIiSpSYuSEiItLtWCk2FEeGwQ0REZEOYaCUU6Ol3MSwWJYiIiIiQ2HmhoiISIdYlrKNwQ0REZEOoSTlXFnKTYyKZSkiIiIyFGZuiIiIdIiT+NnG4IaIiEiHWJayjcENERGRDr05+YITMxSLcTtTjPvIiIiIKFFi5oaIiEiHWJayjcENERGRDnGeG9tYliIiIiJDYeaGiIhIh9zd3NTizO2NisENERGRDrEsZRvLUkRERGQozNwQERHpEEdL2cbghoiISJecm8RPDFy8Me4jIyIiokSJmRsiIiIdYlnKNgY3REREOsSzgtvG4IaIiEiHmLmxjT03REREZCjM3BAREekQJ/GzjcENERGRDrEsZRvLUkRERGQozNwQERHptiwV8xyFmxj3xJnM3BAREen4rODOLI7IkyePuRRmufTq1Uutr1WrVoR1PXr0sNrHjRs3pFGjRpIyZUrJkiWLDBkyREJDQ8XVmLkhIiKiaB05ckRev35tvnzmzBmpV6+efPjhh+brunbtKuPHjzdfRhCjwW0R2GTNmlX2798vt2/flvbt20vSpEll8uTJ4koMboiIiHQorkdLZc6c2eryF198Ifnz55eaNWtaBTMIXiKzdetWOXfunGzbtk28vb2lTJkyMmHCBBk2bJiMHTtWkiVLJq7CshQREZEORVYicnSBgIAAqyU4OFii8+rVK/npp5+kU6dO5v3A8uXLJVOmTFKiRAkZMWKEvHjxwrzuwIEDUrJkSRXYaOrXr6/u8+zZs+JKzNwQERElYjlz5rS6PGbMGJVJicpvv/0mT548kY4dO5qva926teTOnVuyZcsmp06dUhmZCxcuyK+//qrW37lzxyqwAe0y1rlSogtudu3aJbVr15bHjx9LunTpomyc6t+/v1pIP6av+kE27N8mF29elRTJUkjFomVkfKdBUihHXvM2feeMkV1/HZTbj+5JqhQppWKxMjL+k0FSOGc+8zZpGhaLsO/Fw6ZLi5oN4+yxEMHgj7tLs6q+6j388lWwHDr3l3y+aJpcunnN/ATN6Tte3ilTRXwyZpHnL1/IwfPHZeTC6er3QJMzs4/M7jNOapaqKM+DXsjybWtl1KKv5HXYmx6KKsXLy8ROg6VQznySMrmn3Lh3Sxb+vlLmrF3CF8LgZSl/f3/x8vIyX588efJob7tw4UJp0KCBCmQ03bp1M/8bGRofHx+pU6eOXLlyRZWv4lKiC26qVKmimpjSpk2rLi9ZskQFMIhAwzdOpUqVKp6OkmJq35mj0vW9VlK+UAkJff1axv44S5p93kWOfL9BBTJQpkBx+ahWY8mZxUceP3sqk5d/K81GdpEzi/zEw8PDvK+5AyZJvfLVzJfTpv7vl58orlQvWUHmbfhJjl08LUnck8i4TwbKxkmLpGy3hvIi+KXa5q9LZ2XljvXif/+2ZEiTVj5v20c2Tl4kRTq+I2FhYeLu7i6/jv9B7j5+ILUHtpSsGTLLgsFTJSQ0VMYsmaH2ERj0Qt3P6WsXJDDopQp2vuk7Xv170R+r+IIbeBI/Ly8vq+AmOtevX1d9M1pGxpaKFSuqn5cvX1bBDXpxDh8+bLXN3bt31U9bfToxleiCGzQs2fMkhm+cIn1YO+EHq8vzBk6WfK2qyV+Xzkm1km+p6zo1+Mi8Prd3dhndvq9U7vW+XL/3r+TzyWVelzZVGvHOwPcBxa+mI7tYXe721TDxX3VIyhYsroJ5sAw+btz9V8b9OEuOzN2g3t/XbvtL3XLVpGiuAtJoREe59+ShnLp6XsYvnSUTOw+RiT/NkZDQEDl55bxaLPeDjFHVEm8xuEmg3P//P2duHxOLFy9Ww7gx8ikqJ06cUD+RwYHKlSvLpEmT5N69e+r24OfnpwKrYsUiZssN11CMsfK9e/dWCzIsaE4aNWqUmEwmtR4lJQwfS58+verMRmrs0qVLVlFl48aN1XpkX4oXLy6///67uSyFaBWZGvz7k08+kadPn5ojYK3OiLLUrFmzzHXEjz/+2OoYQ0JC1HEtXbpUXca3oylTpkjevHnF09NTSpcuLWvWrImz54wiFxD4TP3Et9nI4NvqT35rJU/WHJIjk3XQO2juRMndsorU6v+xLN36P/P7jyg+eaVMo34i6xgZlJTa1/tABTU377/pY0B59sw/F1Vgo/E7tlcF8MVyF4h0P6XzF5WKRcvKn6etv2lT4hYWFqaCmw4dOkiSJP/lR1B6wsinY8eOyT///CPr169Xn9M1atSQUqVKqW18fX1VENOuXTs5efKkbNmyRUaOHKnmybGnFGaIzM2PP/4onTt3Vimso0ePqlperly51Bh6NDAhmMGTh4gPTUsNGzZUQ8wwXh5PFDq59+zZo4IbXJ86depIS1QIYEaPHq2aniCy7dq0aaPG8T9//ty8Hi8KusDff/99dRmBDTrH582bJwULFlT33bZtW5UBshwmZwkd6ZZd6egYJ9f+Eg77/gupVKycFMtT0Grd/I0/y6hF01XKvWCOvLJu0gJJlvS/YYgj2/aRGqUrSsoUKWT78f0y8NsJEvjyhfRs2o4vEcUbfAGb1uNz2X/2mJy7/t8XOuj2XmuZ1HmIpPZMJRf8r0qjzzqqjAx4p88s9548sNpeu4x1Iv9lbC4v2yOZ0maQJB4eMnH5HFmyeXWcPDbSx7mltm3bpibiwyip8FURrMNnamBgoGpSbt68uQpeNCj7b9y4UXr27KmyOPh8RpBkOS+O4YMbPDEzZ85UT37hwoXl9OnT6jKyOghq9u3bp4ITbegZtkf3NoIQPPF4UtHQBPny/dcoGv7FQGYI9xFVqQpD1fAirF27VkWcsGLFCmnSpImkSZNGBSiYgAgvLF4w7T737t0r33//vc3gBgHRuHHjnH6uKHIDv5sg569fkq3Tf4qw7qPa70ntspXlzqMH8vWvi6XDlIHiN325pEj25tvDsNY9zduWzl9MXgS9lNn/W8zghuLVrF5jpHieglJnUKsI69Bzs/34PtVP079FZ/nps9nyzsCWEhzyyqH7qDO4taT2TClvFykjEzoNkqu3rssvuza58FGQns8K7uvrG2kWG5/Bu3fvjvb2GE2lVVJiU4IsS0GlSpWsokoEDcjWIAuDVJjWqAQZM2ZUAdD582++ffTt21cmTpwoVatWVUPaMCTNGbi/jz76SAVRgKh03bp1KqOjNUshi4OZGpHZ0RaUrJCqswVzAKAkpi3oWCfXGPTdRNl8eLds+mKJZA9XbgKk4wtkz6P6cH76bKZc9L+mRlnZ8lbhUvLvgzsOf1AQucrMT0dLw4q1pf7Q9vLvgzdNmJYCXjyXK7euqz6c1hP7qtF/TavWU+vuPr4vWdJlstpeu4x1lq7fvSln/7koizf/okZKoTmZSG8SbObGGV26dFHZlk2bNqkZEZEh+eqrr6RPn5j/kiKQQQYGjVBogEJfzbvvvqvWoVwFuL/s2bNb3S6qOiLWubrOmNjhG8XguZNkw4Ft8vsXS1QvTbS3UYspysDl9NXzkj61lyS3KF0RxWVg06RKPfEd2lYFH9HB90J8K9dKrYfOn5BhLXtK5rQZ5P7TR+q6OuWqytPAZ3L+xmWb+3F3c+d7PiFzsiwlztw2gUuwwc2hQ4esLh88eFD1sqAZCSfZwnqtLPXw4UPVM2PZbY0UGU7YhQUZkvnz50ca3KA0ZXmuDFtwX9jnqlWr5I8//lDlL/T3AO4XQQrKYbZKUBR3pajVuzbJytHfSBrPVHL30ZtvpV6p0ohn8hSqyfJ/e/5Qf9gzpU2vvgHPWL1AlaPqV6ihtv390E659/ihvF2ktCRPlkx2/nVApq+aL32b/zdZFVFclqI+rt1YPhzXU56/DBTv9G8yLghMgl4FS56sOdX8S9uP7ZUHTx+pTOWgj7vJy1dBsuXwmzLBtuN7VRCzcOg0+XzBNPHOkEnGdOgv329YLq9C3vTldG/cRvzv3VL9OlCtZAXp37yzfLf+zaAJSnjioyylFwk2uEGgMHDgQOnevbscP35c5syZo7IvCHCaNm2qGovRz4Kel+HDh6uMCa4HzFuDEVSFChVSI6t27twpRYsWjfR+MCoKmZft27erEU4YfWV5oi9LGDWFhuGLFy+qfWpwDIMHD5YBAwaoJtZq1aqpMhP6gtDwjIYpihsLNq1UPxsMs37OMWdN23rvqyDmwNlj8t26ZfLk+VOVmq9aorxs+2qFZE6XUW2b1COJzN+4QkbM/0JlgvJlyyVTug6Vju/+d3I4oriCoAP8pr0pi2u6fjVMjfQLfhUsVYu/Jb2bdVDZRYyI2nv6iJrPRsvS4O9S8zHdZXbvcbJr5irVSI9J/MYvnW2VpcFklsh2Yo6oq7dvyMhF02TB729+p4j0xM2UAMe3omkYw7fxC4nGXXRYo7safTRIwSFg6devn2osxqgoDDVD8IPAB5ChQXbl5s2bKrhA+QjNyOjNiWyGYux79erVKgOkTTsd2QzF6OlBlgYNUdeuXbNKB+Jp/Prrr2Xu3Lly9epVte9y5crJZ599po7PHhgthQbnfx9YzxZJZCSRzf5MZBihYSK7bqsvuLH1d1z7rNh5ZYukThPzyWafPwuU2vnrx+qxxpcEG9zgbKHaPDOJBYMbSgwY3JChxWVwc3Wr88FNPl9DBjcJdrQUERERkaF6boiIiMg2NhTrLLhBXwwRERElrBmK9SJBBjdEREQUNWZubGPPDRERERkKMzdEREQ6hKKSc5P4GReDGyIiIr2WpZzpuRHjhjcsSxEREZGhMHNDRESkQ2woto3BDRERkQ4xuLGNZSkiIiIyFGZuiIiIdIiT+NnG4IaIiEiHWJayjWUpIiIiMhRmboiIiHSIZSnbGNwQERHpEMtStjG4ISIi0iEGN7ax54aIiIgMhZkbIiIiHWLPjW0MboiIiHSIZSnbWJYiIiIiQ2HmhoiISIeYubGNwQ0REZEeubmpvhtnbm9ULEsRERGRoTBzQ0REpEvIvDiTfXETo2JwQ0REpEMcCm4by1JEREQUrbFjx5oDKm0pUqSIeX1QUJD06tVLMmbMKKlTp5bmzZvL3bt3rfZx48YNadSokaRMmVKyZMkiQ4YMkdDQUHE1Zm6IiIh0KD5GSxUvXly2bdtmvpwkyX9hxIABA2TTpk2yevVqSZs2rfTu3Vs++OAD2bdvn1r/+vVrFdhkzZpV9u/fL7dv35b27dtL0qRJZfLkyeJKDG6IiIh0KD6CmyRJkqjgJLynT5/KwoULZcWKFfLOO++o6xYvXixFixaVgwcPSqVKlWTr1q1y7tw5FRx5e3tLmTJlZMKECTJs2DCVFUqWLJm4CstSREREOhS+RBSTxVGXLl2SbNmySb58+aRNmzaqzATHjh2TkJAQqVu3rnlblKxy5colBw4cUJfxs2TJkiqw0dSvX18CAgLk7Nmz4krM3BARESViAQEBVpeTJ0+ulvAqVqwoS5YskcKFC6uS0rhx46R69epy5swZuXPnjsq8pEuXzuo2CGSwDvDTMrDR1mvrXInBDRERkW4HgjtTlnojZ86cYmnMmDGqTBRegwYNzP8uVaqUCnZy584tv/zyi3h6ekpCwuCGiIgoEffc+Pv7i5eXl/n6yLI2kUGWplChQnL58mWpV6+evHr1Sp48eWKVvcFoKa1HBz8PHz5stQ9tNFVkfTzOYM8NERFRIubl5WW12BvcPH/+XK5cuSI+Pj5Svnx5Nepp+/bt5vUXLlxQPTmVK1dWl/Hz9OnTcu/ePfM2fn5+6j6LFSvm0sfEzA0REZEOxfUkfoMHD5bGjRurUtStW7dU+crDw0NatWqlhn537txZBg4cKBkyZFABS58+fVRAg5FS4Ovrq4KYdu3aydSpU1WfzciRI9XcOPYGVPZicENERKRDcT0U/ObNmyqQefjwoWTOnFmqVaumhnnj3zBz5kxxd3dXk/cFBwerkVDfffed+fYIhDZu3Cg9e/ZUQU+qVKmkQ4cOMn78+Bg/BpuPzWQymVy+V4pxxzqi338fWNc/iYwkTUPXpp+JEpTQMJFdt9W8L7H1d1z7rDh366Sk8UoT4/08C3gmxbKVjtVjjS/M3BAREekQzy1lG4MbIiIiHYqPGYr1gqOliIiIyFCYuSEiItLxNH7O3d6YGNwQERHpEEMb2xjcEBER6RAbim1jzw0REREZCjM3REREusTClC0MboiIiHSIoY1tLEsRERGRoTBzQ0REpEvM3djC4IaIiEiHOFrKNpaliIiIyFAY3BAREZGhsCxFRESkQzxxpm3M3BAREZGhMHNDRESkQ8zc2MbMDRERERkKMzdEREQ6xKHgtjFzQ0RERIbC4IaIiIgMhWUpIiIiHbcUO3N7o2LmhoiIiAyFmRsiIiJd4okzbWFwQ0REpEMMbWxjWYqIiIgMhZkbIiIiHeI8N7YxuCEiItIlFqZsYVmKiIiIDIWZGyIiIh1i3sY2BjdERES6ZdyJ+JzB4IaIiEiH2FBsG3tuiIiIyFAY3BAREVG0pkyZIhUqVJA0adJIlixZpFmzZnLhwgWrbWrVqmXOKGlLjx49rLa5ceOGNGrUSFKmTKn2M2TIEAkNDRVXYlmKiIhIt6fNjHnPjZuDt929e7f06tVLBTgIRj777DPx9fWVc+fOSapUqczbde3aVcaPH2++jCBG8/r1axXYZM2aVfbv3y+3b9+W9u3bS9KkSWXy5MniKgxuiIiIKFqbN2+2urxkyRKVeTl27JjUqFHDKphB8BKZrVu3qmBo27Zt4u3tLWXKlJEJEybIsGHDZOzYsZIsWTJxBZaliIiIdMnNBYtIQECA1RIcHGzXvT99+lT9zJAhg9X1y5cvl0yZMkmJEiVkxIgR8uLFC/O6AwcOSMmSJVVgo6lfv76637Nnz7roeWHmhoiIKFHPc5MzZ06r68eMGaOyKFEJCwuT/v37S9WqVVUQo2ndurXkzp1bsmXLJqdOnVIZGfTl/Prrr2r9nTt3rAIb0C5jnauwLEVERJSI+fv7i5eXl/ly8uTJo70Nem/OnDkje/futbq+W7du5n8jQ+Pj4yN16tSRK1euSP78+SWusCxFRESkQ+FHJcVkAQQ2lkt0wU3v3r1l48aNsnPnTsmRI0eU21asWFH9vHz5svqJXpy7d+9abaNdttWnExMMboiIiBJxz429TCaTCmzWrl0rO3bskLx580Z7mxMnTqifyOBA5cqV5fTp03Lv3j3zNn5+fiqoKlasmLgKy1JERERkVylqxYoVsm7dOjXXjdYjkzZtWvH09FSlJ6xv2LChZMyYUfXcDBgwQI2kKlWqlNoWQ8cRxLRr106mTp2q9jFy5Ei1b3vKYfZi5oaIiEiH4jZvIzJ37lw1QgoT9SEToy2rVq1S6zGMG0O8EcAUKVJEBg0aJM2bN5cNGzaY9+Hh4aFKWviJLE7btm3VPDeW8+K4AjM3REREuhS35wU3mUxRrseoK0z0Fx2Mpvr9998lNjG4ISIi0iGeONM2lqWIiIjIUBjcEBERkaGwLEVERKRDcX3iTD1hcJOAaM1az549i+9DIYo9oWF8dsnw7+/omm9dISDgWbzePiFjcJOAaEFNkbyum8iIiIji5+855n+JDRhyjdl8C+Yp5PS+smbN6rIzcSckbqa4CC/JLjgR2a1bt9TkSNq02BS7cCZaDF8Mf24VIiPg+zvu4SMVgQ1OHOnuHnttrUFBQfLq1Sun95MsWTJJkSKFGA0zNwkIfhGiO08HxQ7tnCpERsT3d9yKrYyNJQQkRgxKXIWjpYiIiMhQGNwQERGRoTC4oUQNJ2obM2aMS0/YRpRQ8P1NiRUbiomIiMhQmLkhIiIiQ2FwQ0RERIbC4IaIiIgMhcENkR3Gjh0rZcqU4XNFurBr1y41EeiTJ0+i3C5Pnjwya9asODsuorjChmKi8L8Ubm6ydu1aadasmfm658+fS3BwsGTMmJHPFyV4mLn20aNH4u3trd7PS5Yskf79+0cIdu7fvy+pUqWSlClTxtuxEsUGzlBMZIfUqVOrhUgPtHMPRSdz5sxxcjxEcY1lKUowatWqJX379pWhQ4dKhgwZ1B9nlIM0+NbZpUsX9QcZ08m/8847cvLkSat9TJw4UbJkyaLOz4Vthw8fblVOOnLkiNSrV08yZcqkpkivWbOmHD9+3CpND++//776xqtdtixLbd26VU17Hv5bcL9+/dQxafbu3SvVq1cXT09Pdf4qPLbAwECXP2+k3/d779691YL3It6To0aNMp9N+vHjx9K+fXtJnz69yqw0aNBALl26ZL799evXpXHjxmo9si/FixeX33//PUJZCv/+5JNP5OnTp+o6LNrvlWVZqnXr1vLxxx9bHWNISIg6rqVLl5rPfzdlyhTJmzevel+XLl1a1qxZE2fPGZG9GNxQgvLjjz+qP9SHDh2SqVOnyvjx48XPz0+t+/DDD+XevXvyxx9/yLFjx6RcuXJSp04dlX6H5cuXy6RJk+TLL79U63PlyiVz58612j9OaNehQwcVeBw8eFAKFiwoDRs2NJ+RHcEPLF68WG7fvm2+bAn3mS5dOvnf//5nvu7169eyatUqadOmjbp85coVeffdd6V58+Zy6tQptQ73iQ8yIsv3e5IkSeTw4cMye/ZsmTFjhixYsECt69ixoxw9elTWr18vBw4cUEEP3qsIOKBXr16qVLpnzx45ffq0et9Hll2sUqWKCmDwhQDvaSyDBw+OsB3euxs2bFAlWM2WLVvkxYsXKtgHBDYIdObNmydnz56VAQMGSNu2bWX37t18USlhwVnBiRKCmjVrmqpVq2Z1XYUKFUzDhg0z/fnnnyYvLy9TUFCQ1fr8+fObvv/+e/XvihUrmnr16mW1vmrVqqbSpUvbvM/Xr1+b0qRJY9qwYYP5OvxarF271mq7MWPGWO2nX79+pnfeecd8ecuWLabkyZObHj9+rC537tzZ1K1bN6t94DG4u7ubXr58adfzQcZ/vxctWtQUFhZmvg7vdVx38eJF9T7ct2+fed2DBw9Mnp6epl9++UVdLlmypGns2LGR7nvnzp3q9tr7cfHixaa0adNG2C537tymmTNnqn+HhISYMmXKZFq6dKl5fatWrUwff/yx+jd+91KmTGnav3+/1T7wXsd2RAkJMzeUoJQqVcrqso+Pj8rWoPyEb5Ro6NX6X7Bcu3ZNZUngwoUL8vbbb1vdPvzlu3fvSteuXVXGBqUAfJvFfm/cuOHQceJbLtL9t27dMmeNGjVqpDI6gONFE6flsdavX1+l9XHMRFCpUiVVJtJUrlxZlZ7OnTunMjoVK1Y0r8N7v3DhwnL+/Hl1GWVOlGGrVq2qTiGCDKEzcH8fffSRei8DSqjr1q0zZyMvX76ssjgo61q+r5HJ0X4HiRIKNhRTgpI0aVKry/jDj4AAAQgCHQQU4WkBhT1Qknr48KEqAeTOnVudewcfKBhd4ogKFSpI/vz5ZeXKldKzZ081ugrBjAbH2717d/UBFB7KZUTOQk8ZAuZNmzapPjCUjL766ivp06dPjPeJQAZ9aPhCgXIw+mpQXgWtXIX7y549u9XteG42SmgY3JAuoL/mzp076tul1uQbHr7VokcGTZia8D0z+/btk++++071LoC/v788ePAgQoCFHhp7PgjwLTdHjhzi7u6uMjeWx4tv3wUKFHD4sVLigd4yS1ofWLFixSQ0NFStR88MIChHdhLrNGhU79Gjh1pGjBgh8+fPjzS4wegpe97TuC/sEz1i6G1Dn5v2hQP3iyAGWU4EQEQJGctSpAt169ZVGRbMPYNvqf/884/s379fPv/8c9V0CfijvnDhQtWkidQ+UvZI1Vum/fHBsWzZMpXaxwcHAhR8O7WE4Gn79u0qmMKIFVtwW4y0QhNzixYtrL69Dhs2TB0fGohPnDihjgcpfjYUkyUECgMHDlRBy88//yxz5sxRo+7wPm3atKkqoaIRHWVONO4iY4LrAfPWoOEXZU68D3fu3ClFixaN9AnGexqZF7yvEcyjvGQLRk2hYRiZG60kBRiBiEZkNBHjdwylKNwvjhmXiRISBjekCwhQMMy1Ro0aalhroUKFpGXLlmo4LCYqA/whxrdX/AFG5gR/9DHiBMO2NQh+ELBgfbt27VTZCEPHLSG1jz/s+AZbtmxZm8eErAx6ehBAWX4IaL1DGEFy8eJFNRwc+xk9erRky5bN5c8N6ReyjC9fvlTvI4x+QmDTrVs384i98uXLy3vvvacCe/S643dAy6QgE4PbIKBB6Qi/E8hK2srIILuDod6YSgEjEW3BexlZRwRS6OexNGHCBDVcHSUw7X5RpsLQcKKEhDMUk6Gh+RHz5SBbQ5TQ5rnB3Ek8/QGR67HnhgwDqXak09Fk6eHhodL827ZtM8+TQ0REiQODGzJc6Qo9MEFBQarBGBPtoV+HiIgSD5aliIiIyFDYUExERESGwuCGiIiIDIXBDRERERkKgxsiIiIyFAY3RGQFEx9iJmjL+VgwG25cw3nEMALuyZMnNrfB+t9++83ufY4dO1bNLeMMzI6N+8XM00SUMDG4IdJJwIEPVCw4TxBmRx4/frw6/1Bs+/XXX9XMtK4KSIiIYhvnuSHSCUx1jyn5g4OD1Xw+mHofU/HjlBPh4SznCIJcIUOGDC7ZDxFRXGHmhkgncGJOnEoid+7c0rNnTzU54fr1661KSZjAEOevwgSG2lnPP/roI0mXLp0KUnDSRZRVNDg/EU7ciPUZM2aUoUOHqnMYWQpflkJwhROD4txbOCZkkXDOLuy3du3aapv06dOrDA6OC8LCwtT5iHAOIpyotHTp0rJmzRqr+0HAhvMjYT32Y3mc9sJxYR8pU6aUfPnyqfMghYSERNju+++/V8eP7fD8PH361Gr9ggUL1LmTcF6yIkWK2DxnExElTAxuiHQKQQAyNBqc8Rlnl8bpJjZu3Kg+1HEqCpzN+c8//5R9+/ZJ6tSpVQZIux1OErpkyRJZtGiROvv0o0ePZO3atdGe7BGntvj666/V2dURKGC/CBYwIzTgOG7fvi2zZ89WlxHYLF26VJ0e4+zZs+rM0jjLNU4uqgVhH3zwgTRu3Fj1snTp0kWGDx/u8HOCx4rHgxM/4r7nz58vM2fOtNrm8uXL8ssvv8iGDRtk8+bN8tdff8mnn35qXr98+XJ1klMEinh8kydPVkESz3xNpCMmIkrwOnToYGratKn6d1hYmMnPz8+UPHly0+DBg83rvb29TcHBwebbLFu2zFS4cGG1vQbrPT09TVu2bFGXfXx8TFOnTjWvDwkJMeXIkcN8X1CzZk1Tv3791L8vXLiAtI66/8js3LlTrX/8+LH5uqCgIFPKlClN+/fvt9q2c+fOplatWql/jxgxwlSsWDGr9cOGDYuwr/Cwfu3atTbXT5s2zVS+fHnz5TFjxpg8PDxMN2/eNF/3xx9/mNzd3U23b99Wl/Pnz29asWKF1X4mTJhgqly5svr3tWvX1P3+9ddfNu+XiOIXe26IdALZGGRIkJFBmad169Zq9I+mZMmSVn02J0+eVFkKZDMs4bxbV65cUaUYZFcqVqxoXpckSRJ56623IpSmNMiq4KSkNWvWtPu4cQw4qSnO0G4J2aOyZcuqfyNDYnkcULlyZXHUqlWrVEYJj+/58+eq4drLy8tqm1y5ckn27Nmt7gfPJ7JNeK5w286dO0vXrl3N22A/adOmdfh4iCh+MLgh0gn0ocydO1cFMOirQSBiKVWqVFaX8eFevnx5VWYJL3PmzDEuhTkKxwGbNm2yCioAPTuucuDAAWnTpo2MGzdOleMQjKxcuVKV3hw9VpSzwgdbCOqISB8Y3BDpBIIXNO/aq1y5ciqTkSVLlgjZC42Pj48cOnRIatSoYc5QHDt2TN02MsgOIcuBXpnIzrauZY7QqKwpVqyYCmJu3LhhM+OD5l2tOVpz8OBBccT+/ftVs/Xnn39uvu769esRtsNx3Lp1SwWI2v24u7urJmxvb291/dWrV1WgRET6xIZiIoPCh3OmTJnUCCk0FF+7dk3NQ9O3b1+5efOm2qZfv37yxRdfqInw/v77b9VYG9UcNXny5JEOHTpIp06d1G20faJBFxBcYJQUSmj3799XmRCUegYPHqyaiNGUi7LP8ePHZc6cOeYm3R49esilS5dkyJAhqjy0YsUK1RjsiIIFC6rABdka3AfKU5E1R2MEFB4DynZ4XvB8YMQURqIBMj9ogMbtL168KKdPn1ZD8GfMmOHQ8RBR/GFwQ2RQGOa8Z88e1WOCkUjIjqCXBD03WiZn0KBB0q5dO/Vhj94TBCLvv/9+lPtFaaxFixYqEMIwafSmBAYGqnUoOyE4wEgnZEF69+6trsckgBhxhKABx4ERWyhTYWg44Bgx0goBE4aJY1QVRik5okmTJiqAwn1iFmJkcnCf4SH7heejYcOG4uvrK6VKlbIa6o2RWhgKjoAGmSpkmxBoacdKRAmfG7qK4/sgiIiIiFyFmRsiIiIyFAY3REREZCgMboiIiMhQGNwQERGRoTC4ISIiIkNhcENERESGwuCGiIiIDIXBDRERERkKgxsiIiIyFAY3REREZCgMboiIiMhQGNwQERGRGMn/AUpCN/ObhbS7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_test_bpe = trainer_bpe.predict(tokenized_bpe[\"test\"])\n",
    "logits_test_bpe = pred_test_bpe.predictions\n",
    "labels_test_bpe = pred_test_bpe.label_ids\n",
    "pred_labels_bpe = logits_test_bpe.argmax(axis=-1)\n",
    "\n",
    "acc_test_bpe = metric_acc.compute(predictions=pred_labels_bpe, references=labels_test_bpe)\n",
    "f1_test_bpe = metric_f1.compute(predictions=pred_labels_bpe, references=labels_test_bpe, average=\"macro\")\n",
    "\n",
    "print(\"DistilRoBERTa Test Accuracy:\", acc_test_bpe)\n",
    "print(\"DistilRoBERTa Test Macro-F1:\", f1_test_bpe)\n",
    "cm_bpe = confusion_matrix(labels_test_bpe, pred_labels_bpe)\n",
    "\n",
    "print(\"\\n=== Confusion Matrix (Text) ===\")\n",
    "print(f\"{'':20} {'Predicted Negative':>20} {'Predicted Positive':>20}\")\n",
    "print(f\"{'Actual Negative':20} {cm_bpe[0, 0]:>20} {cm_bpe[0, 1]:>20}\")\n",
    "print(f\"{'Actual Positive':20} {cm_bpe[1, 0]:>20} {cm_bpe[1, 1]:>20}\")\n",
    "\n",
    "disp_bpe = ConfusionMatrixDisplay(confusion_matrix=cm_bpe, display_labels=[\"negative\", \"positive\"])\n",
    "disp_bpe.plot(cmap=\"Greens\")\n",
    "plt.title(\"DistilRoBERTa Confusion Matrix (Test)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b660de5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I747530/Documents/NLP_project/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Example 1 (index=0)\n",
      "Gold label      : negative\n",
      "Predicted label : negative  (p = 0.987)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "the tortuous emotional impact is degrading, whether adult or adolescent the personal values shown in this movie belong in a bad psychodrama if anywhere at all. this movie has a plot, but it is all evil from start to end. this is no way for people to act and degrades both sexes all the way through th...\n",
      "\n",
      "====================================================================================================\n",
      "Example 2 (index=1)\n",
      "Gold label      : negative\n",
      "Predicted label : negative  (p = 0.985)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "anyone who knows anything about evolution wouldn't even need to see the film to say \"fake\". \"it's never been disproved\" also is a weak argument. saying the universe was created by a giant hippo cannot be disproved. although, to be fair, it does seem like the only people who do believe are the same p...\n",
      "\n",
      "====================================================================================================\n",
      "Example 3 (index=2)\n",
      "Gold label      : negative\n",
      "Predicted label : negative  (p = 0.986)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "i'm glad i rented this movie for one reason: its shortcomings made me want to read allende's book and get the full story. pros: the movie is beautiful, the period is depicted well and consistently (to the best of my knowledge), and meryl and glenn do good jobs. cons: this is the worst acting job i'v...\n",
      "\n",
      "====================================================================================================\n",
      "Example 4 (index=3)\n",
      "Gold label      : negative\n",
      "Predicted label : positive  (p = 0.824)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes, the votes are in. this film may very well be the plan 9 from outer space for our generation. but whereas ed wood's film, for all its flaws, retains a certain charm despite it all, this film defines the word \"charmless\" to the nth degree. in fact, i'd suggest to the editors of the oxford english...\n",
      "\n",
      "====================================================================================================\n",
      "Example 5 (index=4)\n",
      "Gold label      : negative\n",
      "Predicted label : positive  (p = 0.956)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "this mini-series is actually more entertaining than some others with much bigger budgets and grander aspirations. sotd falls somewhere between \"kung-fu\" and \"h r pufnstuff\" on the entertainment spectrum. if it weren't so long (nearly 3 hours) i think that kids would like it quite a bit. it's got adv...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Label id ↔ name mapping\n",
    "id2label = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "def show_minilm_predictions(\n",
    "    num_examples: int = 5,\n",
    "    split: str = \"test\",\n",
    "    max_review_chars: int = 300,\n",
    "):\n",
    "    \"\"\"\n",
    "    Print a few test reviews with:\n",
    "    - gold label\n",
    "    - predicted label\n",
    "    - prediction probability\n",
    "    \"\"\"\n",
    "    # HuggingFace test datasets\n",
    "    ds_text = hf_dataset[split]          # original text + labels\n",
    "    ds_tok  = tokenized_minilm[split]    # tokenized for MiniLM\n",
    "\n",
    "    # Take first `num_examples` (or sample randomly if you prefer)\n",
    "    indices = list(range(min(num_examples, len(ds_tok))))\n",
    "    small_tok_ds = ds_tok.select(indices)\n",
    "\n",
    "    # Use Trainer to predict on this small subset\n",
    "    preds = trainer_minilm.predict(small_tok_ds)\n",
    "    logits = preds.predictions           # shape (N, 2) for binary classification\n",
    "    probs = F.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        ex = ds_text[idx]\n",
    "        text = ex[\"review_clean_transformer\"]\n",
    "        gold = id2label[ex[\"label\"]]\n",
    "        pred_id = int(probs[i].argmax())\n",
    "        pred_label = id2label[pred_id]\n",
    "        pred_prob = float(probs[i][pred_id])\n",
    "\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"Example {i+1} (index={idx})\")\n",
    "        print(f\"Gold label      : {gold}\")\n",
    "        print(f\"Predicted label : {pred_label}  (p = {pred_prob:.3f})\")\n",
    "        print(\"-\" * 100)\n",
    "        # Show truncated text for readability\n",
    "        if len(text) > max_review_chars:\n",
    "            print(text[:max_review_chars] + \"...\")\n",
    "        else:\n",
    "            print(text)\n",
    "        print()  # blank line\n",
    "\n",
    "show_minilm_predictions(num_examples=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca19b1d",
   "metadata": {},
   "source": [
    "## Task 4: Model Evaluation & Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59cfa5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "N_SAMPLE = 100\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "indices_100 = random.sample(range(len(hf_dataset[\"test\"])), k=N_SAMPLE)\n",
    "\n",
    "sample_text_ds = hf_dataset[\"test\"].select(indices_100)          # original text + gold labels\n",
    "sample_tok_ds   = tokenized_minilm[\"test\"].select(indices_100)   # tokenized for MiniLM\n",
    "\n",
    "len(sample_text_ds), len(sample_tok_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88bbc130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I747530/Documents/NLP_project/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniLM on 100-sample subset:\n",
      "  Accuracy:   0.860\n",
      "  Macro-F1:   0.858\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Run MiniLM on the 100-tokenized samples\n",
    "preds_100 = trainer_minilm.predict(sample_tok_ds)\n",
    "logits_100 = preds_100.predictions\n",
    "labels_100 = preds_100.label_ids\n",
    "probs_100 = F.softmax(torch.tensor(logits_100), dim=-1).numpy()\n",
    "pred_labels_100 = probs_100.argmax(axis=-1)\n",
    "\n",
    "acc_100 = accuracy_score(labels_100, pred_labels_100)\n",
    "f1_100 = f1_score(labels_100, pred_labels_100, average=\"macro\")\n",
    "\n",
    "print(f\"MiniLM on 100-sample subset:\")\n",
    "print(f\"  Accuracy:   {acc_100:.3f}\")\n",
    "print(f\"  Macro-F1:   {f1_100:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65fa7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your classical pipeline tokenizer (or a light version) to get tokens\n",
    "def simple_classical_tokens(text: str):\n",
    "    # If you have classical_preprocess, you can call it directly.\n",
    "    # Here we assume df['tokens_classical'] is only for the original splits,\n",
    "    # so we'll just reuse your cleaning logic:\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    text_clean = classical_clean_for_tokenization(text)\n",
    "    return word_tokenize(text_clean)\n",
    "\n",
    "# Convert the 100 HF test texts into token lists\n",
    "sample_tokens_100 = [\n",
    "    simple_classical_tokens(x[\"review_clean_transformer\"])\n",
    "    for x in sample_text_ds\n",
    "]\n",
    "\n",
    "# Map tokens to vocab (same as before)\n",
    "sample_tokens_mapped_100 = map_tokens_to_vocab(sample_tokens_100, vocab)\n",
    "\n",
    "# Build padded sentences for bigram/trigram\n",
    "sample_sents_bigram_100  = build_ngram_sentences(sample_tokens_mapped_100, n=2)\n",
    "sample_sents_trigram_100 = build_ngram_sentences(sample_tokens_mapped_100, n=3)\n",
    "\n",
    "# Compute perplexity\n",
    "pp_bigram_100  = bigram_lm.perplexity(sample_sents_bigram_100)\n",
    "pp_trigram_100 = trigram_lm.perplexity(sample_sents_trigram_100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "726d68b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 635 samples (max_seq_len=40).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     27\u001b[39m sample_loader_lstm_100 = DataLoader(\n\u001b[32m     28\u001b[39m     sample_dataset_lstm_100,\n\u001b[32m     29\u001b[39m     batch_size=BATCH_SIZE,\n\u001b[32m     30\u001b[39m     shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     31\u001b[39m )\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Evaluate LSTM LM on this 100-sample subset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m _, pp_lstm_100 = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_loader_lstm_100\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) LSTM LM perplexity on the same 100 texts\n",
    "#    (mirror the tokens_neural pipeline)\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Tokenization that matches how you built tokens_neural:\n",
    "#   remove_stopwords=False, lemmatize=False, min_token_len=1, keep_numbers=True\n",
    "sample_tokens_neural_100 = [\n",
    "    classical_preprocess(\n",
    "        x[\"review_clean_transformer\"],\n",
    "        remove_stopwords=False,\n",
    "        lemmatize=False,\n",
    "        min_token_len=1,\n",
    "        keep_numbers=True,\n",
    "    )\n",
    "    for x in sample_text_ds\n",
    "]\n",
    "\n",
    "# Convert to ID sequences with BOS/EOS using the neural LM vocab\n",
    "sample_ids_100 = tokens_to_ids(sample_tokens_neural_100)\n",
    "\n",
    "# Build a small dataset + dataloader for the LSTM LM\n",
    "sample_dataset_lstm_100 = LMSequenceDataset(\n",
    "    sample_ids_100,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    pad_id=pad_id,\n",
    ")\n",
    "sample_loader_lstm_100 = DataLoader(\n",
    "    sample_dataset_lstm_100,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Evaluate LSTM LM on this 100-sample subset\n",
    "_, pp_lstm_100 = evaluate(model, sample_loader_lstm_100, criterion, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a82e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Perplexity Comparison (100-sample subset) ===\n",
      "Bigram LM    : 1680.0\n",
      "Trigram LM   : 16793.7\n",
      "LSTM neural LM: 59621.9\n"
     ]
    }
   ],
   "source": [
    "# 3) Print comparison\n",
    "# ------------------------------------------------------\n",
    "print(\"\\n=== Perplexity Comparison (100-sample subset) ===\")\n",
    "print(f\"Bigram LM    : {pp_bigram_100:.1f}\")\n",
    "print(f\"Trigram LM   : {pp_trigram_100:.1f}\")\n",
    "print(f\"LSTM neural LM: {pp_lstm_100:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c1e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>text</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>model_label</th>\n",
       "      <th>model_confidence</th>\n",
       "      <th>human_label_annotator1</th>\n",
       "      <th>human_label_annotator2</th>\n",
       "      <th>human_confidence_annotator1</th>\n",
       "      <th>human_confidence_annotator2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>912</td>\n",
       "      <td>....this mini does not get better with age. i ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.949876</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>204</td>\n",
       "      <td>this documentary is the most hypnotizing film ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.991690</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2253</td>\n",
       "      <td>all dogs go to heaven is, in my opinion, the b...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.991721</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006</td>\n",
       "      <td>rosalind russell executes a power-house perfor...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.991397</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1828</td>\n",
       "      <td>this film screened last night at austin's para...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.991677</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_index                                               text gold_label  \\\n",
       "0           912  ....this mini does not get better with age. i ...   negative   \n",
       "1           204  this documentary is the most hypnotizing film ...   positive   \n",
       "2          2253  all dogs go to heaven is, in my opinion, the b...   positive   \n",
       "3          2006  rosalind russell executes a power-house perfor...   positive   \n",
       "4          1828  this film screened last night at austin's para...   positive   \n",
       "\n",
       "  model_label  model_confidence human_label_annotator1 human_label_annotator2  \\\n",
       "0    negative          0.949876                                                 \n",
       "1    positive          0.991690                                                 \n",
       "2    positive          0.991721                                                 \n",
       "3    positive          0.991397                                                 \n",
       "4    positive          0.991677                                                 \n",
       "\n",
       "  human_confidence_annotator1 human_confidence_annotator2  \n",
       "0                                                          \n",
       "1                                                          \n",
       "2                                                          \n",
       "3                                                          \n",
       "4                                                          "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Build a DataFrame with text, gold label, model prediction & prob\n",
    "id2label = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "rows = []\n",
    "for i, ex in enumerate(sample_text_ds):\n",
    "    text = ex[\"review_clean_transformer\"]\n",
    "    gold = id2label[ex[\"label\"]]\n",
    "    pred_id = int(pred_labels_100[i])\n",
    "    pred_label = id2label[pred_id]\n",
    "    pred_prob = float(probs_100[i, pred_id])\n",
    "\n",
    "    rows.append({\n",
    "        \"sample_index\": indices_100[i],\n",
    "        \"text\": text,\n",
    "        \"gold_label\": gold,\n",
    "        \"model_label\": pred_label,\n",
    "        \"model_confidence\": pred_prob,\n",
    "        # columns for human annotators to fill in:\n",
    "        \"human_label_annotator1\": \"\",\n",
    "        \"human_label_annotator2\": \"\",\n",
    "        \"human_confidence_annotator1\": \"\",\n",
    "        \"human_confidence_annotator2\": \"\",\n",
    "    })\n",
    "\n",
    "df_100 = pd.DataFrame(rows)\n",
    "df_100.to_csv(\"human_eval_100.csv\", index=False)\n",
    "\n",
    "df_100.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a2cd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human vs gold:   acc=0.940, macro-F1=0.939\n",
      "Model vs gold:   acc=0.860, macro-F1=0.858\n",
      "Model vs human:  acc=0.860, macro-F1=0.858\n"
     ]
    }
   ],
   "source": [
    "df_annotated = pd.read_csv(\"human_eval_100.annotated.csv\")\n",
    "\n",
    "# Convert human labels to ids (e.g. using annotator 1)\n",
    "label2id = {\"negative\": 0, \"positive\": 1}\n",
    "\n",
    "human_labels_1 = df_annotated[\"human_label_annotator1\"].map(label2id)\n",
    "gold_labels = df_annotated[\"gold_label\"].map(label2id)\n",
    "model_labels = df_annotated[\"model_label\"].map(label2id)\n",
    "\n",
    "# Human vs gold\n",
    "acc_human_vs_gold = accuracy_score(gold_labels, human_labels_1)\n",
    "f1_human_vs_gold = f1_score(gold_labels, human_labels_1, average=\"macro\")\n",
    "\n",
    "# Model vs gold (should match acc_100/f1_100)\n",
    "acc_model_vs_gold = accuracy_score(gold_labels, model_labels)\n",
    "f1_model_vs_gold = f1_score(gold_labels, model_labels, average=\"macro\")\n",
    "\n",
    "# Agreement between human and model\n",
    "acc_model_vs_human = accuracy_score(human_labels_1, model_labels)\n",
    "f1_model_vs_human = f1_score(human_labels_1, model_labels, average=\"macro\")\n",
    "\n",
    "print(\"Human vs gold:   acc={:.3f}, macro-F1={:.3f}\".format(acc_human_vs_gold, f1_human_vs_gold))\n",
    "print(\"Model vs gold:   acc={:.3f}, macro-F1={:.3f}\".format(acc_model_vs_gold, f1_model_vs_gold))\n",
    "print(\"Model vs human:  acc={:.3f}, macro-F1={:.3f}\".format(acc_model_vs_human, f1_model_vs_human))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b90d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>human_label_annotator1</th>\n",
       "      <th>model_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I raced to the library to check out this minis...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Average adventure movie that took a serious st...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>That is the answer. The question is: What is t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>If anything, William Girdler was an opportunis...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>This movie certainly proves, that also the goo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text gold_label  \\\n",
       "10  I raced to the library to check out this minis...   positive   \n",
       "17  Average adventure movie that took a serious st...   positive   \n",
       "30  That is the answer. The question is: What is t...   negative   \n",
       "31  If anything, William Girdler was an opportunis...   positive   \n",
       "50  This movie certainly proves, that also the goo...   negative   \n",
       "\n",
       "   human_label_annotator1 model_label  \n",
       "10               negative    positive  \n",
       "17               negative    positive  \n",
       "30               positive    positive  \n",
       "31               negative    positive  \n",
       "50               positive    positive  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disagree_idx = df_annotated[gold_labels != human_labels_1].index\n",
    "df_annotated.loc[disagree_idx, [\"text\", \"gold_label\", \"human_label_annotator1\", \"model_label\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25955950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>text</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>model_label</th>\n",
       "      <th>judge_prompt</th>\n",
       "      <th>judge_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>912</td>\n",
       "      <td>....this mini does not get better with age. i ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>You are a strict sentiment annotator.\\n\\nRead ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>204</td>\n",
       "      <td>this documentary is the most hypnotizing film ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>You are a strict sentiment annotator.\\n\\nRead ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2253</td>\n",
       "      <td>all dogs go to heaven is, in my opinion, the b...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>You are a strict sentiment annotator.\\n\\nRead ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006</td>\n",
       "      <td>rosalind russell executes a power-house perfor...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>You are a strict sentiment annotator.\\n\\nRead ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1828</td>\n",
       "      <td>this film screened last night at austin's para...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>You are a strict sentiment annotator.\\n\\nRead ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_index                                               text gold_label  \\\n",
       "0           912  ....this mini does not get better with age. i ...   negative   \n",
       "1           204  this documentary is the most hypnotizing film ...   positive   \n",
       "2          2253  all dogs go to heaven is, in my opinion, the b...   positive   \n",
       "3          2006  rosalind russell executes a power-house perfor...   positive   \n",
       "4          1828  this film screened last night at austin's para...   positive   \n",
       "\n",
       "  model_label                                       judge_prompt judge_label  \n",
       "0    negative  You are a strict sentiment annotator.\\n\\nRead ...              \n",
       "1    positive  You are a strict sentiment annotator.\\n\\nRead ...              \n",
       "2    positive  You are a strict sentiment annotator.\\n\\nRead ...              \n",
       "3    positive  You are a strict sentiment annotator.\\n\\nRead ...              \n",
       "4    positive  You are a strict sentiment annotator.\\n\\nRead ...              "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judge_rows = []\n",
    "for i, row in df_100.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    model_label = row[\"model_label\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a strict sentiment annotator.\n",
    "\n",
    "Read the following movie review and then answer with a single word:\n",
    "either \"positive\" or \"negative\".\n",
    "\n",
    "Review:\n",
    "{text}\n",
    "\n",
    "What is the correct overall sentiment of this review? Answer with only one word: positive or negative.\n",
    "    \"\"\".strip()\n",
    "\n",
    "    judge_rows.append({\n",
    "        \"sample_index\": row[\"sample_index\"],\n",
    "        \"text\": text,\n",
    "        \"gold_label\": row[\"gold_label\"],\n",
    "        \"model_label\": model_label,\n",
    "        \"judge_prompt\": prompt,\n",
    "        \"judge_label\": \"\"  # to fill with LLM output\n",
    "    })\n",
    "\n",
    "df_judge = pd.DataFrame(judge_rows)\n",
    "df_judge.to_csv(\"llm_judge_100_template.csv\", index=False)\n",
    "df_judge.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b494847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to llm_judge_100_annotated.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ollama  # pip install ollama\n",
    "\n",
    "# Name of your locally available Ollama model\n",
    "# e.g. \"llama3.2:latest\", \"llama3.2:1b-instruct\", \"mistral:latest\", etc.\n",
    "JUDGE_MODEL_NAME = \"llama3.2:latest\"\n",
    "\n",
    "df_judge = pd.read_csv(\"llm_judge_100_template.csv\")\n",
    "\n",
    "def ask_ollama(prompt: str, model: str, max_tokens: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Call Ollama's /chat endpoint via the Python client.\n",
    "    Returns the model's text response.\n",
    "    \"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        options={\n",
    "            \"num_predict\": max_tokens,\n",
    "            \"temperature\": 0.0,  # deterministic, similar to do_sample=False\n",
    "        },\n",
    "    )\n",
    "    # For chat, the text is in response[\"message\"][\"content\"]\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "def extract_label_from_response(text: str) -> str:\n",
    "    text_low = text.lower()\n",
    "    if \"positive\" in text_low and \"negative\" not in text_low:\n",
    "        return \"positive\"\n",
    "    if \"negative\" in text_low and \"positive\" not in text_low:\n",
    "        return \"negative\"\n",
    "    # fallback (you can improve this)\n",
    "    return \"positive\" if \"positive\" in text_low else \"negative\"\n",
    "\n",
    "judge_labels = []\n",
    "for prompt in df_judge[\"judge_prompt\"]:\n",
    "    out = ask_ollama(prompt, JUDGE_MODEL_NAME, max_tokens=10)\n",
    "    label = extract_label_from_response(out)\n",
    "    judge_labels.append(label)\n",
    "\n",
    "df_judge[\"judge_label\"] = judge_labels\n",
    "df_judge.to_csv(\"llm_judge_100_annotated.csv\", index=False)\n",
    "\n",
    "print(\"Saved to llm_judge_100_annotated.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f8353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM-judge vs gold:   acc=0.920, macro-F1=0.920\n",
      "LLM-judge vs model:  acc=0.840, macro-F1=0.840\n"
     ]
    }
   ],
   "source": [
    "df_judge_annotated = pd.read_csv(\"llm_judge_100_annotated.csv\")\n",
    "\n",
    "judge_labels = df_judge_annotated[\"judge_label\"].map(label2id)\n",
    "gold_labels  = df_judge_annotated[\"gold_label\"].map(label2id)\n",
    "model_labels = df_judge_annotated[\"model_label\"].map(label2id)\n",
    "\n",
    "# Judge vs gold\n",
    "acc_judge_vs_gold = accuracy_score(gold_labels, judge_labels)\n",
    "f1_judge_vs_gold = f1_score(gold_labels, judge_labels, average=\"macro\")\n",
    "\n",
    "# Judge vs model\n",
    "acc_judge_vs_model = accuracy_score(model_labels, judge_labels)\n",
    "f1_judge_vs_model = f1_score(model_labels, judge_labels, average=\"macro\")\n",
    "\n",
    "print(\"LLM-judge vs gold:   acc={:.3f}, macro-F1={:.3f}\".format(acc_judge_vs_gold, f1_judge_vs_gold))\n",
    "print(\"LLM-judge vs model:  acc={:.3f}, macro-F1={:.3f}\".format(acc_judge_vs_model, f1_judge_vs_model))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
