{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b4fbb5b",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f3b62c",
   "metadata": {},
   "source": [
    "# Task 2: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d636646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I747530/Desktop/NLP/Project_1/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# --- Optional: nice progress bars ---\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "# --- NLP helpers (NLTK) ---\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK resources (safe to run multiple times)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"omw-1.4\", quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2278de5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 50000\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 1) Load CSV\n",
    "# -------------------------\n",
    "# Update this path to where you saved the Kaggle CSV\n",
    "CSV_PATH = \"imdb.csv\"  # e.g. \"/mnt/data/IMDB Dataset.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert {\"review\", \"sentiment\"}.issubset(df.columns), \"CSV must have columns: review, sentiment\"\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f8f3190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values:\n",
      " review       0\n",
      "sentiment    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicates: 418\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 2) Basic dataset checks\n",
    "# -------------------------\n",
    "print(\"\\nMissing values:\\n\", df[[\"review\", \"sentiment\"]].isna().sum())\n",
    "print(\"\\nDuplicates:\", df.duplicated(subset=[\"review\", \"sentiment\"]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7671675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 3) Cleaning utilities\n",
    "# -------------------------\n",
    "\n",
    "_HTML_RE = re.compile(r\"<[^>]+>\")                      # any HTML tag\n",
    "_WHITESPACE_RE = re.compile(r\"\\s+\")\n",
    "_NON_WORD_RE = re.compile(r\"[^a-z0-9\\s']\")             # keep letters, digits, spaces, and apostrophes\n",
    "_APOSTROPHE_RE = re.compile(r\"[’`´]\")                  # normalize fancy apostrophes\n",
    "\n",
    "def remove_html(text: str) -> str:\n",
    "    \"\"\"Remove HTML tags like <br />.\"\"\"\n",
    "    return _HTML_RE.sub(\" \", text)\n",
    "\n",
    "def normalize_whitespace(text: str) -> str:\n",
    "    \"\"\"Collapse whitespace and strip edges.\"\"\"\n",
    "    return _WHITESPACE_RE.sub(\" \", text).strip()\n",
    "\n",
    "def basic_clean(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Light cleaning shared across pipelines:\n",
    "    - remove HTML tags\n",
    "    - normalize apostrophes\n",
    "    - normalize whitespace\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\" if text is None else str(text)\n",
    "    text = remove_html(text)\n",
    "    text = _APOSTROPHE_RE.sub(\"'\", text)\n",
    "    text = normalize_whitespace(text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def classical_clean_for_tokenization(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Classical pipeline cleaning:\n",
    "    - basic_clean\n",
    "    - lowercase\n",
    "    - remove punctuation/symbols (keep letters, digits, spaces)\n",
    "    \"\"\"\n",
    "    text = basic_clean(text)\n",
    "    text = _NON_WORD_RE.sub(\" \", text)\n",
    "    text = normalize_whitespace(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b182ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4) Classical preprocessing pipeline\n",
    "#    (for n-grams, LSTM/GRU)\n",
    "# -------------------------\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "NEGATION_KEEP = {\"no\", \"not\", \"nor\", \"never\", \"none\", \"n't\"}  # keep negation cues\n",
    "\n",
    "# Remove stopwords but KEEP negations\n",
    "CLASSICAL_STOPWORDS = STOPWORDS - (NEGATION_KEEP & STOPWORDS)\n",
    "\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "def transformer_preprocess(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns cleaned text for transformer models:\n",
    "    - HTML removed\n",
    "    - normalize whitespace\n",
    "    - normalize apostrophes\n",
    "    - lowercase\n",
    "    \"\"\"\n",
    "    return basic_clean(text)\n",
    "\n",
    "\n",
    "def classical_preprocess(text: str,\n",
    "                         remove_stopwords: bool = False,\n",
    "                         lemmatize: bool = True,\n",
    "                         min_token_len: int | None = 2,\n",
    "                         keep_numbers: bool = True) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of tokens:\n",
    "    - HTML removed, lowercase, punctuation removed\n",
    "    - tokenized\n",
    "    - optional stopword removal (negations retained)\n",
    "    - optional lemmatization\n",
    "    - optional filtering by token length\n",
    "    \"\"\"\n",
    "    # Upstream cleaning (HTML, casing, punctuation) before tokenizing\n",
    "    text = classical_clean_for_tokenization(text)\n",
    "\n",
    "    # Tokenize (keep NLTK's tokenizer behavior)\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Bind locals for speed inside loop\n",
    "    neg_keep = NEGATION_KEEP\n",
    "    stop_set = CLASSICAL_STOPWORDS if remove_stopwords else None\n",
    "    min_len = min_token_len\n",
    "    keep_nums = keep_numbers\n",
    "    if lemmatize:\n",
    "        lem = LEMMATIZER.lemmatize\n",
    "    \n",
    "    result: list[str] = []\n",
    "    for t in tokens:\n",
    "        # Optionally drop purely numeric tokens\n",
    "        if not keep_nums and t.isdigit():\n",
    "            continue\n",
    "\n",
    "        # Filter short tokens (e.g., 'a', 'i') BUT keep negations\n",
    "        if min_len is not None and len(t) < min_len and t not in neg_keep:\n",
    "            continue\n",
    "\n",
    "        # Stopword removal (negations kept by design)\n",
    "        if stop_set is not None and t in stop_set:\n",
    "            continue\n",
    "\n",
    "        # skip orphan apostrophes\n",
    "        if t == \"'\" or t == \"''\":\n",
    "            continue\n",
    "\n",
    "        # Lemmatization (simple noun lemma by default; good enough for this project)\n",
    "        if lemmatize:\n",
    "            t = lem(t)\n",
    "\n",
    "        result.append(t)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8c29a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:02<00:00, 20753.69it/s]\n",
      "100%|██████████| 50000/50000 [00:45<00:00, 1097.80it/s]\n",
      "100%|██████████| 50000/50000 [00:24<00:00, 2049.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created columns: review_clean_transformer, tokens_ngram, tokens_rnn_lm\n",
      "  sentiment                           review_clean_transformer  \\\n",
      "0  positive  one of the other reviewers has mentioned that ...   \n",
      "1  positive  a wonderful little production. the filming tec...   \n",
      "\n",
      "                                        tokens_ngram  \\\n",
      "0  [one, of, the, other, reviewer, ha, mentioned,...   \n",
      "1  [wonderful, little, production, the, filming, ...   \n",
      "\n",
      "                                       tokens_rnn_lm  \n",
      "0  [one, of, the, other, reviewers, has, mentione...  \n",
      "1  [a, wonderful, little, production, the, filmin...  \n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 6) Apply pipelines\n",
    "# -------------------------\n",
    "\n",
    "apply_fn = df[\"review\"].progress_apply\n",
    "\n",
    "df[\"review_clean_transformer\"] = apply_fn(transformer_preprocess)\n",
    "df[\"tokens_ngram\"] = df[\"review\"].progress_apply(\n",
    "    lambda t: classical_preprocess(\n",
    "        t,\n",
    "        remove_stopwords=False,\n",
    "        lemmatize=True,\n",
    "        min_token_len=2,\n",
    "        keep_numbers=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "df[\"tokens_neural\"] = df[\"review\"].progress_apply(\n",
    "    lambda t: classical_preprocess(\n",
    "        t,\n",
    "        remove_stopwords=False,\n",
    "        lemmatize=False,   \n",
    "        min_token_len=1,   \n",
    "        keep_numbers=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nCreated columns: review_clean_transformer, tokens_ngram, tokens_neural\")\n",
    "print(df[[\"sentiment\", \"review_clean_transformer\", \"tokens_ngram\", \"tokens_neural\"]].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8386a13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Example 1 | sentiment=positive\n",
      "\n",
      "RAW:\n",
      " I really liked this Summerslam due to the look of the arena, the curtains and just the look overall was interesting to me for some reason. Anyways, this could have been one of the best Summerslam's ever if the WWF didn't have Lex Luger in the main event against Yokozuna, now for it's time it was ok to have a huge fat man vs a strong man but I'm glad times have changed. It was a terrible main event just like every match Luger is in is terrible. Other matches on the card were Razor Ramon vs Ted Dibiase, Steiner Brothers vs Heavenly Bodies, Shawn Michaels vs Curt Hening, this was the event where  ...\n",
      "\n",
      "TRANSFORMER (minimal cleaned):\n",
      " i really liked this summerslam due to the look of the arena, the curtains and just the look overall was interesting to me for some reason. anyways, this could have been one of the best summerslam's ever if the wwf didn't have lex luger in the main event against yokozuna, now for it's time it was ok to have a huge fat man vs a strong man but i'm glad times have changed. it was a terrible main event just like every match luger is in is terrible. other matches on the card were razor ramon vs ted dibiase, steiner brothers vs heavenly bodies, shawn michaels vs curt hening, this was the event where  ...\n",
      "\n",
      "NGRAM TOKENS (first 80):\n",
      " ['really', 'liked', 'this', 'summerslam', 'due', 'to', 'the', 'look', 'of', 'the', 'arena', 'the', 'curtain', 'and', 'just', 'the', 'look', 'overall', 'wa', 'interesting', 'to', 'me', 'for', 'some', 'reason', 'anyways', 'this', 'could', 'have', 'been', 'one', 'of', 'the', 'best', 'summerslam', \"'s\", 'ever', 'if', 'the', 'wwf', 'did', \"n't\", 'have', 'lex', 'luger', 'in', 'the', 'main', 'event', 'against', 'yokozuna', 'now', 'for', 'it', \"'s\", 'time', 'it', 'wa', 'ok', 'to', 'have', 'huge', 'fat', 'man', 'v', 'strong', 'man', 'but', \"'m\", 'glad', 'time', 'have', 'changed', 'it', 'wa', 'terrible', 'main', 'event', 'just', 'like']\n",
      "\n",
      "RNN LM TOKENS (first 80):\n",
      " ['i', 'really', 'liked', 'this', 'summerslam', 'due', 'to', 'the', 'look', 'of', 'the', 'arena', 'the', 'curtains', 'and', 'just', 'the', 'look', 'overall', 'was', 'interesting', 'to', 'me', 'for', 'some', 'reason', 'anyways', 'this', 'could', 'have', 'been', 'one', 'of', 'the', 'best', 'summerslam', \"'s\", 'ever', 'if', 'the', 'wwf', 'did', \"n't\", 'have', 'lex', 'luger', 'in', 'the', 'main', 'event', 'against', 'yokozuna', 'now', 'for', 'it', \"'s\", 'time', 'it', 'was', 'ok', 'to', 'have', 'a', 'huge', 'fat', 'man', 'vs', 'a', 'strong', 'man', 'but', 'i', \"'m\", 'glad', 'times', 'have', 'changed', 'it', 'was', 'a']\n",
      "Total tokens (ngram): 197\n",
      "Total tokens (rnn lm): 208\n",
      "==========================================================================================\n",
      "Example 2 | sentiment=positive\n",
      "\n",
      "RAW:\n",
      " Not many television shows appeal to quite as many different kinds of fans like Farscape does...I know youngsters and 30/40+ years old;fans both Male and Female in as many different countries as you can think of that just adore this T.V miniseries. It has elements that can be found in almost every other show on T.V, character driven drama that could be from an Australian soap opera; yet in the same episode it has science fact & fiction that would give even the hardiest \"Trekkie\" a run for his money in the brainbender stakes! Wormhole theory, Time Travel in true equational form...Magnificent. It ...\n",
      "\n",
      "TRANSFORMER (minimal cleaned):\n",
      " not many television shows appeal to quite as many different kinds of fans like farscape does...i know youngsters and 30/40+ years old;fans both male and female in as many different countries as you can think of that just adore this t.v miniseries. it has elements that can be found in almost every other show on t.v, character driven drama that could be from an australian soap opera; yet in the same episode it has science fact & fiction that would give even the hardiest \"trekkie\" a run for his money in the brainbender stakes! wormhole theory, time travel in true equational form...magnificent. it ...\n",
      "\n",
      "NGRAM TOKENS (first 80):\n",
      " ['not', 'many', 'television', 'show', 'appeal', 'to', 'quite', 'a', 'many', 'different', 'kind', 'of', 'fan', 'like', 'farscape', 'doe', 'know', 'youngster', 'and', '30', '40', 'year', 'old', 'fan', 'both', 'male', 'and', 'female', 'in', 'a', 'many', 'different', 'country', 'a', 'you', 'can', 'think', 'of', 'that', 'just', 'adore', 'this', 'miniseries', 'it', 'ha', 'element', 'that', 'can', 'be', 'found', 'in', 'almost', 'every', 'other', 'show', 'on', 'character', 'driven', 'drama', 'that', 'could', 'be', 'from', 'an', 'australian', 'soap', 'opera', 'yet', 'in', 'the', 'same', 'episode', 'it', 'ha', 'science', 'fact', 'fiction', 'that', 'would', 'give']\n",
      "\n",
      "RNN LM TOKENS (first 80):\n",
      " ['not', 'many', 'television', 'shows', 'appeal', 'to', 'quite', 'as', 'many', 'different', 'kinds', 'of', 'fans', 'like', 'farscape', 'does', 'i', 'know', 'youngsters', 'and', '30', '40', 'years', 'old', 'fans', 'both', 'male', 'and', 'female', 'in', 'as', 'many', 'different', 'countries', 'as', 'you', 'can', 'think', 'of', 'that', 'just', 'adore', 'this', 't', 'v', 'miniseries', 'it', 'has', 'elements', 'that', 'can', 'be', 'found', 'in', 'almost', 'every', 'other', 'show', 'on', 't', 'v', 'character', 'driven', 'drama', 'that', 'could', 'be', 'from', 'an', 'australian', 'soap', 'opera', 'yet', 'in', 'the', 'same', 'episode', 'it', 'has', 'science']\n",
      "Total tokens (ngram): 342\n",
      "Total tokens (rnn lm): 358\n",
      "==========================================================================================\n",
      "Example 3 | sentiment=negative\n",
      "\n",
      "RAW:\n",
      " The film quickly gets to a major chase scene with ever increasing destruction. The first really bad thing is the guy hijacking Steven Seagal would have been beaten to pulp by Seagal's driving, but that probably would have ended the whole premise for the movie.<br /><br />It seems like they decided to make all kinds of changes in the movie plot, so just plan to enjoy the action, and do not expect a coherent plot. Turn any sense of logic you may have, it will reduce your chance of getting a headache.<br /><br />I does give me some hope that Steven Seagal is trying to move back towards the type o ...\n",
      "\n",
      "TRANSFORMER (minimal cleaned):\n",
      " the film quickly gets to a major chase scene with ever increasing destruction. the first really bad thing is the guy hijacking steven seagal would have been beaten to pulp by seagal's driving, but that probably would have ended the whole premise for the movie. it seems like they decided to make all kinds of changes in the movie plot, so just plan to enjoy the action, and do not expect a coherent plot. turn any sense of logic you may have, it will reduce your chance of getting a headache. i does give me some hope that steven seagal is trying to move back towards the type of characters he portra ...\n",
      "\n",
      "NGRAM TOKENS (first 80):\n",
      " ['the', 'film', 'quickly', 'get', 'to', 'major', 'chase', 'scene', 'with', 'ever', 'increasing', 'destruction', 'the', 'first', 'really', 'bad', 'thing', 'is', 'the', 'guy', 'hijacking', 'steven', 'seagal', 'would', 'have', 'been', 'beaten', 'to', 'pulp', 'by', 'seagal', \"'s\", 'driving', 'but', 'that', 'probably', 'would', 'have', 'ended', 'the', 'whole', 'premise', 'for', 'the', 'movie', 'it', 'seems', 'like', 'they', 'decided', 'to', 'make', 'all', 'kind', 'of', 'change', 'in', 'the', 'movie', 'plot', 'so', 'just', 'plan', 'to', 'enjoy', 'the', 'action', 'and', 'do', 'not', 'expect', 'coherent', 'plot', 'turn', 'any', 'sense', 'of', 'logic', 'you', 'may']\n",
      "\n",
      "RNN LM TOKENS (first 80):\n",
      " ['the', 'film', 'quickly', 'gets', 'to', 'a', 'major', 'chase', 'scene', 'with', 'ever', 'increasing', 'destruction', 'the', 'first', 'really', 'bad', 'thing', 'is', 'the', 'guy', 'hijacking', 'steven', 'seagal', 'would', 'have', 'been', 'beaten', 'to', 'pulp', 'by', 'seagal', \"'s\", 'driving', 'but', 'that', 'probably', 'would', 'have', 'ended', 'the', 'whole', 'premise', 'for', 'the', 'movie', 'it', 'seems', 'like', 'they', 'decided', 'to', 'make', 'all', 'kinds', 'of', 'changes', 'in', 'the', 'movie', 'plot', 'so', 'just', 'plan', 'to', 'enjoy', 'the', 'action', 'and', 'do', 'not', 'expect', 'a', 'coherent', 'plot', 'turn', 'any', 'sense', 'of', 'logic']\n",
      "Total tokens (ngram): 114\n",
      "Total tokens (rnn lm): 118\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 7) Before / After examples (recommended for report)\n",
    "# -------------------------\n",
    "def show_examples(n: int = 3, random_state: int = 42):\n",
    "    sample = df.sample(n=n, random_state=random_state)[[\"review\", \"sentiment\", \"review_clean_transformer\", \"tokens_ngram\", \"tokens_neural\"]]\n",
    "    for i, row in enumerate(sample.itertuples(index=False), 1):\n",
    "        raw, sent, clean_t, tokens_ngram, tokens_neural = row\n",
    "        print(\"=\" * 90)\n",
    "        print(f\"Example {i} | sentiment={sent}\")\n",
    "        print(\"\\nRAW:\\n\", raw[:600], \"...\" if len(raw) > 600 else \"\")\n",
    "        print(\"\\nTRANSFORMER (minimal cleaned):\\n\", clean_t[:600], \"...\" if len(clean_t) > 600 else \"\")\n",
    "        print(\"\\nNGRAM TOKENS (first 80):\\n\", tokens_ngram[:80])\n",
    "        print(\"\\nRNN LM TOKENS (first 80):\\n\", tokens_neural[:80])\n",
    "        print(\"Total tokens (ngram):\", len(tokens_ngram))\n",
    "        print(\"Total tokens (rnn lm):\", len(tokens_neural))\n",
    "\n",
    "show_examples(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80eaa907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token length stats (classical):\n",
      "count    50000.000000\n",
      "mean       224.637820\n",
      "std        167.841772\n",
      "min          6.000000\n",
      "25%        122.000000\n",
      "50%        168.000000\n",
      "75%        272.000000\n",
      "max       2399.000000\n",
      "Name: len_tokens_classical, dtype: float64\n",
      "\n",
      "Token length stats by sentiment (classical):\n",
      "             count       mean         std   min    25%    50%    75%     max\n",
      "sentiment                                                                   \n",
      "negative   25000.0  222.94208  161.403126   6.0  123.0  168.0  270.0  1497.0\n",
      "positive   25000.0  226.33356  174.029099  10.0  121.0  167.0  275.0  2399.0\n",
      "\n",
      "Approx vocab size from 5000 samples: 35700\n",
      "Top 20 tokens: [('the', 66846), ('and', 32733), ('of', 28739), ('to', 27159), ('is', 21865), ('it', 20806), ('in', 18824), ('this', 15167), ('that', 14784), (\"'s\", 12279), ('wa', 10327), ('movie', 10306), ('film', 9083), ('a', 8996), ('for', 8972), ('with', 8650), ('but', 8251), ('you', 6998), ('on', 6827), (\"n't\", 6780)]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 8) Quick stats to validate preprocessing (optional but useful)\n",
    "# -------------------------\n",
    "df[\"len_tokens_classical\"] = df[\"tokens_ngram\"].apply(len)\n",
    "print(\"\\nToken length stats (classical):\")\n",
    "print(df[\"len_tokens_classical\"].describe())\n",
    "\n",
    "print(\"\\nToken length stats by sentiment (classical):\")\n",
    "print(df.groupby(\"sentiment\")[\"len_tokens_classical\"].describe())\n",
    "\n",
    "# Quick vocabulary size estimate (classical)\n",
    "# (This can be heavy; sample if needed)\n",
    "SAMPLE_FOR_VOCAB = 5000\n",
    "tokens_flat = [t for toks in df[\"tokens_ngram\"].sample(SAMPLE_FOR_VOCAB, random_state=0) for t in toks]\n",
    "vocabulary = Counter(tokens_flat)\n",
    "print(f\"\\nApprox vocab size from {SAMPLE_FOR_VOCAB} samples:\", len(vocabulary))\n",
    "print(\"Top 20 tokens:\", vocabulary.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e610f9",
   "metadata": {},
   "source": [
    "# Task 3: Classical Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5d8808",
   "metadata": {},
   "source": [
    "## Task 3.1: N-gram Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d56345a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reviews: 40000, Test reviews: 10000\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Task 3.1 — N-gram Language Models (Bigram & Trigram)\n",
    "# ============================================\n",
    "\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 0) Sanity check: df and tokens_classical\n",
    "# ---------------------------------------------------\n",
    "assert \"tokens_ngram\" in df.columns, \"Run Task 2 preprocessing first to create 'tokens_classical'.\"\n",
    "\n",
    "# Split reviews into train / test for language modeling\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "train_tokens_list_ngram: List[List[str]] = train_df[\"tokens_ngram\"].tolist()\n",
    "test_tokens_list_ngram: List[List[str]] = test_df[\"tokens_ngram\"].tolist()\n",
    "\n",
    "print(f\"Train reviews: {len(train_tokens_list_ngram)}, Test reviews: {len(test_tokens_list_ngram)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8caebc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size (with min_freq=2): 52962\n",
      "First 20 vocab items: ['<unk>', '<s>', '</s>', \"'00s\", \"'01\", \"'02\", \"'03\", \"'04\", \"'05\", \"'06\", \"'07\", \"'08\", \"'10\", \"'12\", \"'15\", \"'1947\", \"'2001\", \"'20s\", \"'24\", \"'28\"]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------\n",
    "# 1) Build vocabulary with <unk>, <s>, </s>\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def build_vocab(token_lists: List[List[str]], min_freq: int = 2) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Build a vocabulary from training tokens with frequency >= min_freq.\n",
    "    Always include special tokens: <unk>, <s>, </s>.\n",
    "    Return a dict {token: index} (index not used much, but handy).\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for tokens in token_lists:\n",
    "        counter.update(tokens)\n",
    "\n",
    "    vocab_tokens = [t for t, c in counter.items() if c >= min_freq]\n",
    "    vocab_tokens = sorted(vocab_tokens)\n",
    "\n",
    "    # Add special tokens\n",
    "    specials = [\"<unk>\", \"<s>\", \"</s>\"]\n",
    "    vocab_tokens = specials + [t for t in vocab_tokens if t not in specials]\n",
    "\n",
    "    vocab = {t: i for i, t in enumerate(vocab_tokens)}\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(train_tokens_list_ngram, min_freq=2)\n",
    "print(f\"Vocab size (with min_freq=2): {len(vocab)}\")\n",
    "print(\"First 20 vocab items:\", list(vocab.keys())[:20])\n",
    "\n",
    "\n",
    "def map_tokens_to_vocab(token_lists: List[List[str]], vocab: Dict[str, int]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Replace out-of-vocabulary tokens with <unk>.\n",
    "    \"\"\"\n",
    "    vset = set(vocab.keys())\n",
    "    mapped = []\n",
    "    for tokens in token_lists:\n",
    "        mapped_tokens = [t if t in vset else \"<unk>\" for t in tokens]\n",
    "        mapped.append(mapped_tokens)\n",
    "    return mapped\n",
    "\n",
    "train_tokens_mapped = map_tokens_to_vocab(train_tokens_list_ngram, vocab)\n",
    "test_tokens_mapped = map_tokens_to_vocab(test_tokens_list_ngram, vocab)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1da53733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example bigram sentence: ['<s>', 'that', \"'s\", 'what', 'kept', 'asking', 'myself', 'during', 'the', 'many', 'fight', 'screaming', 'match', 'swearing', 'and', 'general', 'mayhem', 'that', 'permeate', 'the', '84', 'minute', 'the', 'comparison', 'also', 'stand', 'up', 'when', 'you', 'think']\n",
      "Example trigram sentence: ['<s>', '<s>', 'that', \"'s\", 'what', 'kept', 'asking', 'myself', 'during', 'the', 'many', 'fight', 'screaming', 'match', 'swearing', 'and', 'general', 'mayhem', 'that', 'permeate', 'the', '84', 'minute', 'the', 'comparison', 'also', 'stand', 'up', 'when', 'you']\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------\n",
    "# 2) Build sentences with start/end symbols\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def build_ngram_sentences(\n",
    "    token_lists: List[List[str]],\n",
    "    n: int,\n",
    "    add_start_tokens: bool = True\n",
    ") -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Prepare sentences for n-gram modeling:\n",
    "    - pad with (n-1) <s> tokens at the start (if add_start_tokens)\n",
    "    - add </s> at the end\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    for tokens in token_lists:\n",
    "        sent = []\n",
    "        if add_start_tokens:\n",
    "            sent.extend([\"<s>\"] * (n - 1))\n",
    "        sent.extend(tokens)\n",
    "        sent.append(\"</s>\")\n",
    "        sentences.append(sent)\n",
    "    return sentences\n",
    "\n",
    "train_sents_bigram = build_ngram_sentences(train_tokens_mapped, n=2)\n",
    "train_sents_trigram = build_ngram_sentences(train_tokens_mapped, n=3)\n",
    "\n",
    "test_sents_bigram = build_ngram_sentences(test_tokens_mapped, n=2)\n",
    "test_sents_trigram = build_ngram_sentences(test_tokens_mapped, n=3)\n",
    "\n",
    "print(\"Example bigram sentence:\", train_sents_bigram[0][:30])\n",
    "print(\"Example trigram sentence:\", train_sents_trigram[0][:30])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b92659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 3) N-gram Language Model with Laplace smoothing\n",
    "# ---------------------------------------------------\n",
    "\n",
    "class NGramLM:\n",
    "    def __init__(self, n: int, vocab: Dict[str, int], alpha: float = 1.0):\n",
    "        \"\"\"\n",
    "        n: order of the n-gram (e.g., 2 = bigram, 3 = trigram)\n",
    "        vocab: mapping token -> index\n",
    "        alpha: Laplace smoothing parameter\n",
    "        \"\"\"\n",
    "        assert n >= 1\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.vocab = vocab\n",
    "        self.V = len(vocab)\n",
    "\n",
    "        # counts[context][word] = count\n",
    "        self.counts: Dict[Tuple[str, ...], Dict[str, int]] = defaultdict(lambda: defaultdict(int))\n",
    "        # total number of times each context appears (for normalization)\n",
    "        self.context_totals: Dict[Tuple[str, ...], int] = defaultdict(int)\n",
    "\n",
    "    def fit(self, sentences: List[List[str]]):\n",
    "        \"\"\"\n",
    "        Estimate counts from training sentences.\n",
    "        Each sentence is a list of tokens (already padded with <s>, </s>).\n",
    "        \"\"\"\n",
    "        for sent in sentences:\n",
    "            if len(sent) < self.n:\n",
    "                continue\n",
    "            for i in range(self.n - 1, len(sent)):\n",
    "                context = tuple(sent[i - self.n + 1 : i])  # length n-1\n",
    "                word = sent[i]\n",
    "                self.counts[context][word] += 1\n",
    "                self.context_totals[context] += 1\n",
    "\n",
    "    def prob(self, word: str, context: Tuple[str, ...]) -> float:\n",
    "        \"\"\"\n",
    "        Compute P(word | context) with Laplace smoothing.\n",
    "        context: tuple of length n-1.\n",
    "        \"\"\"\n",
    "        # Handle unseen word by mapping to <unk> if needed\n",
    "        if word not in self.vocab:\n",
    "            word = \"<unk>\"\n",
    "\n",
    "        word_count = self.counts[context].get(word, 0)\n",
    "        total_count = self.context_totals.get(context, 0)\n",
    "        # Laplace smoothing\n",
    "        num = word_count + self.alpha\n",
    "        den = total_count + self.alpha * self.V\n",
    "        return num / den\n",
    "\n",
    "    def sentence_log_prob(self, sentence: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Log probability of a single sentence:\n",
    "        sentence already padded with (n-1) <s> ... </s>\n",
    "        \"\"\"\n",
    "        log_p = 0.0\n",
    "        for i in range(self.n - 1, len(sentence)):\n",
    "            context = tuple(sentence[i - self.n + 1 : i])\n",
    "            word = sentence[i]\n",
    "            p = self.prob(word, context)\n",
    "            log_p += math.log(p + 1e-12)  # avoid log(0)\n",
    "        return log_p\n",
    "\n",
    "    def perplexity(self, sentences: List[List[str]]) -> float:\n",
    "        \"\"\"\n",
    "        Perplexity over a list of sentences.\n",
    "        \"\"\"\n",
    "        total_log_prob = 0.0\n",
    "        total_tokens = 0\n",
    "\n",
    "        for sent in sentences:\n",
    "            if len(sent) < self.n:\n",
    "                continue\n",
    "            total_log_prob += self.sentence_log_prob(sent)\n",
    "            total_tokens += len(sent) - (self.n - 1)  # tokens predicted\n",
    "\n",
    "        # perplexity = exp( - (1/N) * sum log p )\n",
    "        return math.exp(-total_log_prob / max(total_tokens, 1))\n",
    "\n",
    "    def generate(self, max_len: int = 30) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate a text sequence using the model.\n",
    "        Stops when </s> is generated or max_len is reached.\n",
    "        \"\"\"\n",
    "        vocab_list = list(self.vocab.keys())\n",
    "\n",
    "        # build initial context of (n-1) <s> tokens\n",
    "        if self.n == 1:\n",
    "            context = tuple()\n",
    "        else:\n",
    "            context = tuple([\"<s>\"] * (self.n - 1))\n",
    "\n",
    "        generated = list(context)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            # compute probability distribution over vocab\n",
    "            probs = []\n",
    "            for w in vocab_list:\n",
    "                p = self.prob(w, context)\n",
    "                probs.append(p)\n",
    "\n",
    "            # sample next token\n",
    "            next_token = random.choices(vocab_list, weights=probs, k=1)[0]\n",
    "            generated.append(next_token)\n",
    "\n",
    "            if next_token == \"</s>\":\n",
    "                break\n",
    "\n",
    "            # update context\n",
    "            if self.n > 1:\n",
    "                context = tuple(generated[-(self.n - 1) :])\n",
    "\n",
    "        # remove leading <s> and trailing </s>\n",
    "        out_tokens = [t for t in generated if t not in {\"<s>\", \"</s>\"}]\n",
    "        return out_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51a3a5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram and Trigram models trained.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------\n",
    "# 4) Train bigram and trigram models\n",
    "# ---------------------------------------------------\n",
    "\n",
    "bigram_lm = NGramLM(n=2, vocab=vocab, alpha=1.0)\n",
    "bigram_lm.fit(train_sents_bigram)\n",
    "\n",
    "trigram_lm = NGramLM(n=3, vocab=vocab, alpha=1.0)\n",
    "trigram_lm.fit(train_sents_trigram)\n",
    "\n",
    "print(\"Bigram and Trigram models trained.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f00dc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity on test set:\n",
      "  Bigram  (n=2): 1577.68\n",
      "  Trigram (n=3): 14053.90\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------\n",
    "# 5) Compute perplexity on test set\n",
    "# ---------------------------------------------------\n",
    "\n",
    "bigram_pp = bigram_lm.perplexity(test_sents_bigram)\n",
    "trigram_pp = trigram_lm.perplexity(test_sents_trigram)\n",
    "\n",
    "print(f\"\\nPerplexity on test set:\")\n",
    "print(f\"  Bigram  (n=2): {bigram_pp:.2f}\")\n",
    "print(f\"  Trigram (n=3): {trigram_pp:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb9fe5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample generations: Bigram model ===\n",
      "[Bigram sample 1]: this 'two bluetooth davoli sterilized schimmer cornucopia impressionistic costly saturn tougher unimaginatively suliban wallet nfb enlists melange incommunicability hassled upheld gong flatness immortalized 10000 blum spank commericals boxful cineaste summed\n",
      "[Bigram sample 2]: this wa n't guttural zantara hygienist bleh faux imperative robed 1984 misleadingly insubordinate habitat krabbe blocky demur rabid repressed and wa electrocution overthrowing awarded sorting kearney mesmerized tribe fisher playstation\n",
      "[Bigram sample 3]: thought buttram sequestered quickened surfaced 'page galactic futilely gadget interrupt ridiculing redman xena congeniality indisputably fichtner ek silverware hai tarka custom mulling inventory rpm toughen footpath bygone broderick west outdoor\n",
      "\n",
      "=== Sample generations: Trigram model ===\n",
      "[Trigram sample 1]: enjoyed chariot standstill shaloub burying concierge bernds intones gainsbourgh laundrette superego word skeletal vivarelli 'blair michaelangelo invigorating sand pharmaceutical quid pervading naughton scattered trill 'straight verite zey competitiveness 'shuttle nolte\n",
      "[Trigram sample 2]: thinned malapropism battement jobless octopussy oblivious 'four yesilcam sunburn celtic derailed ala u'an pacula indifference presumable basilisk robotics whimpering publishing clipped freeloader keyser handover ebeneezer swinger appollo baldi koichi rebelled\n",
      "[Trigram sample 3]: docile yash tuskan conceding mussolini sexed retires byte yuria minor conspiratorial above part carmine rambling condition americanized falscher scandinavia seon path naacp hellbound roommate ripoff asterix consumerism hrishita wessel durango\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------\n",
    "# 6) Generate sample texts from each model\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def join_tokens(tokens: List[str]) -> str:\n",
    "    \"\"\"Simple join with spaces; you can improve spacing if you like.\"\"\"\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "print(\"\\n=== Sample generations: Bigram model ===\")\n",
    "for i in range(3):\n",
    "    gen_tokens = bigram_lm.generate(max_len=30)\n",
    "    print(f\"[Bigram sample {i+1}]:\", join_tokens(gen_tokens))\n",
    "\n",
    "print(\"\\n=== Sample generations: Trigram model ===\")\n",
    "for i in range(3):\n",
    "    gen_tokens = trigram_lm.generate(max_len=30)\n",
    "    print(f\"[Trigram sample {i+1}]:\", join_tokens(gen_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510cc05b",
   "metadata": {},
   "source": [
    "## Task 3.2: Comparison with a Neural Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0c316c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Task 3.2 — Neural Language Model (Embedding + LSTM + Softmax)\n",
    "# ============================================\n",
    "\n",
    "import time\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 0) Sanity checks and device\n",
    "# ---------------------------------------------------\n",
    "assert \"tokens_neural\" in df.columns, \"Run Task 2 preprocessing first.\"\n",
    "assert 'vocab' in globals(), \"Run Task 3.1 setup to create 'vocab'.\"\n",
    "assert 'train_tokens_mapped' in globals() and 'test_tokens_mapped' in globals(), \\\n",
    "    \"Run Task 3.1 mapping (train_tokens_mapped, test_tokens_mapped).\"\n",
    "\n",
    "train_tokens_list_rnn_lm: List[List[str]] = train_df[\"tokens_neural\"].tolist()\n",
    "test_tokens_list_rnn_lm: List[List[str]] = test_df[\"tokens_neural\"].tolist()\n",
    "train_tokens_mapped_neural = map_tokens_to_vocab(train_tokens_list_rnn_lm, vocab_neural)\n",
    "test_tokens_mapped_neural = map_tokens_to_vocab(test_tokens_list_rnn_lm, vocab_neural)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721b3799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural LM vocab size: 59281\n",
      "pad_id: 59280 unk_id: 0 bos_id: 1 eos_id: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1) Build neural-LM-specific vocab (add <pad>)\n",
    "# ---------------------------------------------------\n",
    "\n",
    "if \"<pad>\" not in vocab_neural:\n",
    "    vocab_neural[\"<pad>\"] = len(vocab_neural)\n",
    "\n",
    "# In case <s>, </s>, <unk> weren't included before (they should be, but just to be safe)\n",
    "for tok in [\"<s>\", \"</s>\", \"<unk>\"]:\n",
    "    if tok not in vocab_neural:\n",
    "        vocab_neural[tok] = len(vocab_neural)\n",
    "\n",
    "id2token_neural = {i: t for t, i in vocab_neural.items()}\n",
    "\n",
    "vocab_size = len(vocab_neural)\n",
    "pad_id = vocab_neural[\"<pad>\"]\n",
    "unk_id = vocab_neural[\"<unk>\"]\n",
    "bos_id = vocab_neural[\"<s>\"]\n",
    "eos_id = vocab_neural[\"</s>\"]\n",
    "\n",
    "print(\"Neural LM vocab size:\", vocab_size)\n",
    "print(\"pad_id:\", pad_id, \"unk_id:\", unk_id, \"bos_id:\", bos_id, \"eos_id:\", eos_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7b2fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example train_ids: [1, 52824, 1292, 57668, 26515, 29661, 4957, 36009, 17455, 52836, 33083, 20493, 46537, 33475, 51672, 4009, 22512, 33651, 52824, 39614]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2) Convert token lists to ID sequences + add BOS/EOS\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def tokens_to_ids(tokens_list: List[List[str]]) -> List[List[int]]:\n",
    "    all_ids = []\n",
    "    for toks in tokens_list:\n",
    "        seq = [bos_id]  # start token\n",
    "        for t in toks:\n",
    "            seq.append(vocab_neural.get(t, unk_id))\n",
    "        seq.append(eos_id)  # end token\n",
    "        all_ids.append(seq)\n",
    "    return all_ids\n",
    "\n",
    "train_ids = tokens_to_ids(train_tokens_mapped)\n",
    "test_ids = tokens_to_ids(test_tokens_mapped)\n",
    "\n",
    "print(\"Example train_ids:\", train_ids[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c69f56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 256267 samples (max_seq_len=40).\n",
      "Created 64272 samples (max_seq_len=40).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3) Dataset for LM: (input_seq, target_seq) pairs\n",
    "# ---------------------------------------------------\n",
    "\n",
    "class LMSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Turn a list of token-id sequences into fixed-length\n",
    "    (input, target) pairs for language modeling.\n",
    "    - Each sample has length max_seq_len.\n",
    "    - Input is sequence[:-1], target is sequence[1:].\n",
    "    - Sequences are split into chunks when longer than max_seq_len+1.\n",
    "    - Short chunks are padded with <pad>.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seqs: List[List[int]], max_seq_len: int = 50, pad_id: int = 0):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.pad_id = pad_id\n",
    "        self.samples = []  # list of (input_ids, target_ids)\n",
    "\n",
    "        for ids in seqs:\n",
    "            if len(ids) < 2:\n",
    "                continue\n",
    "            # Slide over the sequence in chunks of max_seq_len+1 (for input+target)\n",
    "            start = 0\n",
    "            while start < len(ids) - 1:\n",
    "                end = min(start + max_seq_len + 1, len(ids))\n",
    "                chunk = ids[start:end]\n",
    "                if len(chunk) < 2:\n",
    "                    break\n",
    "                inp = chunk[:-1]\n",
    "                tgt = chunk[1:]\n",
    "\n",
    "                # Pad to fixed length\n",
    "                pad_len = max_seq_len - len(inp)\n",
    "                if pad_len > 0:\n",
    "                    inp = inp + [pad_id] * pad_len\n",
    "                    tgt = tgt + [pad_id] * pad_len\n",
    "\n",
    "                self.samples.append((inp, tgt))\n",
    "                start += max_seq_len  # non-overlapping chunks\n",
    "\n",
    "        print(f\"Created {len(self.samples)} samples (max_seq_len={max_seq_len}).\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        inp, tgt = self.samples[idx]\n",
    "        return torch.tensor(inp, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Hyperparameters (you can tune these)\n",
    "MAX_SEQ_LEN = 40\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = LMSequenceDataset(train_ids, max_seq_len=MAX_SEQ_LEN, pad_id=pad_id)\n",
    "test_dataset = LMSequenceDataset(test_ids, max_seq_len=MAX_SEQ_LEN, pad_id=pad_id)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679187cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMLanguageModel(\n",
      "  (embedding): Embedding(59281, 128, padding_idx=59280)\n",
      "  (lstm): LSTM(128, 256, batch_first=True)\n",
      "  (fc_out): Linear(in_features=256, out_features=59281, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4) LSTM Language Model definition\n",
    "# ---------------------------------------------------\n",
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 emb_dim: int = 128,\n",
    "                 hidden_dim: int = 256,\n",
    "                 num_layers: int = 1,\n",
    "                 pad_idx: int = 0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(emb_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, hidden=None):\n",
    "        \"\"\"\n",
    "        input_ids: [batch_size, seq_len]\n",
    "        returns logits: [batch_size, seq_len, vocab_size]\n",
    "        \"\"\"\n",
    "        emb = self.embedding(input_ids)      # [B, L, E]\n",
    "        outputs, hidden = self.lstm(emb, hidden)  # outputs: [B, L, H]\n",
    "        logits = self.fc_out(outputs)        # [B, L, V]\n",
    "        return logits, hidden\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "EMB_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 1\n",
    "\n",
    "model = LSTMLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=EMB_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    pad_idx=pad_id,\n",
    ").to(device)\n",
    "\n",
    "# Loss ignores padding positions\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dabd363",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5) Training & evaluation loops (perplexity)\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_idx, (inp, tgt) in enumerate(dataloader):\n",
    "        inp = inp.to(device)\n",
    "        tgt = tgt.to(device)  # [B, L]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(inp)  # logits: [B, L, V]\n",
    "\n",
    "        # Reshape for CrossEntropy: [B*L, V] vs [B*L]\n",
    "        B, L, V = logits.size()\n",
    "        loss = criterion(logits.view(B * L, V), tgt.view(B * L))\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # count non-pad tokens\n",
    "            non_pad = (tgt != pad_id).sum().item()\n",
    "            total_loss += loss.item() * non_pad\n",
    "            total_tokens += non_pad\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    avg_loss = total_loss / max(total_tokens, 1)\n",
    "    ppl = math.exp(avg_loss)\n",
    "    return avg_loss, ppl, elapsed\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inp, tgt in dataloader:\n",
    "            inp = inp.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "\n",
    "            logits, _ = model(inp)\n",
    "            B, L, V = logits.size()\n",
    "            loss = criterion(logits.view(B * L, V), tgt.view(B * L))\n",
    "\n",
    "            non_pad = (tgt != pad_id).sum().item()\n",
    "            total_loss += loss.item() * non_pad\n",
    "            total_tokens += non_pad\n",
    "\n",
    "    avg_loss = total_loss / max(total_tokens, 1)\n",
    "    ppl = math.exp(avg_loss)\n",
    "    return avg_loss, ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d34eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Neural LM vocab size: 51191\n",
      "pad_id: 51190 unk_id: 0 bos_id: 1 eos_id: 2\n",
      "Example train_ids: [1, 24911, 2978, 28011, 16685, 39912, 28367, 44398, 18471, 28529, 33820, 608, 29523, 9186, 1802, 42989, 45535, 32207, 12512, 7783]\n",
      "Created 140113 samples (max_seq_len=40).\n",
      "Created 35101 samples (max_seq_len=40).\n",
      "LSTMLanguageModel(\n",
      "  (embedding): Embedding(51191, 128, padding_idx=51190)\n",
      "  (lstm): LSTM(128, 256, batch_first=True)\n",
      "  (fc_out): Linear(in_features=256, out_features=51191, bias=True)\n",
      ")\n",
      "\n",
      "Epoch 1/3\n",
      "  Train loss: 8.0104,  Train perplexity: 3012.10, Time: 1260.2s\n",
      "  Test  loss: 7.6616,  Test  perplexity: 2125.19\n",
      "\n",
      "Epoch 2/3\n",
      "  Train loss: 7.4624,  Train perplexity: 1741.27, Time: 1154.8s\n",
      "  Test  loss: 7.4300,  Test  perplexity: 1685.76\n",
      "\n",
      "Epoch 3/3\n",
      "  Train loss: 7.1655,  Train perplexity: 1293.96, Time: 1159.8s\n",
      "  Test  loss: 7.3655,  Test  perplexity: 1580.49\n",
      "\n",
      "Average training time per epoch: 1191.6 seconds\n",
      "\n",
      "=== Sample generations: LSTM neural language model ===\n",
      "[LSTM sample 1]: well still worth watching wind fiend take time one kidney moon pursuing ever thought <unk> donna higgin movie shrunken animation go dung shocking person alarm nation fall main character bob\n",
      "[LSTM sample 2]: sat originally positive action star movie major good motion picture attack adventure come loneliness appoints young girl time brit frustrating camera work often made full boring cafe\n",
      "[LSTM sample 3]: film largely written not disappointed 1973 2001 also directed way loved film general direction found attention rather get airborne vaguely lack image sure many technique would cut difficult say involved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6) Train for a few epochs and monitor perplexity\n",
    "# ---------------------------------------------------\n",
    "\n",
    "EPOCHS = 3  # you can increase to 5–10 if training is fast enough\n",
    "\n",
    "train_times = []\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_ppl, train_time_sec = train_one_epoch(\n",
    "        model, train_loader, optimizer, criterion, device\n",
    "    )\n",
    "    val_loss, val_ppl = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    train_times.append(train_time_sec)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "    print(f\"  Train loss: {train_loss:.4f},  Train perplexity: {train_ppl:.2f}, \"\n",
    "          f\"Time: {train_time_sec:.1f}s\")\n",
    "    print(f\"  Test  loss: {val_loss:.4f},  Test  perplexity: {val_ppl:.2f}\")\n",
    "\n",
    "avg_train_time = sum(train_times) / len(train_times)\n",
    "print(f\"\\nAverage training time per epoch: {avg_train_time:.1f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0ac4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 7) Text generation with the LSTM LM\n",
    "# ---------------------------------------------------\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def generate_with_lstm(model,\n",
    "                       max_len: int = 30,\n",
    "                       temperature: float = 1.0,\n",
    "                       start_token: str = \"<s>\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate text from the LSTM LM.\n",
    "    - start_token: starting token (usually <s>)\n",
    "    - temperature: >1.0 -> more random, <1.0 -> more greedy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Initialize input sequence with BOS\n",
    "        start_id = vocab_neural.get(start_token, bos_id)\n",
    "        input_ids = torch.tensor([[start_id]], dtype=torch.long).to(device)\n",
    "        generated_ids = [start_id]\n",
    "        hidden = None\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            logits, hidden = model(input_ids, hidden)       # logits: [1, 1, V]\n",
    "            logits = logits[:, -1, :] / max(temperature, 1e-6)  # last time step, temp scaling\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)               # [1, V]\n",
    "            next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if next_id == eos_id:\n",
    "                break\n",
    "\n",
    "            generated_ids.append(next_id)\n",
    "\n",
    "            # Next input is the newly generated token\n",
    "            input_ids = torch.tensor([[next_id]], dtype=torch.long).to(device)\n",
    "\n",
    "        # Convert IDs back to tokens (skip <s>, </s>, <pad>)\n",
    "        tokens = [\n",
    "            id2token_neural[i]\n",
    "            for i in generated_ids\n",
    "            if id2token_neural[i] not in (\"<s>\", \"</s>\", \"<pad>\")\n",
    "        ]\n",
    "        return tokens\n",
    "\n",
    "def join_tokens(tokens: List[str]) -> str:\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "print(\"\\n=== Sample generations: LSTM neural language model ===\")\n",
    "for i in range(3):\n",
    "    gen_tokens = generate_with_lstm(model, max_len=30, temperature=1.0)\n",
    "    print(f\"[LSTM sample {i+1}]:\", join_tokens(gen_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd083e",
   "metadata": {},
   "source": [
    "# Task 4: Transformer Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6489ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import evaluate\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c39ff809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_clean_transformer', 'label', '__index_level_0__'],\n",
       "        num_rows: 40000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['review_clean_transformer', 'label', '__index_level_0__'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_clean_transformer', 'label', '__index_level_0__'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert \"review_clean_transformer\" in df.columns, \"Run Task 2 first\"\n",
    "\n",
    "# Map labels to 0/1\n",
    "df_hf = df.copy()\n",
    "df_hf[\"label\"] = df_hf[\"sentiment\"].map({\"negative\": 0, \"positive\": 1})\n",
    "\n",
    "train_df, temp_df = train_test_split(df_hf, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "hf_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df[[\"review_clean_transformer\", \"label\"]]),\n",
    "    \"validation\": Dataset.from_pandas(val_df[[\"review_clean_transformer\", \"label\"]]),\n",
    "    \"test\": Dataset.from_pandas(test_df[[\"review_clean_transformer\", \"label\"]]),\n",
    "})\n",
    "\n",
    "hf_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec9453d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": metric_acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1\": metric_f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a808c068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40000/40000 [00:04<00:00, 8337.61 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 7783.71 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 8324.15 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_clean_transformer', 'label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 40000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['review_clean_transformer', 'label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_clean_transformer', 'label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MiniLM: small BERT-like model with WordPiece tokenization\n",
    "model_name_minilm = \"microsoft/MiniLM-L12-H384-uncased\"\n",
    "\n",
    "tokenizer_minilm = AutoTokenizer.from_pretrained(model_name_minilm)\n",
    "\n",
    "MAX_LENGTH = 256  # adjust as desired\n",
    "\n",
    "def tokenize_minilm(batch):\n",
    "    return tokenizer_minilm(\n",
    "        batch[\"review_clean_transformer\"],\n",
    "        truncation=True,        # cut off longer sequences\n",
    "        max_length=MAX_LENGTH,  # hard cap at 256 (or 512)\n",
    "        padding=False           # let DataCollatorWithPadding handle dynamic padding\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_minilm = hf_dataset.map(tokenize_minilm, batched=True)\n",
    "tokenized_minilm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16aa9813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "data_collator_minilm = DataCollatorWithPadding(tokenizer=tokenizer_minilm)\n",
    "\n",
    "model_minilm = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name_minilm,\n",
    "    num_labels=2\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84f07d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nv/z73q9gjn4cj9r9b660mdfdf40000gn/T/ipykernel_91553/3518800498.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_minilm = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args_minilm = TrainingArguments(\n",
    "    output_dir=\"./minilm_sentiment\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,        # you can increase to 3–4 if you have GPU\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=False,  # simple setup\n",
    ")\n",
    "\n",
    "# trainer_minilm = Trainer(\n",
    "#     model=model_minilm,\n",
    "#     args=training_args_minilm,\n",
    "#     train_dataset=tokenized_minilm[\"train\"],\n",
    "#     eval_dataset=tokenized_minilm[\"validation\"],\n",
    "#     tokenizer=tokenizer_minilm,\n",
    "#     data_collator=data_collator_minilm,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "# use only 8k for training, 2k for validation\n",
    "train_small = tokenized_minilm[\"train\"].shuffle(seed=42).select(range(8000))\n",
    "val_small   = tokenized_minilm[\"validation\"].shuffle(seed=42).select(range(2000))\n",
    "\n",
    "trainer_minilm = Trainer(\n",
    "    model=model_minilm,\n",
    "    args=training_args_minilm,\n",
    "    train_dataset=train_small,\n",
    "    eval_dataset=val_small,\n",
    "    tokenizer=tokenizer_minilm,\n",
    "    data_collator=data_collator_minilm,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b56b68d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I747530/Desktop/NLP/Project_1/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 06:53, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.250500</td>\n",
       "      <td>0.260514</td>\n",
       "      <td>0.898500</td>\n",
       "      <td>0.898461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.188500</td>\n",
       "      <td>0.244781</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.915988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I747530/Desktop/NLP/Project_1/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/I747530/Desktop/NLP/Project_1/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniLM (validation) metrics:\n",
      "{'eval_loss': 0.2447807639837265, 'eval_accuracy': 0.916, 'eval_f1': 0.9159879022579251, 'eval_runtime': 13.7665, 'eval_samples_per_second': 145.28, 'eval_steps_per_second': 9.08, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "train_result_minilm = trainer_minilm.train()\n",
    "eval_minilm_val = trainer_minilm.evaluate()\n",
    "\n",
    "print(\"MiniLM (validation) metrics:\")\n",
    "print(eval_minilm_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5361db84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I747530/Desktop/NLP/Project_1/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniLM Test Accuracy: {'accuracy': 0.9048}\n",
      "MiniLM Test Macro-F1: {'f1': 0.9047853597969224}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_test_minilm = trainer_minilm.predict(tokenized_minilm[\"test\"])\n",
    "logits_test_minilm = pred_test_minilm.predictions\n",
    "labels_test_minilm = pred_test_minilm.label_ids\n",
    "pred_labels_minilm = logits_test_minilm.argmax(axis=-1)\n",
    "\n",
    "acc_test_minilm = metric_acc.compute(predictions=pred_labels_minilm, references=labels_test_minilm)\n",
    "f1_test_minilm = metric_f1.compute(predictions=pred_labels_minilm, references=labels_test_minilm, average=\"macro\")\n",
    "\n",
    "print(\"MiniLM Test Accuracy:\", acc_test_minilm)\n",
    "print(\"MiniLM Test Macro-F1:\", f1_test_minilm)\n",
    "\n",
    "cm_minilm = confusion_matrix(labels_test_minilm, pred_labels_minilm)\n",
    "disp_minilm = ConfusionMatrixDisplay(confusion_matrix=cm_minilm, display_labels=[\"negative\", \"positive\"])\n",
    "disp_minilm.plot(cmap=\"Blues\")\n",
    "plt.title(\"MiniLM Confusion Matrix (Test)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f2f70b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40000/40000 [00:04<00:00, 8989.39 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 8611.67 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 8817.07 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_clean_transformer', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 40000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['review_clean_transformer', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_clean_transformer', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_distil = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer_distil = AutoTokenizer.from_pretrained(model_name_distil)\n",
    "\n",
    "def tokenize_distil(batch):\n",
    "    return tokenizer_distil(\n",
    "        batch[\"review_clean_transformer\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=False\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_distil = hf_dataset.map(tokenize_distil, batched=True)\n",
    "tokenized_distil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0be4ee2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "data_collator_distil = DataCollatorWithPadding(tokenizer=tokenizer_distil)\n",
    "\n",
    "model_distil = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name_distil,\n",
    "    num_labels=2\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f063a0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nv/z73q9gjn4cj9r9b660mdfdf40000gn/T/ipykernel_91553/1751528350.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_distil = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args_distil = TrainingArguments(\n",
    "    output_dir=\"./distilbert_sentiment\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer_distil = Trainer(\n",
    "    model=model_distil,\n",
    "    args=training_args_distil,\n",
    "    train_dataset=train_small,\n",
    "    eval_dataset=val_small,\n",
    "    tokenizer=tokenizer_distil,\n",
    "    data_collator=data_collator_distil,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a42fb3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I747530/Desktop/NLP/Project_1/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 07:25, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.242400</td>\n",
       "      <td>0.277387</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.889603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.124800</td>\n",
       "      <td>0.293898</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.909993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I747530/Desktop/NLP/Project_1/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/I747530/Desktop/NLP/Project_1/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT (validation) metrics:\n",
      "{'eval_loss': 0.29389840364456177, 'eval_accuracy': 0.91, 'eval_f1': 0.9099927094094622, 'eval_runtime': 17.5441, 'eval_samples_per_second': 113.998, 'eval_steps_per_second': 7.125, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "train_result_distil = trainer_distil.train()\n",
    "eval_distil_val = trainer_distil.evaluate()\n",
    "\n",
    "print(\"DistilBERT (validation) metrics:\")\n",
    "print(eval_distil_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddb7362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I747530/Desktop/NLP/Project_1/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT Test Accuracy: {'accuracy': 0.9012}\n",
      "DistilBERT Test Macro-F1: {'f1': 0.9011857707509882}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAXddJREFUeJzt3QV4U2cXB/DTIqVIcXctrsN9OAwZMFyGDoYzoGPDdYPhQ8YYtsGw4e7DHYYO11FkWKFQoDTf83/5bpa0TUmaVO7t/8dznza5Nzc3QnNyznnf62YymUxCREREZBDuUX0ARERERK7E4IaIiIgMhcENERERGQqDGyIiIjIUBjdERERkKAxuiIiIyFAY3BAREZGhMLghIiIiQ2FwQ0RERIbC4IaizLBhw8TNzc1l+/v8888lS5YsVtdh/7gfcp1ff/1VcufOLXHixJEkSZJE+/eF3t24cUM9H/Pnz3fpfseNG6dex6CgIIkuzp8/L7Fjx5azZ89G9aGQzjG4IZfAH178AdaWePHiSbp06aRGjRoydepUef78uUvu5+7du+rD79SpUy45TiypUqWSypUry6ZNm0JsH3xby6VLly5WgZXlOg8PD8mVK5cMGTJEAgIC1DYIvMLan7bY8yG2atUqqVWrlqRIkULixo2rnusmTZrIzp07JSL9/fff6rFmz55dfv75Z5k9e7YYifYadOzYMdT13377rXmbf//91+H9b9y4MVoE235+fvL999+Lj4+PuLu7h3j/2lqwnSssXrxYJk+eHOL6vHnzSp06ddT/GyJnuPHcUuQK+EBu166djBgxQrJmzSpv376Ve/fuye7du2Xbtm2SKVMmWbt2rRQsWNB8m8DAQLUgELLXsWPHpHjx4jJv3rwQf2hxn/gWisDC/AZ3c5OhQ4eaP1CCHydOrXb//n11/blz52TdunXyySefWN2+WrVq0qZNmxDHguClRIkS6nccy5IlS2TOnDnq8rNnz2TNmjXqsbdo0UIWLVokq1evlhcvXlh90P3+++8yadIkFaRoypQpI9myZQv18eN427dvr463SJEi0rhxY0mTJo34+vqqgOf48eOyf/9+tY+IMGvWLOnatatcvnxZcuTIESH3EZ73hatogTkWvC8QOFrC64LnGgHrw4cPrV43e3Tv3l2mT5+uXkd7YdvXr1+rTFmsWLHEFRBY4P8FHiMe68GDB+Xq1avm9devX1cBRufOnaV8+fLm6xHUli5d2un7x/8xZGeQlQoOXzJq164tV65cUfdHFC4IboicNW/ePPy1Nh09ejTEuh07dpg8PT1NmTNnNr18+dKp+8H+cT+4P3tg26FDh37wOB8/fmyKEyeOqUWLFiFu361btw/eT9u2bU0JEiSwui4oKMhUqlQpk5ubm+nevXshbjN+/Hi1/+vXr9v1WCxv07t3b7X/4BYuXGg6fPiwKaIMHz5c3f/Dhw9NRoTH1qBBA5O7u7tp9erVVuv279+v1jdq1CjczwHeS/b+2X379q3p9evXpohQsGBBU6tWrVz2/8xRderUUX8PQvPmzRtT0qRJTYMHD46Q+6aYgWUpinAff/yxDB48WG7evCm//fZbmL0VyHSUK1dO9XIkTJhQvL295ZtvvlHrkAVC1gaQfQlexgmt58ZeuD9PT09V73cVHBseCz4zr1275vT+Xr16JWPHjlV9Ej/88EOofSmtW7c2Z5MA9/vZZ59JsmTJJH78+FKqVCnZsGGD1W3wvGJfy5Ytk9GjR0uGDBnUt/kqVaqob88aPLf4tg8pU6a06mey1duE21hm2JBdGz58uOTMmVPdR/LkydVzhNc9rPcFMjkjR45U3+SRmcN+8b5ARiP4/SErsG/fPvU84D6QbVm4cKHdz3P69OmlQoUKqnRiCdm3AgUKSP78+UPcZu/evep5RoYSx5cxY0bp06ePes00eB6QtdGeL22x7KvB64qsivY40YMSvOfmwYMH6vmvVKmSVQYIr1WCBAmkadOmYT4+ZGVOnz4tVatWFUcdPnxYatasKYkTJ1bvp4oVK6pMoSWUoHv37q1eCzwGlH2R/Txx4oRaj+PGexB/D7TnwPL/LTJU2AaZT6Lwct1fcqIw4EMXH0Zbt26VTp06hboNykL4YELpCmUj/GHEH2ztj2eePHnU9cHT5eEpwaBshJ4JfDjgw2LatGmqZNSqVasQ26IEEVp/hZeXV4iyRXBa2j1p0qTiLHxgP378WH1w2FOeQMkBz83Lly+lZ8+eKpBYsGCB1KtXT1asWCGffvqp1fbfffed6r/o16+fen7QcNqyZUv1gQb40EWQgPLXzJkzVfBpWWa0BwIXBGjoaUHwgd4PlBrxwYcPQFuwPY4dZbivvvpKHRP2c+HCBXU8lvCewXYdOnSQtm3byty5c1VgUaxYMcmXL59dx4lSYq9evdR7Ao8TwdXy5culb9++5h4qS1iH5xklOzzPR44cUe+pO3fuqHXwxRdfqJ4xBHJoyg4Nyq3YP97feP8jKA3e8ItgAc8/gincB15bbIPHmChRIpkxY0aYj+3AgQPqZ9GiRcUR6OdCnxeeRwS5eK/gePHlBcGdFlSjFw3vL5Tg0EPz6NEj9d7Fa4X7RN8S3l94blCSBTzHlnAfCG7w/sD/MyKHRXXqiIxfltIkTpzYVKRIEfNllIss34KTJk36YLo/rHQ5SkPBU922ylLBFw8PD9P8+fND7DO0bbXl999/D1GWwrFjuXLliumHH35QJan8+fOHWkJytCw1ZcoUtf2qVavs2h6lK2y/d+9e83XPnz83Zc2a1ZQlSxbTu3fv1HW7du1S2+XJk8eqDKLd35kzZ0K8ZsFfo+DPswavB54bTaFChVRJIizB3xenTp1Slzt27Gi1Xb9+/dT1O3futLo/XLdnzx7zdQ8ePFCv71dffRXm/WqPA6UjlCnjxo1r+vXXX9X1GzZsUK/ljRs3Qn0OQiu3jh07Vt3m5s2bHyxL4T2A6728vNTxhrYu+Hu+efPmpvjx45suXbpkfi8FL6WFZtCgQWpbvBfs/X+G92/OnDlNNWrUsHov43Hj/VStWjWr/+cfKuWGVZaCxYsXq/uPyBIrGRvLUhRp8O0srFFT2rBifGOL6OGpKA/gGzQWlMowWgrZgZUrV4bYtn79+uZtLRfcxpK/v78qF2BBsy0yIGXLllWPxxVDm/EtFvDt3B5oWMa3aZR9LF8DZAWQUULJwxJKfZaZKC0z5oqSmuVrjAwdGpLthccByJpYQgYHgpfZkC2wbILF64HypiOPA5k2lF/Q8A0oUSELljlz5lC3R0nT8n2ATB+2R7x08uRJu++3UaNG6njt8eOPP6ryELJUKPsiO4r36ocgk4Lya/BsSVgwOhGvGTJauD0eHxY8VpQv9+zZY/4/i9cYmTVkqcJLy3SGZ0QaEbAsRZEGKX6k1G1BrwBGGyHI+Prrr9UfzYYNG6o/3kiBuxI+9D/66CPz5ebNm6vRR0ilozRm+SGPHhR7+hPQ34HRVoCUO8o6KHlZfvA5Q0vP2zusHj0NJUuWDHE9ynvaesv+EfSLhPYB8+TJE3EVlBXxAYyRZrhvBBD4UA6rvIXjxOsffHQWRonhgxTrLQV/HNpjcfRx4IMcx3br1i010g2vpy3YBuVSjAgMfj8owdgLI/jshZIVpllAeSp16tTq94iiBaMo89mCx4nnGc8TtkPfEcpLGPmE0Ya2RgCGRusl4nxHFF7M3FCkwIc9/viFNXwYQQC+AW7fvl19qKDpEQEPejHevXsXoceHD09kYjDM15GsgiX0wSAIwoL+hx07dqjh8Oi1cAU0EsOZM2ckItjq43Fk2HJwwV83NOpiyDH6YBDcIJhFH4Y2hD4s9n7QuepxoDcJfS/4oEbjMuYRsvUY8R5FBgnzxiAQQmZPawB2JAvpaCC8ZcsW9RMBFf6P2QM9QeghcmTuKe0xjB8/PtQsJhYtE4TnCVky9ANh/iXcBr1Ooc0jZYsWIDo61J5Iw+CGIoXWQIlJ/T4UZCBjM3HiRFU2wegdNDLu2rUrwr/J4Q8+WM5F44y0adOqETPI5hw6dMjp/aG8hG/GKJXYE+yhhHLx4sVQJ+LT1rsKjuvp06dW171580YFi6FlHFACw+O4ffu2ytqENbEdjhMfrsGDTjRM4z5d+TiCBxoNGjRQo8kQvNj6oEWweenSJZkwYYIKbpCZQoCLD/bgXPn+3bx5swoKBwwYoEpZCMK097A9QTJGTdlLm28G2UMtgA++YJST5Xv/yy+/VIEe7gcBFf4v2/s84Db4W4AMH1F4MLihCIfgBMN4kXLH6BtbMBIouMKFC6uf2pBfDHWF4B+kzsIQZYzkQjlKK9u4Qo8ePdSQWYxEchb2gw9PjDrBz9AyEegfwkgdQDkAv2OCNg16JDCrMIbeojfFVfDhh6ybJdxP8CAM/RqW8G0f2bzgQ7ot4XFA8BltEQADZrSNKOibwsgg9LR8KFNk+Xrg9ylTpoTY1lXvX9xeG3E2ZswYFeRgxBl+/xBtEj6MUrMXykt4jTFUPbTgHxMaAl7v4GU4lKIR6Fm+xngewirXYTJKZHvQU0QUHuy5IZdC6hmZAXyDxDdrBDZIWePbNfoRwpp1Fv0Y+IDEhxW2R78KhrWi50VrisUfWPRZYKZcNNbijyT6ShzpVbA8TsD9oGEUmQH0+gQfeopv5Zbz82jQ5xDW8GXAN1ZkKfA4EJQ4Gzj1799fNeQiS4BsljZDMcpf+JaMYEYb6ovHguwIhu9iuDAyJhhOjW/Ff/zxh0v7mPBBiyHAaIjFc/LXX3+pkknwbAcCKsxhgg9LHA8+YLVhw7YUKlRIZSUQLOFDHXOr4HHisSCzEryx25Vw31g+lAnB+xKB0D///KPeP3h+Q+vxweMGvB7IYiIwatasmcPHhWHqCBRRwsU+0LuE12DUqFEqcxTWMaP3BSVB3BazXdsD7xUEUHgvIejAexrzAeHx4n2Ix4wMJUpd+P+K9yWOAcEr7ufo0aPqPWv5PCxdulQ1iWPuKmxXt25d8xeNP//8U2V+iMItqodrkTEEH2KNYbRp0qRRQ0QxpNjPz++DQ34xk3H9+vVN6dKlU7fHTwx3xVBXS2vWrDHlzZvXFDt2bKvhquEdCh4vXjxT4cKFTTNnzgwxZDusoeAVK1YMc4ZizdWrV02xYsWyGhId3hmKNStWrDBVr17dlCxZMvU8pE2b1tS0aVPT7t27Q9x348aNTUmSJFGPs0SJEqb169dbbaMNBV++fPkHhyDbGgqOYeU+Pj6mFClSqOHJGDKM4fDBh4KPGjVKHQOOB7NW586d2zR69Gg1K23w+wg+Wy9mR8awY8wknTFjRtPAgQNNAQEBVtvh/kIbao7XyvL1ssWeGalDew7Onz9vqlq1qilhwoTqOejUqZPpr7/+CvH8BQYGmnr06GFKmTKlGiauPU7tucZ7IrjgrwPe/7g8YcIEq+3wfwyPH8PtLZ/P0EycOFEdq60Zw21NuXDy5ElTw4YNTcmTJ1fD63F/TZo0Uf93AVMJ9O/fXx1DokSJ1P8J/D5jxgyr/bx48ULNBo73Ae7H8v/tpk2b1HWXL18O8zEQhYXnliIiimFQEkIGByObMNlhdIJsHHpygk/OSOQIBjdERDEQzgqOGYbRuO/qqRbCC6VbnOIC8+qEdpoLInsxuCEiIiJDiR7hOhEREZGLMLghIiIiQ2FwQ0RERIbC4IaIiIgMhZP4RSOYYh5n0sXkdDxhHBGR/mC6JExmiFmZI3IUWkBAgDrFibPixo0b5uSqesXgJhpBYIMz6RIRkb7hvGmYrTmiAhvPxAlE3th/UlZbMMM5Zi03WoDD4CYaQcZGqZpOJDYrhmRM9xcfjepDIIowz/2eS44suf77ex4BVMYGgU25NCKxnTgZa6BJ7u27p/bH4IYijLkUhcAmDoMbMqbg5+4iMqJIaS3A54QzX4TdHMv8jB07VlauXKnOy+fp6SllypRRk0F6e3ubT36ME83iJMS3bt1SZ6vHjNM4cbLlSVBDe25wHjzL86zt3r1bnXsM59JDRWPQoEHy+eef232s/AQlIiLSI3cXLA7ACU27desmhw4dUidExklOq1evLv7+/ubWCiw4e/zZs2dl/vz5snnz5lBP8YHZsX19fc0LgiANymQ4gTJOiovZqnv37q1ODIuT8dqLMxRHI35+fu+j25oZmLkhw3q16kJUHwJRhP4dT50srTp/V0RlKc2fFVXTO5e5CQwS2f5PuI/14cOHkipVKhX0VKhQIdRtli9fLq1atVIBUOzY7zthtHOHWQY0lnx8fGTDhg0qQNIgq/P06VMVLNmDmRsiIqIYzM/Pz2p5/fq1XbdDUATJkiULcxsETlpgo0EGKEWKFFKiRAmZO3euGmWmOXjwoFStWtVq+xo1aqjr7cXghoiISK/cnFj+Dz0tyARpC3pr7Jm6BOWismXL2jzJ6b///qv6bTp37mx1/YgRI2TZsmWqtNWoUSP58ssvZdq0aeb19+7dk9SpU1vdBpcReL169UrswdFSREREeoTGXGcal93czMPWLctSHh4eH7wpMi8oG+3bty/U9QhE0DeTN29eGTZsmNW6wYMHm38vUqSIKlmNHz9eevbsKa7CzA0REVEM5uXlZbV8KLjp3r27rF+/Xnbt2hXqXD6YxLBmzZpqODx6a+LEiRPm/kqWLCl37twxl8Mw9879+/ettsFlHBtGadmDmRsiIiI9CseIJysO3hZ9MT169FABC4ZqZ82aNdSMDfpjECCtXbvWrvlzMCIqadKk5qCqdOnSsnHjRqttUMLC9fZicENERBSDy1L2Qilq8eLFsmbNGpWVQW8MoE8HGRUENhga/vLlS/ntt9/MDcqAOW9ixYol69atU1mYUqVKqcAHQcuYMWOkX79+5vvp0qWL/PjjjzJgwABp37697Ny5U/XoYASVvRjcEBER0QfNnDlT/axUqVKIOWswwd6JEyfk8OHD6rocOXJYbYO5a7JkyaJKVNOnT5c+ffqoTBC2mzhxonTq1Mm8LTJCCGSwzZQpU1Tpa86cOSojZC/OcxONcJ4bigk4zw0ZWaTOc1Mro3Nzor0NEtl0O0KPNaowc0NERKRH7m7vF2dub1AcLUVERESGwswNERGRHgWbjC9ctzcoBjdERER6FMmjpfSEwQ0REZEeMXNjE3tuiIiIyFCYuSEiItIjjpayicENERGRHrEsZRPLUkRERGQozNwQERHpEUdL2cTghoiISI/Yc2MTy1JERERkKMzcEBER6REbim1icENERKTb4MaZGYrFsFiWIiIiIkNh5oaIiEivDJx9cQaDGyIiIj3iaCmbGNwQERHpERuKbWLPDRERERkKMzdERER6xBmKbWJwQ0REpNfaizP1F3cxLAM/NCIiIoqJmLkhIiLSI5albGJwQ0REpEccLWUTy1JERERkKMzcEBER6RHLUjYxuCEiItIjjpayiWUpIiIiMhRmboiIiPSIZSmbGNwQERHpEUdL2cTghoiISI94VnCb2HNDREREhsLMDRERkR6x58YmBjdERER6xJ4bm1iWIiIiog8aO3asFC9eXBIlSiSpUqWSBg0ayMWLF622CQgIkG7dukny5MklYcKE0qhRI7l//77VNrdu3ZI6depI/Pjx1X769+8vgYGBVtvs3r1bihYtKh4eHpIjRw6ZP3++OILBDRERkS65iZtb+BdRqR/7/fnnnypwOXTokGzbtk3evn0r1atXF39/f/M2ffr0kXXr1sny5cvV9nfv3pWGDRua1797904FNm/evJEDBw7IggULVOAyZMgQ8zbXr19X21SuXFlOnTolvXv3lo4dO8qWLVvsf2ZMJpPJoUdHEcbPz08SJ04sUjODSBzGnWRMr1ZdiOpDIIrQv+Opk6WVZ8+eiZeXV4R+Vrh1ySduHrHCvR/T63dimnUu3Mf68OFDlXlBEFOhQgW1n5QpU8rixYulcePGapu///5b8uTJIwcPHpRSpUrJpk2b5JNPPlFBT+rUqdU2s2bNEh8fH7W/uHHjqt83bNggZ8+eNd9Xs2bN5OnTp7J582a7jo2foERERDGYn5+f1fL69Wu7bodgBpIlS6Z+Hj9+XGVzqlatat4md+7ckilTJhXcAH4WKFDAHNhAjRo11P2eO3fOvI3lPrRttH3Yg8ENERGRjgdLObNAxowZVSZIW9Bb8yFBQUGqXFS2bFnJnz+/uu7evXsq85IkSRKrbRHIYJ22jWVgo63X1oW1DQKgV69eiT04WoqIiEiH3M29M+FjcnOTIBG5ffu2VVkKTbwfgt4blI327dsn0REzN0RERDGYl5eX1fKh4KZ79+6yfv162bVrl2TIkMF8fZo0aVSjMHpjLGG0FNZp2wQfPaVd/tA2ODZPT0+7HhODGyIiIh1yZqSUWziyPhh/hMBm1apVsnPnTsmaNavV+mLFikmcOHFkx44d5uswVBxDv0uXLq0u4+eZM2fkwYMH5m0w8gqBS968ec3bWO5D20bbhz1YliIiItKh8AQoVhy8LUpRGAm1Zs0aNdeN1iODPh1kVPCzQ4cO0rdvX9VkjIClR48eKijBSCnA0HEEMa1bt5Zx48apfQwaNEjtW8sYdenSRX788UcZMGCAtG/fXgVSy5YtUyOo7MXghoiISIciO7iZOXOm+lmpUiWr6+fNmyeff/65+n3SpEni7u6uJu/DqCuMcpoxY4Z521ixYqmSVteuXVXQkyBBAmnbtq2MGDHCvA0yQghkMGfOlClTVOlrzpw5al92PzTOcxN9cJ4bigk4zw0ZWWTOc+PRo5DT89y8nvZXhB5rVGHmhoiIKAaeN1OcuW00x+CGiIhIhyK7LKUnHC1FREREhsLMDRERkQ4xc2MbgxsiIiIdcvv/P2f2YFQsSxEREZGhMHNDRESkQyxL2cbghoiISIc4FNw2lqWIiIjIUJi5ISIi0iF3NYlf+JuCTcbtJ2ZwQ0REpEfsubGNmRsiIiIdYnBjG3tuiIiIyFCYuSEiItIjJ0+caWLPDRERERmpLOXGE2cSERER6QPLUkRERDrEzI1tDG6IiIh0SJ0205mylBi36YajpYiIiMhQmLkhIiLSIZalbGNwQ0REFANPnOlm3KoUy1JERERkLMzcEBER6RDLUrYxuCEiItIhBje2MbghIiLSIXc3N7WEm5txm244FJyIiIgMhZkbIiIiHeJoKdsY3BAREekQe25sY1mKiIiIDIWZGxuGDRsmq1evllOnTkXuK0IO6dewkzQoVU1yZcgmr94EyOG/T8q3CyfI5bs31PqkCRPL4GbdpUrhspIxRVr51++xrDu8Q4b/PlX8Xr5Q2xTI4q32UyZPUUmeKKncfPiPzNmyVKav/9V8P1g3qvVX6n7ix40ntx7elV+2LpNp6xbwFaNINX7JLFm9f6tcunNdPON6SMm8RWR0+/6SK2M28zbdpwyWnacOiO+jB5LQM76Uwvu3Qz/xzpjdvM2ukwdk+MIpcu7GJUkQz1NaVv1Uhn/eR2LH4seCrs4t5cT5odwMfG4pvov/n9pbtWqVNGjQwPzE9OvXT3r06BGVrw3ZoXy+4jJr02I5fuWsxI4VS4a37CPrh/4iRXp+Ii9fv5K0yVKpZeD8cXLhzlXJlDKdTOsyTF3XYnxvtY8i2fPJw2ePpN1kH7nzr6+Uyl1EpncdLu/evVP7Bv+AVzJr4yI5c/OS+Ae8lDJ5i8mPXYap3+duW87XiiLN3jNHpUvdVlIsVwEJDAqUofMmyifftpeTszdKgnjx37+nc+aTZh/Xk4wp08rj589k9G/T5JNv2svf83dKrFix5PS1C9JgSCfxadZVfuk/Tu7+e196TBsi74LeyXedvuarqRMsS9nmZjKZTBLDhRbcRAU/Pz9JnDixSM0MInFYMQyPFF5J5faCA1L129ay//yxULdpWKaGzO09TpI3K6r+mIdmUufBkjtDNqk1pJ3N+1riM1UFPR2m+ITrWGOqV6suRPUhGMrDp48lU7NSsm38IilXoHio25y59reU+LKenJu7XbKlyyRD5k2QHSf2y/5pK83bbDi0U1qN6SW3lhyURPETRuIjMBb8HU+dLK08e/ZMvLy8IvSzItOwiuIeL/w5iqCAQLk17M8IPdaoEqWfoJUqVZKePXvKgAEDJFmyZJImTRpVDtI8ffpUOnbsKClTplRP/Mcffyx//fWX1T5GjRolqVKlkkSJEqltv/76aylcuLB5/dGjR6VatWqSIkUK9WaoWLGinDhxwrw+S5Ys6uenn36qghztMo5D28/WrVslXrx46ngs9erVSx2TZt++fVK+fHnx9PSUjBkzqsfm7+/v8ueNbPOKn0j9fPLiWZjboCRlK7CBxPETypPntvdRKGseKeldWPaeO8qXg6KU38vn6mfSRIlDXY/s4sJtKyVLmgySIWUadd3rt28kXlwPq+1Q4gp481pOXj4XCUdNrszcOLMYVZSnBxYsWCAJEiSQw4cPy7hx42TEiBGybds2te6zzz6TBw8eyKZNm+T48eNStGhRqVKlijx+/FitX7RokYwePVq+//57tT5Tpkwyc+ZMq/0/f/5c2rZtqwKPQ4cOSc6cOaV27drqei34gXnz5omvr6/5siXcZ5IkSeSPP/4wX4eSxdKlS6Vly5bq8tWrV6VmzZrSqFEjOX36tFqH++zevXsEPntkCf9Rx3cYKAcuHJfzty6H+uQkT5REBn7WVeZuW2bzySvlXVgal62lemqCu/LzLnm67C/ZP365/LTpd5m/fQVfBIoyQUFB0n/WaCmdt6jky5LLat1P6xZJigaF1bL16J+yYcx8iRsnrlpXrVh5OXThpCzdtV79Lfvn33syZvF0tc738cMoeSwU/qHgzixGFeXBTcGCBWXo0KEq6GjTpo189NFHsmPHDhUYHDlyRJYvX66uw/offvhBBRkrVrz/QJk2bZp06NBB2rVrJ7ly5ZIhQ4ZIgQIFrPaPzEqrVq0kd+7ckidPHpk9e7a8fPlS/vzzT7UeWSHAfpE50i5bQo26WbNmsnjx+/4LwDEik4NgBsaOHasCnd69e6tjLVOmjEydOlUWLlwoAQEBoT72169fq/Si5ULhN7nzEMmXKae0mfBVqOsTeSaQVYNmyYU7V2TUkvd/yIPLmymnLBs4XUYvnSE7/joQYn2Vb1tJ2X6NpcdPw6V73TbSpFxtvmQUZXpPHy7nblyWhQMnh1iHnptD01erclXO9FlVyQmZGaharJyM6TBAek4bIonr5peCHWpIjeIV1Tp3dwN/4pFT9uzZI3Xr1pV06dKpL5MYdGPJVnZo/Pjx5m1QHQm+/rvvvrPaDxIEqIKgYoIqCBIfugxuLKVNm1Zla1B+evHihSRPnlwSJkxoXq5fv66yJHDx4kUpUaKE1e2DX75//7506tRJBRwoS6G8hf3eunXLoeNE4LJ79265e/euOWtUp04dFRQBjnf+/PlWx1qjRg31zQrHHBoERDgmbcGLSOEzqdMgqf1RRakxuK388+h+iPUJ48WXtUN+luevXkrT73pI4LvAENvkzpBdNg6fq7I636+YFer93Hzwj5y7dVnmbVsu09YukG+bMTNHURfYbDy8S7aMW2guN1lKnCCR5EifRfXhLB40VS7eviZr9r/PikOvRu3l3h/H5dKvu+XOssNSt3QVdX3WNPw7pBeRXZby9/eXQoUKyfTpoX85RPXDcpk7d666Dy0JoEGFxnI7y8E7+JJfvXp1yZw5s6rIIDBCmwgSE7oaLRUnThyry3giEBAgAEGgg4AiOC2gsAdKUo8ePZIpU6aoJ8vDw0NKly4tb968ceg4ixcvLtmzZ5clS5ZI165dVQMyghkNjveLL75QfTbBoVwWmoEDB0rfvn2tXlQGOOELbOqVrCrVB7dVwUdoGZt1Q+eoPoPGY75UP4PLkzGHbBoxTxbtWiPDFk2x637d3d3F4/9pfqLIgjEgfWaMkLUHtsnWcb9JFjuCEQwbMYlJ3gR77+PvbbrkqdXvy3ZvkAwp00qRHPki7NhJ36OlatWqpRZbUP2wtGbNGqlcubJky/bfNAWAHtng22qQOMDnMwKjuHHjSr58+dSULBMnTpTOnTvrJ7ixBf019+7dk9ixY5ubfIPz9vZWPTIoZ2mC98zs379fZsyYofps4Pbt2/Lvv/+GCLBQd7Yne4MnPkOGDOqDDZkby+M9f/685MiRw+7HiEALCzlXimpaoY58Nra7vHjlL6mTpFDXP3v5XKXgEdhgaLinRzxpN3mAeMVPqBZ46PdYBdIoRSGw2X5yv0xdO9+8DzQc/+v3RP3+Ra0WcvvhXbn4z/ssXLm8H0nv+u1kxobf+PJRpGdslu5aJ8uHzpSEngnk3v97ZJCpwfv8uu8tWfHnRqlSrJykSJxM9dNMWDpbPOPGkxol3peeYOLyOVL9o/Li7uYua/ZvlR+WzZbfvpmsyvCkE842BbtFXAkSVZMNGzaovtrgUIYaOXKk+uLfokUL6dOnj/qsh4MHD0qFChVUYKNBFQS9tU+ePJGkSZPqO7ipWrWqyrBgeDbqbeipQUkITxZGNqEPB6kslJzwO3pc0MSLWp1llIhy1K+//qq2QWakf//+ajSTJQRP6KEpW7asCjZsPXkIbpAeQxNz48aNrQITHx8fKVWqlGogxqgtNEkj2EFz9I8//hiBz1TM9kWt5urntlELra7vNHWg/LZrtRTOlldKeBdS152fudVqG+/OVdRkfJ+Wri6pEieXFpXqqUWDLFDuL6qq33Hm3RGt+0qWVOkl8N07uXbvtgxaOEHmbF0aCY+S6D+z17/v/as+oJXV0zK773fSunpD8YjrIfvPHZMfVy+QJy/8JFWS5Ko0tWviEvW7ZuuxPTJuyUyVySyQLbcsHzrD3HdDMYtfsH5PV3zxRlCDDE3Dhg2trkd1A8kAjJA+cOCAqmCgNIXMDCCpkTVrVqvbpE6d2rxO98ENotGNGzfKt99+qxqGHz58qNJYiOi0B4pg49q1a2rCPTTtNmnSRD7//HPViKz55ZdfVCoLTyZKPmPGjFHbW5owYYIqD/3888+SPn16uXHj/ey2wSErg54e7H/y5MkheofQpIzjRSMUUscoYzVt2jRCnh96z/PTPGE+FRiq/aFtRi9FA3HoNWTNzI2L1EIU1V5tvhTmepSZVo+c88H9bP7e+gsB6Y+rTpyZMVi/Jwb5WE7LEh4oK+EzGk3BlixbMfC5iQwNWjrQg+rKSobhJvHDnDYIgpCt0RtO4kcxASfxIyOLzEn8coytKrGcmMTvXUCgXBm4XbVrWB6rPZmbsCa/3bt3r0pEoFcGDchhOXfunOTPn1/+/vtv1WqCNhM8PsuRWLt27VIjnzENjO4zN/bAkO5Zs2apehzqxL///rts377dPE8OERERhQ2BjSsDMVRMihUr9sHABhAAoYcVk/EC2lFQAXn79q15wBE+0xH42BvY6D640UpX6IFBWQoPHhPtoV+HiIjI+GUpZ0ZLiUMwKvjKlSvmy5jmBMEJ+me0UcHIumB+OrR7BIdmYUzYixFU6MfBZTQTYy46LXBBg/Hw4cPVHHboZT179qwa7Txp0iSHjlXXwQ0ag5GpISIiimkieyj4sWPHVGASvH8GU65oU6NguhR0uzRv/n6whyWUurAe/TyYxBaNwwhuLPtwUG7DKY+6deumsj84dRIm6HVkGLghe270jD03FBOw54aMLDJ7bnJ9X01ixbOeK84R7wLeyiWfbYY8caauMzdEREQxFfIuTo2WEuNicENERKRDkV2W0pMoP7cUERERkSsxc0NERKRDzNzYxuCGiIhIhxjc2MbghoiIKAaffsGI2HNDREREhsLMDRERkQ6xLGUbgxsiIiI9Yl3KJpaliIiIyFCYuSEiItIhlqVsY3BDRESkQ6xK2cayFBERERkKMzdEREQ6xLKUbQxuiIiIdIjBjW0sSxEREZGhMHNDRESkQ8zc2MbghoiISIc4Wso2BjdEREQ6xMyNbey5ISIiIkNh5oaIiEiP3NxU9saZ2xsVgxsiIiIdYlnKNpaliIiIyFCYuSEiItIhZm5sY3BDRESkQxwKbhvLUkRERGQozNwQERHpkBv+OTHiyU04WoqIiIiiEfbc2MayFBERERkKy1JEREQ6xMyNbQxuiIiIdIijpWxjcENERKRDzNzYxp4bIiIiMhRmboiIiPQII7mdOnGmGBYzN0RERDouSzmzOGLPnj1St25dSZcunbrt6tWrrdZ//vnnIfZfs2ZNq20eP34sLVu2FC8vL0mSJIl06NBBXrx4YbXN6dOnpXz58hIvXjzJmDGjjBs3ThzF4IaIiIg+yN/fXwoVKiTTp0+3uQ2CGV9fX/Py+++/W61HYHPu3DnZtm2brF+/XgVMnTt3Nq/38/OT6tWrS+bMmeX48eMyfvx4GTZsmMyePVscwbIUERGRDrm7vV+cub0jatWqpZaweHh4SJo0aUJdd+HCBdm8ebMcPXpUPvroI3XdtGnTpHbt2vLDDz+ojNCiRYvkzZs3MnfuXIkbN67ky5dPTp06JRMnTrQKgj6EmRsiIiIdiuyylD12794tqVKlEm9vb+natas8evTIvO7gwYOqFKUFNlC1alVxd3eXw4cPm7epUKGCCmw0NWrUkIsXL8qTJ0/EXszcEBERxWB+fn4hsi9YHIWSVMOGDSVr1qxy9epV+eabb1SmBwFLrFix5N69eyrwsRQ7dmxJliyZWgf4idtbSp06tXld0qRJ7ToWBjdEREQ65O7mphZnbg9o2rU0dOhQ1efiqGbNmpl/L1CggBQsWFCyZ8+usjlVqlSRyMTghoiIKAZP4nf79m01ekkTnqxNaLJlyyYpUqSQK1euqOAGvTgPHjyw2iYwMFCNoNL6dPDz/v37Vttol2318oSGPTdEREQ65O6CBRDYWC6uCm7u3Lmjem7Spk2rLpcuXVqePn2qRkFpdu7cKUFBQVKyZEnzNhhB9fbtW/M2GFmFHh57S1LA4IaIiIg+CPPRYOQSFrh+/br6/datW2pd//795dChQ3Ljxg3ZsWOH1K9fX3LkyKEagiFPnjyqL6dTp05y5MgR2b9/v3Tv3l2VszBSClq0aKGaiTH/DYaML126VKZMmSJ9+/YVR7AsRUREpENuTvbcuDl422PHjknlypXNl7WAo23btjJz5kw1+d6CBQtUdgbBCuarGTlypFUmCEO9EdCgTIVRUo0aNZKpU6ea1ydOnFi2bt0q3bp1k2LFiqmy1pAhQxwaBg4MboiIiHQosk+cWalSJTGZTDbXb9my5YP7wMioxYsXh7kNGpH37t0rzmBZioiIiAyFmRsiIqIYPBTciBjcEBER6VBkl6X0hGUpIiIiMhRmboiIiHTIcq6a8N4+Rgc3a9eutXuH9erVc+Z4iIiIyA7suXEyuGnQoIHd9bt3797ZtS0RERFRlAU3mBqZiIiIog82FEdQz01AQIDEixfPmV0QERFROLAs5cJ+IpSdMJ1y+vTpJWHChHLt2jV1/eDBg+WXX35xdHdEREQUDm4uWIzK4eBm9OjRMn/+fBk3bpw6uZUmf/78MmfOHFcfHxEREVHEBjcLFy6U2bNnS8uWLSVWrFjm6wsVKiR///23o7sjIiIiJ8pSzixG5XDPzT///KNOYR5a0/Hbt29ddVxEREQUBndx8vQLYtzgxuHMTd68eUM9W+eKFSukSJEirjouIiIiosjJ3AwZMkTatm2rMjjI1qxcuVIuXryoylXr168P31EQERGRQzgU3IWZm/r168u6detk+/btkiBBAhXsXLhwQV1XrVo1R3dHRERE4QxunOm3cWPPjbXy5cvLtm3b+GYkIiIi40zid+zYMZWx0fpwihUr5srjIiIiojA4O1eNm4GfXYeDmzt37kjz5s1l//79kiRJEnXd06dPpUyZMrJkyRLJkCFDRBwnERERWeAMxS7suenYsaMa8o2szePHj9WC39FcjHVEREREusrc/Pnnn3LgwAHx9vY2X4ffp02bpnpxiIiIKOIxc+PC4CZjxoyhTtaHc06lS5fO0d0RERFROGCwkzMjntwM3HTjcFlq/Pjx0qNHD9VQrMHvvXr1kh9++MHVx0dERESh4OkXnMzcJE2a1Co69Pf3l5IlS0rs2O9vHhgYqH5v3769NGjQwJ5dEhEREUVdcDN58uSIuXciIiIKFw4FdzK4wekWiIiIKPpgQ3EETOIHAQEB8ubNG6vrvLy8nNklERERUeQGN+i38fHxkWXLlsmjR49CHTVFREREEYuZGxeOlhowYIDs3LlTZs6cKR4eHjJnzhwZPny4GgaOM4MTERFR5J0V3JnFqBzO3ODs3whiKlWqJO3atVMT9+XIkUMyZ84sixYtkpYtW0bMkRIRERFFROYGp1vIli2bub8Gl6FcuXKyZ88eR3dHRERE4fwAd3YxKocfGwKb69evq99z586tem+0jI52Ik0iIiKKYM6WpNyMW5ZyOLhBKeqvv/5Sv3/99dcyffp0iRcvnvTp00f69+8fEcdIREREFHE9NwhiNFWrVpW///5bjh8/rvpuChYs6OjuiIiIKBw4WiqC5rkBNBJjISIiosjD4MbJ4Gbq1Klir549e9q9LREREYWPs8O53Ry8LQYN4eTZqNb4+vrKqlWrzOeTfPv2rQwaNEg2btwo165dk8SJE6vqznfffaemitFkyZJFbt68abXfsWPHqjYXzenTp6Vbt25y9OhRSZkypTpZN6ahcXlwM2nSJLufKAY3RERExuPv7y+FChVSJ8lu2LCh1bqXL1/KiRMnZPDgwWqbJ0+eSK9evaRevXpy7Ngxq21HjBghnTp1Ml9OlCiR+Xc/Pz+pXr26CoxmzZolZ86cUfeHAUudO3d2bXCjjY6iyOG76DBPY0GG5VkzV1QfAlHECQyKtGfXXdzU4sztHVGrVi21hAaZmm3btlld9+OPP0qJEiXk1q1bkilTJqtgJk2aNKHuB/Pl4bROc+fOlbhx40q+fPnk1KlTMnHiRIeCGyMPcyciIjIsV81Q7OfnZ7W8fv3aJcf37NkzdR/Bp4lBqSp58uRSpEgRVeYKDAw0rzt48KBUqFBBBTaaGjVqyMWLF1U2yF4MboiIiGKwjBkzqsyLtqAHxlk4sTbOQ9m8eXOrSgRaV5YsWSK7du2SL774QsaMGWPVT3Pv3j1JnTq11b60y1gXaaOliIiISL+jpW7fvm0VgOC8kc5Ac3GTJk3EZDKp81Ba6tu3r/l3TB+DDA2CHARUzt6vJQY3REREOuT2/3/O3B4Q2FgGN64IbDAiCifZ/tB+S5YsqcpSN27cEG9vb9WLc//+fatttMu2+nRCw7IUERERuSywuXz5smzfvl311XwImoXd3d0lVapU6nLp0qXVkHPsS4NGZQQ+SZMmjdjgZu/evdKqVSt1EP/884+67tdff5V9+/aFZ3dEREQURQ3F9nrx4oUKRrBoI6nxO0ZDIRhp3LixGvaNEU/v3r1TPTJYMPpJaxaePHmyOoUT5sLBdjjrAeIJLXBp0aKFKlV16NBBzp07J0uXLpUpU6ZYlbMiJLj5448/VOeyp6ennDx50txVja5oNAYRERFR5PXcOLM4AoELRjhhAQQc+H3IkCEq0bF27Vq5c+eOFC5cWNKmTWteDhw4oLZHTw2aiStWrKiGeI8ePVoFN7NnzzbfBxqat27dqgKnYsWKyVdffaX278gw8HD13IwaNUpNrNOmTRt1kJqyZcuqdURERGQ8lSpVUk3CtoS1DooWLSqHDh364P2g0RgVImc4HNxgrDnGoAeHaOvp06dOHQwRERHZ5/0UfuFvnXUzcNutw48M3cpXrlwJcT36bbJly+aq4yIiIqIwqNDGmbKUhH+kleGCG5wPAueLOHz4sGpGunv3rmoK6tevn3Tt2jVijpKIiIisuTnXVCzGjW0cL0vhzJ1BQUFSpUoVdaIslKjQJITgBmfuJCIiItJVcINo79tvv5X+/fur8hSGhuXNm1cSJkwYMUdIREREETaJnxGFe4ZijENHUENERET6Pf2CETkc3FSuXDnMiX8w3TIRERGRboIbTM5jCbMSYobCs2fPStu2bV15bERERGRDeGYZtuTMbQ0X3EyaNCnU64cNG6b6b4iIiCjiuf//nzO3NyqXPTKcG2Lu3Lmu2h0RERFR5DYUB4cTYsWLF89VuyMiIqIwsCzlwuCmYcOGIc4l4evrq06oNXjwYEd3R0REROHA4MaFwQ3OIWXJ3d1dvL29ZcSIEVK9enVHd0dEREQUdcHNu3fvpF27dlKgQAFJmjSpa4+EiIiI7Pb+tJlOzHMjxh0t5VBDcaxYsVR2hmf/JiIiilrOnFfKzclh5IYbLZU/f365du1axBwNERER2cWpM4K7OTe7seGCm1GjRqmTZK5fv141Evv5+VktRERERLrouUHD8FdffSW1a9dWl+vVq2eV0sKoKVxGXw4RERFFLJ440wXBzfDhw6VLly6ya9cue29CREREEcTdzV0tztxeYnpwg8wMVKxYMSKPh4iIiCjyhoIbubOaiIhITziJn4uCm1y5cn0wwHn8+LEjuyQiIiInum7Cz82wz7tDwQ36boLPUExERESk2+CmWbNmkipVqog7GiIiIrKLs3PVuBu41cTu4Ib9NkRERNEHh4Lb5u7oaCkiIiIiQ2RugoKCIvZIiIiIyG7ubs6VltyNW5VyrOeGiIiIogc3N3e1OHN7o2JwQ0REpEPsubHNuGEbERERxUjM3BAREekQh4LbxuCGiIhIh3j6BdtYliIiIiJDYeaGiIhIh9zFTS3O3N6omLkhIiLScVnKmcURe/bskbp160q6dOnUbVevXh1ist8hQ4ZI2rRpxdPTU6pWrSqXL18OcXLtli1bipeXlyRJkkQ6dOggL168sNrm9OnTUr58eYkXL55kzJhRxo0bJ45icENEREQf5O/vL4UKFZLp06eHuh5ByNSpU2XWrFly+PBhSZAggdSoUUMCAgLM2yCwOXfunGzbtk3Wr1+vAqbOnTub1/v5+Un16tUlc+bMcvz4cRk/frwMGzZMZs+eLY5gWYqIiEiHInsSv1q1aqklNMjaTJ48WQYNGiT169dX1y1cuFBSp06tMjw48faFCxdk8+bNcvToUfnoo4/UNtOmTZPatWvLDz/8oDJCixYtkjdv3sjcuXMlbty4ki9fPjl16pRMnDjRKgj6EGZuiIiIdNxz48yiZUssl9evX4ujrl+/Lvfu3VOlKE3ixImlZMmScvDgQXUZP1GK0gIbwPbu7u4q06NtU6FCBRXYaJD9uXjxojx58sTu42FwQ0REFINlzJhRBSLaMnbsWIf3gcAGkKmxhMvaOvxMlSqV1frYsWNLsmTJrLYJbR+W92EPlqWIiIhi8Dw3t2/fVg2+Gg8PD9E7Zm6IiIh0fXap8P2T/5elENhYLuEJbtKkSaN+3r9/3+p6XNbW4eeDBw+s1gcGBqoRVJbbhLYPy/uwB4MbIiIiHVIhijNDwcV189xkzZpVBR87duwwX4f+HfTSlC5dWl3Gz6dPn6pRUJqdO3dKUFCQ6s3RtsEIqrdv35q3wcgqb29vSZo0qd3Hw+CGiIiIPgjz0WDkEhatiRi/37p1SwVLvXv3llGjRsnatWvlzJkz0qZNGzUCqkGDBmr7PHnySM2aNaVTp05y5MgR2b9/v3Tv3l2NpMJ20KJFC9VMjPlvMGR86dKlMmXKFOnbt684gj03REREOhTZMxQfO3ZMKleubL6sBRxt27aV+fPny4ABA9RcOBiyjQxNuXLl1NBvTManwVBvBDRVqlRRo6QaNWqk5sbRoKF569at0q1bNylWrJikSJFCTQzoyDBwcDNhcDpFC0jh4YX1ffSPVXMXkZEkqJU7qg+BKOIEBons9pVnz55F2N9x7bNi7smfJH4iz3Dv5+XzV9K+yBcReqxRhWUpIiIiMhSWpYiIiHTov1FP4b+9UTG4ISIi0iFMU+PcPDdiWCxLERERkaEwc0NERKRDLEvZxuCGiIgoBp9+wYhYliIiIiJDYeaGiIhIhyJ7Ej89YXBDRESkQyxL2cbghoiISIfe523C313iZuDOFOM+MiIiIoqRmLkhIiLSIZalbGNwQ0REpEOc58Y2lqWIiIjIUJi5ISIi0iF3Nze1OHN7o2JwQ0REpEMsS9nGshQREREZCjM3REREOsTRUrYxuCEiItIl5ybxEwMXb4z7yIiIiChGYuaGiIhIh1iWso3BDRERkQ7xrOC2MbghIiLSIWZubGPPDRERERkKMzdEREQ6xEn8bGNwQ0REpEMsS9nGshQREREZCjM3REREui1LhT9H4SY8cSYRERFFIzwruG0sSxEREZGhsCxFRESkQxwtZRuDGyIiIh3iaCnbWJYiIiIiQ4lxwc3u3btVtPv06dMwt8uSJYtMnjw50o6LXGP80p+kfM/GkrphUcncrIw0HdFNLt25ZrVNzQGtJUGt3FZLz2lDrbYJvh7L8t0b+DJRpOvX9AvZN/UPebDyhNxcclCWDZkhOTNkNa9PmjCxTOw6WP6as1kerzktlxbulgldB4lX/IRW+ymWq4BsHLtAfFcck7vLj8ra0b9Igay5zeuxz83fL5Qbvx+QJ2vPyPl5O2Ro294SOxYT/NG9LOXMP6OKccFNmTJlxNfXVxInTqwuz58/X5IkSRJiu6NHj0rnzp2j4AjJGfvOHJXOdVvIrklLZd2YufI2MFDqfdtR/ANeWm3XruZncnXRXvMyqn3/EPua1XeM1TZ1y1Tli0ORrnyB4jJr3W9SsU8T+WRgO4kdO7asHz1X4nt4qvVpk6dSy8Cfv5diXepIpwlfS7Vi5WVWnzHmfSSIF1/WjJojtx/elQq9P5Mq/ZrLi5f+KsDRghf8X1m0fbXU/aa9FOpYQ/r/NFra1Wwig1v35KsezctSziyOwJf+0PbRrVs3tb5SpUoh1nXp0sVqH7du3ZI6depI/PjxJVWqVNK/f38JDAwUV4txIXncuHElTZo0H9wuZcqUkXI85Fr4A27pp75jJUvzMnLy8jkpV6C4+XpPD09Jkyzs1zhJAq8PbkMU0eoP6mh1ufMEH7m99LAUyZlP9p89JudvXpbmo3qY11/3vS3DFkySuf1/kFjuseRd0DvxzphNknsllZELp8idf++p7UYv+lGOzVovmVKlk2u+t+TGvdtq0dx6cFcqFFwnZfN/xBc5mnL//z9nbu8IfOl/9+6d+fLZs2elWrVq8tlnn5mv69Spk4wYMcJ8GUGMBrdFYIPP4AMHDqhEQ5s2bSROnDgyZsx/wbhhMzeI/rp3764WZFhSpEghgwcPFpPJpNY/efJEPSFJkyZVT1ytWrXk8uXL5tvfvHlT6tatq9YnSJBA8uXLJxs3bgxRlsLv7dq1k2fPnpmjzGHDhoUoS7Vo0UKaNm1qdYxv375Vx7Vw4UJ1OSgoSMaOHStZs2YVT09PKVSokKxYsSLSnjMKnd/L5+pn0kTvM3WaZbvWSaampeSjLnVlyLwJ8jLgVYjb9pkxQm1ToddnsmDLH+b3H1FU8oqfSP188vyZ7W0SJBK/ly9UYAOX7lyXf589kbY1P5M4seNIvLge8nmNxnLh5hW5ef+fUPeRLW0mlQHae+ZIBD0S0puUKVOqwERb1q9fL9mzZ5eKFSuat8FnsuU2Xl5e5nVbt26V8+fPy2+//SaFCxdWn90jR46U6dOny5s3b2JG5mbBggXSoUMHOXLkiBw7dkyViDJlyqSiws8//1wFM2vXrlVPnI+Pj9SuXVs9aYgAkSLDE7Vnzx4V3OD6hAmt689aiQoBzJAhQ+TixYvqutC2a9mypYpMX7x4YV6/ZcsWefnypXz66afqMgIbvGCzZs2SnDlzqvtu1aqVejNYvvCWXr9+rRaNn5+fy54/eh9wDvhpjJTOW1TyZcllfkqaVPpEMqZOJ2mTpZKz1y/J4Lk/yOU7N+T3wdPM2yAVX7FQKfH0iCc7TuyXPtOHi3+Av3xZvw2fWooy+AI2vsu3cuDccZWxCQ0yNAObfylzNy01X/filb/UGNBKlg2dodbBlbs3pd637c0BkGbXxCVSOEc+FQDN2bhERiycEsGPiqJ6tJRfsM8eDw8PtYQFn7H4zOvbt6/VMSxatEhdj8AGSQYkJrTszcGDB6VAgQKSOnVq8/Y1atSQrl27yrlz56RIkSJi+OAmY8aMMmnSJPWkeXt7y5kzZ9RlZHUQ1Ozfv18FJ9qTie1Xr16tghDU9Bo1aqSeRMiWLZvNEhUyQ7iPsEpVePIRJK1atUpat26trlu8eLHUq1dPEiVKpAIUpNS2b98upUuXNt/nvn375KeffrIZ3CAgGj58uNPPFYWuz/QRcv7GZdn+w2Kr69vX/i8Llz+rtyo91Rn4uVy7e0uypcukrv+6xfsPACicI6/K7ExeMZfBDUWpyd2GSr4sOaXKV81DXZ8ofgJZNWK2XLh1VUb99l+wjkAFPTgHz52Qtt/1lVju7tK7UQdZOWK2lOvZSALe/Pclq/WY3pIwfgIpmC23jOngI30adZCJK6zLvWSseW4yZsxodf3QoUPNVQxb8HmLCgiSDRpUOTJnzizp0qWT06dPq8QDEgcrV65U6+/du2cV2IB2GetcKdoGN6VKlbKKBhE0TJgwQWVh0FBXsmRJ87rkyZOrAOjChQvqcs+ePVUkiBRY1apVVaBTsGDBcB8L7q9JkyYqiEJw4+/vL2vWrJElS5ao9VeuXFFZHNQeg0e2YUWiAwcOVFGvBtFz8DcZhU/fGSNk05HdsnX8b5I+Zdg9VsVzv39vXPW9aQ5uQtvmu99nyOs3b8Qjbly+LBTpJn05RGqXrCxV+7WUf/69H2J9Qs8EsnbUL/L8lb80HfGlBL77r0mzaeW6kil1etWUrJVX237/lfiuOCp1S1eV5X/+NxJQ68n5+9ZVcXePJdN7jpTJK+eqTCgZ0+3bt63KRx/K2sAvv/yiykoIZDSWg3CQXEibNq1UqVJFrl69qspXkSnaBjfO6Nixo8q2bNiwQQU4yJAgMOrR47+mO0ehNIUMzIMHD2Tbtm2qr6ZmzZpqHcpVgPtLnz691e3CepPYk/ojx+AP91czR8raA9vVsNYsaTJ88Danr/6tfqZJlirMbTDkloENRVVgU69MNak+oJXcvH8n1IzNutFz5fXbN9J4WBf10xJGVgWZgqz6xhCs4DLOT2SLu5u7xIkdW/0MEgY30Y6TZSn5/20R2FgGNx+CvlZUKrSMjC1aEgIJAAQ3qJCg1cTS/fvvA3V7BvoYIrg5fPiw1eVDhw6pXpa8efOqYWNYr5WlHj16pFJfWKdBBgRD0LAgQ/Lzzz+HGtygNGXZ/W0L7gv7XLp0qWzatEmVv9DfA7hfBCkoh9kqQVHklaKW7V4vS4dMV99k7z1+qK5PnCCR6p9B6QnraxSvIMm8kqieG5+fxkq5/B9JgazeatuNh3bKg6ePpHjuQiqdv/PEATV/Tq9G7fgyUpSUopB5+Wx4V9U7kzppCnX9M//nqpyEwGb96HniGS+etBvXT81vo81x8/DZYxXEoG9sTMcBal8z1/4m7u5u0q/JFxL47p38efr939pmleuq4eBnb1xSwVGxnPllZLuvZMWejVZZIIo+our0C/PmzVPDuDHyKSynTp1SP5HB0Sowo0ePVkkC3B6QLEBgZfn5bejgBoECSjZffPGFnDhxQqZNm6ayLwhw6tevrxqL0c+Cnpevv/5aZUxwPfTu3Vuly3LlyqVGVu3atUvy5MkT6v1gVBQyLzt27FAjnND4ZDl0zRLqiWgYvnTpktqnBsfQr18/6dOnj/pDUq5cOTUCC31BeNHatm0bQc8SBffzht/Vz5o+bULMWdO6WkOJGyeO7Dp5QKavXiD+Aa8kQ8q0Ur9cdfFp1tW8bezYceSndYvFZ/ZYwRddlKq+6+yj5vwgimxf1G2pfm4bv8jq+k4TfOS3batU82+JPIXVdZh4z5J328py6/4/aiLLRkO/kG9b9ZDdk5aqLM5fVy5I/UEdzF8AEOj0bdJJcqZ/P5cJhoLPXPebTFs5L9IeK0V/QUFBKrjB5xpaNjQoPaEXFYN70CqCnht8JlaoUMHcFlK9enUVxKC9Y9y4carPZtCgQWoQkKurGNE2uMFQ71evXkmJEiUkVqxY0qtXL3M9D08sLn/yySeqrwVPHoZ6a5kUZGLwZN25c0cFFygfoRnZVkYG2R0M9UYGKKxGKpSmEHWiYaps2bJW6zCcDSOjUAK7du2amhiwaNGi8s0337j8uSHb/De9LzHZgmBmy/jfwtym+kfl1UIUHXjW/G+kX2j2nj7ywW1g58kDarEFGRospB9RkbnZvn27Sj60b98+RBUE6zACGX2pqHSg3xXBiwaf5Rg+jp5YZHEwUAdBkuW8OK7iZoqGk3dgRBTGwMe00x+goRijt3wf/eNQ/ZNIT3AqCyLDCgwS2e2rsvcR9Xdc+6zYdW2rJEyUINz7efHcXypnqx6hxxpVouUkfkRERESGK0sRERFR9Gso1oNoGdzgtAhEREQU8TMUG1G0DG6IiIgobMzc2MaeGyIiIjIUZm6IiIh0CEUl53pujIvBDRERkV7LUs703IhxwxuWpYiIiMhQmLkhIiLSITYU28bghoiISIcY3NjGshQREREZCjM3REREOsRJ/GxjcENERKRDLEvZxrIUERERGQozN0RERDrEspRtDG6IiIh0iGUp2xjcEBER6RCDG9vYc0NERESGwswNERGRDrHnxjYGN0RERDrEspRtLEsRERGRoTBzQ0REpEPM3NjG4IaIiEiP3NxU340ztzcqlqWIiIjIUJi5ISIi0iVkXpzJvriJUTG4ISIi0iEOBbeNZSkiIiIyFGZuiIiIdIijpWxjcENERKRDDG5sY3BDRESkQ+y5sY09N0RERGQozNwQERHpdiB4+Idzu4lxMbghIiLSIfbc2MayFBERERkKgxsiIiIdNxQ7szhi2LBhIW6fO3du8/qAgADp1q2bJE+eXBImTCiNGjWS+/fvW+3j1q1bUqdOHYkfP76kSpVK+vfvL4GBgeJqLEsRERHpUFSUpfLlyyfbt283X44d+78wok+fPrJhwwZZvny5JE6cWLp37y4NGzaU/fv3q/Xv3r1TgU2aNGnkwIED4uvrK23atJE4ceLImDFjxJUY3BAREZFdEMwgOAnu2bNn8ssvv8jixYvl448/VtfNmzdP8uTJI4cOHZJSpUrJ1q1b5fz58yo4Sp06tRQuXFhGjhwpPj4+KisUN25ccRWWpYiIiHQosstScPnyZUmXLp1ky5ZNWrZsqcpMcPz4cXn79q1UrVpVNChZZcqUSQ4ePKgu42eBAgVUYKOpUaOG+Pn5yblz58SVmLkhIiKKwWUpPz8/q+s9PDzUElzJkiVl/vz54u3trUpKw4cPl/Lly8vZs2fl3r17KvOSJEkSq9sgkME6wE/LwEZbr61zJQY3REREMVjGjBmtLg8dOlSViYKrVauW+feCBQuqYCdz5syybNky8fT0lOiEwQ0REZGOp/Fz7vYit2/fFi8vL/O1oWVtQoMsTa5cueTKlStSrVo1efPmjTx9+tQqe4PRUlqPDn4eOXLEah/aaKrQ+nicwZ4bIiIiHYc2ziyAwMZysTe4efHihVy9elXSpk0rxYoVU6OeduzYYV5/8eJF1ZNTunRpdRk/z5w5Iw8ePDBvs23bNnWfefPmFVdi5oaIiEiHIvvEmf369ZO6deuqUtTdu3dV+SpWrFjSvHlzNfS7Q4cO0rdvX0mWLJkKWHr06KECGoyUgurVq6sgpnXr1jJu3DjVZzNo0CA1N469AZW9GNwQERHRB925c0cFMo8ePZKUKVNKuXLl1DBv/A6TJk0Sd3d3NXnf69ev1UioGTNmmG+PQGj9+vXStWtXFfQkSJBA2rZtKyNGjBBXczOZTCaX75XCBR3riH59H/1jVf8kMpIEtf6b0ZTIcAKDRHb7qnlfIurvuPZZceneeUnklSjc+3nu91xypckboccaVZi5ISIiirHtxMbEhmIiIiIyFGZuiIiIdIm5G1sY3BAREelQZI+W0hOWpYiIiMhQGNwQERGRobAsRUREFINPnGlEzNwQERGRoTBzQ0REpEPM3NjGzA0REREZCjM3REREOsSh4LYxc0NERESGwuCGiIiIDIVlKSIiIl1ybii4cCg4ERERkT4wc0NERKRLPHGmLQxuiIiIdIihjW1sKCYiIiJDYeaGiIhIhzjPjW0MboiIiHSJhSlbWJYiIiIiQ2HmhoiISIeYt7GNwQ0REZFuOTOJn3ExuCEiItIhNhTbxp4bIiIiMhQGN0RERGQoLEsRERHp9rSZ4e+5cTNwvw4zN0RERGQozNwQERHpEgeD28LghoiISIcY2tjGshQREREZCjM3REREOsR5bmxjcENERKRLLEzZwrIUERERGQqDGyIiIh3nbZxZHDF27FgpXry4JEqUSFKlSiUNGjSQixcvWm1TqVIlc7lMW7p06WK1za1bt6ROnToSP358tZ/+/ftLYGCguBLLUkRERLoUuWWpP//8U7p166YCHAQj33zzjVSvXl3Onz8vCRIkMG/XqVMnGTFihPkyghjNu3fvVGCTJk0aOXDggPj6+kqbNm0kTpw4MmbMGHEVBjdEREQ6FNkNxZs3b7a6PH/+fJV5OX78uFSoUMEqmEHwEpqtW7eqYGj79u2SOnVqKVy4sIwcOVJ8fHxk2LBhEjduXHEFlqWIiIjIYc+ePVM/kyVLZnX9okWLJEWKFJI/f34ZOHCgvHz50rzu4MGDUqBAARXYaGrUqCF+fn5y7tw5cRVmboiIiGIwPz8/q8seHh5qCUtQUJD07t1bypYtq4IYTYsWLSRz5sySLl06OX36tMrIoC9n5cqVav29e/esAhvQLmOdqzC4ISIiisEnzsyYMaPV9UOHDlUlorCg9+bs2bOyb98+q+s7d+5s/h0ZmrRp00qVKlXk6tWrkj17doksDG6iEZPJpH4+93se1YdCFHECg/jskuHf39rf84jk5+Rnhd//b3/79m3x8vIyX/+hrE337t1l/fr1smfPHsmQIUOY25YsWVL9vHLligpu0Itz5MgRq23u37+vftrq0wkPBjfRyPPn799oubLmjupDISIiJ/+eJ06cOEKeQzTdIhDImSWX0/tKkyaN6o+JFy/eB7dFwNajRw9ZtWqV7N69W7JmzfrB25w6dUr9RAYHSpcuLaNHj5YHDx6oZmTYtm2bCq7y5s0rruJmiozwkuyCGubdu3fVHALOdMCTY7VmpGSDf3MhMgK+vyMfPlIR2KDnxN094sbsBAQEyJs3b1wSKMWzI7CBL7/8UhYvXixr1qwRb29v8/UI4jw9PVXpCetr164tyZMnVz03ffr0UdkdDCPXhoJjhBSen3Hjxqk+m9atW0vHjh1dOhScwQ3FaPjjj/+Y6PpncENGw/c3uZKtL93z5s2Tzz//XH1JbNWqlerF8ff3V18cP/30Uxk0aJDV39ebN29K165dVfYH8+O0bdtWvvvuO4kd23XFJAY3FKPxjz8ZGd/fFFNxnhsiIiIyFAY3FKNhVACGPX5odACRHvH9TTEVy1JERERkKMzcEBERkaEwuCEiIiJDYXBDREREhsLghsgOOM8KJp4i0gPMH4I5SZ4+fRrmdlmyZJHJkydH2nERRRY2FBMF/0/h5qamF2/QoIH5uhcvXsjr16/VrJtE0R1mrn38+LE62zLez/Pnz1dncA4e7Dx8+FBNohY/fvwoO1aiiMBzSxHZIWHChGoh0gPt3EMfkjJlykg5HqLIxrIURRuVKlWSnj17yoABAyRZsmTqjzPKQRp868T5R/AHGVN5f/zxx/LXX39Z7WPUqFHqZGw4Pxe2/frrr63KSUePHpVq1aqpE8XhtAsVK1aUEydOWKXpAVOG4xuvdtmyLLV161Z1Lpbg34J79eqljkmzb98+KV++vDrnCqYhx2PDlORE2vsdZ1fGgvci3pODBw82n036yZMn0qZNG0maNKnKrNSqVUsuX75sNYV93bp11XpkX/LlyycbN24MUZbC7+3atVOnGMF1WLT/V5ZlqRYtWkjTpk2tXpy3b9+q41q4cKH5/Hdjx45VJ0zE+7pQoUKyYsUKvqAU7TC4oWhlwYIF6g/14cOH1UnVRowYoc4YC5999pk6k+ymTZvk+PHjUrRoUalSpYpKv8OiRYvU2Wa///57tT5Tpkwyc+ZMq/3jhHY4jwkCj0OHDknOnDnVSd60M7Ij+NHOleLr62u+bAn3mSRJEvnjjz/M1+FkcEuXLpWWLVuqyziBXM2aNaVRo0bq5HFYh/vEBxmR5fsd59M5cuSITJkyRSZOnChz5sxR63CunmPHjsnatWvl4MGDKujBexUBB3Tr1k2VSvfs2SNnzpxR7/vQsotlypRRAQy+EOA9jaVfv34htsN7d926daoEq9myZYu8fPlSBfuAwAaBzqxZs+TcuXPqpIg4l5B2UkSiaANnBSeKDipWrGgqV66c1XXFixc3+fj4mPbu3Wvy8vIyBQQEWK3Pnj276aefflK/lyxZ0tStWzer9WXLljUVKlTI5n2+e/fOlChRItO6devM1+G/xapVq6y2Gzp0qNV+evXqZfr444/Nl7ds2WLy8PAwPXnyRF3u0KGDqXPnzlb7wGNwd3c3vXr1yq7ng4z/fs+TJ48pKCjIfB3e67ju0qVL6n24f/9+87p///3X5OnpaVq2bJm6XKBAAdOwYcNC3feuXbvU7bX347x580yJEycOsV3mzJlNkyZNUr+/ffvWlCJFCtPChQvN65s3b25q2rSp+h3/9+LHj286cOCA1T7wXsd2RNEJMzcUrRQsWNDqctq0aVW2BuUnfKNEQ6/W/4Ll+vXrKksCFy9elBIlSljdPvjl+/fvS6dOnVTGBqUAfJvFfm/duuXQceJbLtL9d+/eNWeN6tSpozI6gONFE6flsdaoUUOl9XHMRFCqVCmrMy2XLl1alZ7Onz+vMjolS5Y0r8N739vbWy5cuKAuo8yJMmzZsmXVKUSQIXQG7q9JkybqvQwooa5Zs8acjbxy5YrK4qCsa/m+RiZH+z9IFF2woZiilThx4lhdxh9+BAQIQBDoIKAITgso7IGS1KNHj1QJIHPmzOrcO/hAwegSRxQvXlyyZ88uS5Yska5du6rRVQhmNDjeL774Qn0ABYdyGZGz0FOGgHnDhg2qDwwlowkTJkiPHj3CvU8EMuhDwxcKlIPRV4PyKmjlKtxf+vTprW7Hc7NRdMPghnQB/TX37t1T3y61Jt/g8K0WPTJowtQE75nZv3+/zJgxQ/UuwO3bt+Xff/8NEWChh8aeDwJ8y82QIYO4u7urzI3l8eLbd44cORx+rBRzoLfMktYHljdvXgkMDFTr0TMDCMqRncQ6DRrVu3TpopaBAwfKzz//HGpwg9FT9ryncV/YJ3rE0NuGPjftCwfuF0EMspwIgIiiM5alSBeqVq2qMiyYewbfUm/cuCEHDhyQb7/9VjVdAv6o//LLL6pJE6l9pOyRqrdM++OD49dff1WpfXxwIEDBt1NLCJ527NihgimMWLEFt8VIKzQxN27c2Orbq4+Pjzo+NBCfOnVKHQ9S/GwoJksIFPr27auClt9//12mTZumRt3hfVq/fn1VQkUjOsqcaNxFxgTXA+atQcMvypx4H+7atUvy5MkT6hOM9zQyL3hfI5hHeckWjJpCwzAyN1pJCjACEY3IaCLG/zGUonC/OGZcJopOGNyQLiBAwTDXChUqqGGtuXLlkmbNmqnhsJioDPCHGN9e8QcYmRP80ceIEwzb1iD4QcCC9a1bt1ZlIwwdt4TUPv6w4xtskSJFbB4TsjLo6UEAZfkhoPUOYQTJpUuX1HBw7GfIkCGSLl06lz83pF/IMr569Uq9jzD6CYFN586dzSP2ihUrJp988okK7NHrjv8DWiYFmRjcBgENSkf4P4GspK2MDLI7GOqNqRQwEtEWvJeRdUQghX4eSyNHjlTD1VEC0+4XZSoMDSeKTjhDMRkamh8xXw6yNUTRbZ4bzJ3E0x8QuR57bsgwkGpHOh1NlrFixVJp/u3bt5vnySEiopiBwQ0ZrnSFHpiAgADVYIyJ9tCvQ0REMQfLUkRERGQobCgmIiIiQ2FwQ0RERIbC4IaIiIgMhcENERERGQqDGyKygokPMRO05XwsmA03suE8YhgB9/TpU5vbYP3q1avt3uewYcPU3DLOwOzYuF/MPE1E0RODGyKdBBz4QMWC8wRhduQRI0ao8w9FtJUrV6qZaV0VkBARRTTOc0OkE5jqHlPyv379Ws3ng6n3MRU/TjkRHM5yjiDIFZIlS+aS/RARRRZmboh0AifmxKkkMmfOLF27dlWTE65du9aqlIQJDHH+KkxgqJ31vEmTJpIkSRIVpOCkiyiraHB+Ipy4EeuTJ08uAwYMUOcwshS8LIXgCicGxbm3cEzIIuGcXdhv5cqV1TZJkyZVGRwcFwQFBanzEeEcRDhRaaFChWTFihVW94OADedHwnrsx/I47YXjwj7ix48v2bJlU+dBevv2bYjtfvrpJ3X82A7Pz7Nnz6zWz5kzR507Cecly507t81zNhFR9MTghkinEAQgQ6PBGZ9xdmmcbmL9+vXqQx2nosDZnPfu3Sv79++XhAkTqgyQdjucJHT+/Pkyd+5cdfbpx48fy6pVqz54skec2mLq1Knq7OoIFLBfBAuYERpwHL6+vjJlyhR1GYHNwoUL1ekxzp07p84sjbNc4+SiWhDWsGFDqVu3rupl6dixo3z99dcOPyd4rHg8OPEj7vvnn3+WSZMmWW1z5coVWbZsmaxbt042b94sJ0+elC+//NK8ftGiReokpwgU8fjGjBmjgiSe+ZpIR0xEFO21bdvWVL9+ffV7UFCQadu2bSYPDw9Tv379zOtTp05tev36tfk2v/76q8nb21ttr8F6T09P05YtW9TltGnTmsaNG2de//btW1OGDBnM9wUVK1Y09erVS/1+8eJFpHXU/Ydm165dav2TJ0/M1wUEBJjix49vOnDggNW2HTp0MDVv3lz9PnDgQFPevHmt1vv4+ITYV3BYv2rVKpvrx48fbypWrJj58tChQ02xYsUy3blzx3zdpk2bTO7u7iZfX191OXv27KbFixdb7WfkyJGm0qVLq9+vX7+u7vfkyZM275eIohZ7boh0AtkYZEiQkUGZp0WLFmr0j6ZAgQJWfTZ//fWXylIgm2EJ5926evWqKsUgu1KyZEnzutixY8tHH30UojSlQVYFJyWtWLGi3ceNY8BJTXGGdkvIHhUpUkT9jgyJ5XFA6dKlxVFLly5VGSU8vhcvXqiGay8vL6ttMmXKJOnTp7e6HzyfyDbhucJtO3ToIJ06dTJvg/0kTpzY4eMhoqjB4IZIJ9CHMnPmTBXAoK8GgYilBAkSWF3Gh3uxYsVUmSW4lClThrsU5igcB2zYsMEqqAD07LjKwYMHpWXLljJ8+HBVjkMwsmTJElV6c/RYUc4KHmwhqCMifWBwQ6QTCF7QvGuvokWLqkxGqlSpQmQvNGnTppXDhw9LhQoVzBmK48ePq9uGBtkhZDnQKxPa2da1zBEalTV58+ZVQcytW7dsZnzQvKs1R2sOHTokjjhw4IBqtv7222/N1928eTPEdjiOu3fvqgBRux93d3fVhJ06dWp1/bVr11SgRET6xIZiIoPCh3OKFCnUCCk0FF+/fl3NQ9OzZ0+5c+eO2qZXr17y3XffqYnw/v77b9VYG9YcNVmyZJG2bdtK+/bt1W20faJBFxBcYJQUSmgPHz5UmRCUevr166eaiNGUi7LPiRMnZNq0aeYm3S5dusjly5elf//+qjy0ePFi1RjsiJw5c6rABdka3AfKU6E1R2MEFB4DynZ4XvB8YMQURqIBMj9ogMbtL126JGfOnFFD8CdOnOjQ8RBR1GFwQ2RQGOa8Z88e1WOCkUjIjqCXBD03Wibnq6++ktatW6sPe/SeIBD59NNPw9wvSmONGzdWgRCGSaM3xd/fX61D2QnBAUY6IQvSvXt3dT0mAcSIIwQNOA6M2EKZCkPDAceIkVYImDBMHKOqMErJEfXq1VMBFO4TsxAjk4P7DA7ZLzwftWvXlurVq0vBggWthnpjpBaGgiOgQaYK2SYEWtqxElH054au4qg+CCIiIiJXYeaGiIiIDIXBDRERERkKgxsiIiIyFAY3REREZCgMboiIiMhQGNwQERGRoTC4ISIiIkNhcENERESGwuCGiIiIDIXBDRERERkKgxsiIiIyFAY3REREJEbyP/WhMPAep9DEAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_test_distil = trainer_distil.predict(tokenized_distil[\"test\"])\n",
    "logits_test_distil = pred_test_distil.predictions\n",
    "labels_test_distil = pred_test_distil.label_ids\n",
    "pred_labels_distil = logits_test_distil.argmax(axis=-1)\n",
    "\n",
    "acc_test_distil = metric_acc.compute(predictions=pred_labels_distil, references=labels_test_distil)\n",
    "f1_test_distil = metric_f1.compute(predictions=pred_labels_distil, references=labels_test_distil, average=\"macro\")\n",
    "\n",
    "print(\"DistilBERT Test Accuracy:\", acc_test_distil)\n",
    "print(\"DistilBERT Test Macro-F1:\", f1_test_distil)\n",
    "\n",
    "cm_distil = confusion_matrix(labels_test_distil, pred_labels_distil)\n",
    "disp_distil = ConfusionMatrixDisplay(confusion_matrix=cm_distil, display_labels=[\"negative\", \"positive\"])\n",
    "disp_distil.plot(cmap=\"Greens\")\n",
    "plt.title(\"DistilBERT Confusion Matrix (Test)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b660de5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I747530/Desktop/NLP/Project_1/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Example 1 (index=0)\n",
      "Gold label      : negative\n",
      "Predicted label : negative  (p = 0.980)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "the tortuous emotional impact is degrading, whether adult or adolescent the personal values shown in this movie belong in a bad psychodrama if anywhere at all. this movie has a plot, but it is all evil from start to end. this is no way for people to act and degrades both sexes all the way through th...\n",
      "\n",
      "====================================================================================================\n",
      "Example 2 (index=1)\n",
      "Gold label      : negative\n",
      "Predicted label : negative  (p = 0.969)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "anyone who knows anything about evolution wouldn't even need to see the film to say \"fake\". \"it's never been disproved\" also is a weak argument. saying the universe was created by a giant hippo cannot be disproved. although, to be fair, it does seem like the only people who do believe are the same p...\n",
      "\n",
      "====================================================================================================\n",
      "Example 3 (index=2)\n",
      "Gold label      : negative\n",
      "Predicted label : negative  (p = 0.967)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "i'm glad i rented this movie for one reason: its shortcomings made me want to read allende's book and get the full story. pros: the movie is beautiful, the period is depicted well and consistently (to the best of my knowledge), and meryl and glenn do good jobs. cons: this is the worst acting job i'v...\n",
      "\n",
      "====================================================================================================\n",
      "Example 4 (index=3)\n",
      "Gold label      : negative\n",
      "Predicted label : negative  (p = 0.716)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "yes, the votes are in. this film may very well be the plan 9 from outer space for our generation. but whereas ed wood's film, for all its flaws, retains a certain charm despite it all, this film defines the word \"charmless\" to the nth degree. in fact, i'd suggest to the editors of the oxford english...\n",
      "\n",
      "====================================================================================================\n",
      "Example 5 (index=4)\n",
      "Gold label      : negative\n",
      "Predicted label : positive  (p = 0.813)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "this mini-series is actually more entertaining than some others with much bigger budgets and grander aspirations. sotd falls somewhere between \"kung-fu\" and \"h r pufnstuff\" on the entertainment spectrum. if it weren't so long (nearly 3 hours) i think that kids would like it quite a bit. it's got adv...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Label id ↔ name mapping\n",
    "id2label = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "def show_minilm_predictions(\n",
    "    num_examples: int = 5,\n",
    "    split: str = \"test\",\n",
    "    max_review_chars: int = 300,\n",
    "):\n",
    "    \"\"\"\n",
    "    Print a few test reviews with:\n",
    "    - gold label\n",
    "    - predicted label\n",
    "    - prediction probability\n",
    "    \"\"\"\n",
    "    # HuggingFace test datasets\n",
    "    ds_text = hf_dataset[split]          # original text + labels\n",
    "    ds_tok  = tokenized_minilm[split]    # tokenized for MiniLM\n",
    "\n",
    "    # Take first `num_examples` (or sample randomly if you prefer)\n",
    "    indices = list(range(min(num_examples, len(ds_tok))))\n",
    "    small_tok_ds = ds_tok.select(indices)\n",
    "\n",
    "    # Use Trainer to predict on this small subset\n",
    "    preds = trainer_minilm.predict(small_tok_ds)\n",
    "    logits = preds.predictions           # shape (N, 2) for binary classification\n",
    "    probs = F.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        ex = ds_text[idx]\n",
    "        text = ex[\"review_clean_transformer\"]\n",
    "        gold = id2label[ex[\"label\"]]\n",
    "        pred_id = int(probs[i].argmax())\n",
    "        pred_label = id2label[pred_id]\n",
    "        pred_prob = float(probs[i][pred_id])\n",
    "\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"Example {i+1} (index={idx})\")\n",
    "        print(f\"Gold label      : {gold}\")\n",
    "        print(f\"Predicted label : {pred_label}  (p = {pred_prob:.3f})\")\n",
    "        print(\"-\" * 100)\n",
    "        # Show truncated text for readability\n",
    "        if len(text) > max_review_chars:\n",
    "            print(text[:max_review_chars] + \"...\")\n",
    "        else:\n",
    "            print(text)\n",
    "        print()  # blank line\n",
    "\n",
    "show_minilm_predictions(num_examples=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca19b1d",
   "metadata": {},
   "source": [
    "## Task 4: Model Evaluation & Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cfa5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "N_SAMPLE = 100\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "indices_100 = random.sample(range(len(hf_dataset[\"test\"])), k=N_SAMPLE)\n",
    "\n",
    "sample_text_ds = hf_dataset[\"test\"].select(indices_100)          # original text + gold labels\n",
    "sample_tok_ds   = tokenized_minilm[\"test\"].select(indices_100)   # tokenized for MiniLM\n",
    "\n",
    "len(sample_text_ds), len(sample_tok_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bbc130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I747530/Desktop/NLP/Project_1/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniLM on 100-sample subset:\n",
      "  Accuracy:   0.860\n",
      "  Macro-F1:   0.858\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Run MiniLM on the 100-tokenized samples\n",
    "preds_100 = trainer_minilm.predict(sample_tok_ds)\n",
    "logits_100 = preds_100.predictions\n",
    "labels_100 = preds_100.label_ids\n",
    "probs_100 = F.softmax(torch.tensor(logits_100), dim=-1).numpy()\n",
    "pred_labels_100 = probs_100.argmax(axis=-1)\n",
    "\n",
    "acc_100 = accuracy_score(labels_100, pred_labels_100)\n",
    "f1_100 = f1_score(labels_100, pred_labels_100, average=\"macro\")\n",
    "\n",
    "print(f\"MiniLM on 100-sample subset:\")\n",
    "print(f\"  Accuracy:   {acc_100:.3f}\")\n",
    "print(f\"  Macro-F1:   {f1_100:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fa7050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram LM perplexity on 100-sample subset:  4128.74\n",
      "Trigram LM perplexity on 100-sample subset: 29667.64\n"
     ]
    }
   ],
   "source": [
    "# Use your classical pipeline tokenizer (or a light version) to get tokens\n",
    "def simple_classical_tokens(text: str):\n",
    "    # If you have classical_preprocess, you can call it directly.\n",
    "    # Here we assume df['tokens_classical'] is only for the original splits,\n",
    "    # so we'll just reuse your cleaning logic:\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    text_clean = classical_clean_for_tokenization(text)\n",
    "    return word_tokenize(text_clean)\n",
    "\n",
    "# Convert the 100 HF test texts into token lists\n",
    "sample_tokens_100 = [\n",
    "    simple_classical_tokens(x[\"review_clean_transformer\"])\n",
    "    for x in sample_text_ds\n",
    "]\n",
    "\n",
    "# Map tokens to vocab (same as before)\n",
    "sample_tokens_mapped_100 = map_tokens_to_vocab(sample_tokens_100, vocab)\n",
    "\n",
    "# Build padded sentences for bigram/trigram\n",
    "sample_sents_bigram_100  = build_ngram_sentences(sample_tokens_mapped_100, n=2)\n",
    "sample_sents_trigram_100 = build_ngram_sentences(sample_tokens_mapped_100, n=3)\n",
    "\n",
    "# Compute perplexity\n",
    "pp_bigram_100  = bigram_lm.perplexity(sample_sents_bigram_100)\n",
    "pp_trigram_100 = trigram_lm.perplexity(sample_sents_trigram_100)\n",
    "\n",
    "print(f\"Bigram LM perplexity on 100-sample subset:  {pp_bigram_100:.2f}\")\n",
    "print(f\"Trigram LM perplexity on 100-sample subset: {pp_trigram_100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c1e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>text</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>model_label</th>\n",
       "      <th>model_confidence</th>\n",
       "      <th>human_label_annotator1</th>\n",
       "      <th>human_label_annotator2</th>\n",
       "      <th>human_confidence_annotator1</th>\n",
       "      <th>human_confidence_annotator2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>912</td>\n",
       "      <td>....this mini does not get better with age. I ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.913921</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>204</td>\n",
       "      <td>This documentary is the most hypnotizing film ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.990822</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2253</td>\n",
       "      <td>All Dogs Go to Heaven is, in my opinion, the b...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.990861</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006</td>\n",
       "      <td>Rosalind Russell executes a power-house perfor...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.990012</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1828</td>\n",
       "      <td>This film screened last night at Austin's Para...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.990885</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_index                                               text gold_label  \\\n",
       "0           912  ....this mini does not get better with age. I ...   negative   \n",
       "1           204  This documentary is the most hypnotizing film ...   positive   \n",
       "2          2253  All Dogs Go to Heaven is, in my opinion, the b...   positive   \n",
       "3          2006  Rosalind Russell executes a power-house perfor...   positive   \n",
       "4          1828  This film screened last night at Austin's Para...   positive   \n",
       "\n",
       "  model_label  model_confidence human_label_annotator1 human_label_annotator2  \\\n",
       "0    negative          0.913921                                                 \n",
       "1    positive          0.990822                                                 \n",
       "2    positive          0.990861                                                 \n",
       "3    positive          0.990012                                                 \n",
       "4    positive          0.990885                                                 \n",
       "\n",
       "  human_confidence_annotator1 human_confidence_annotator2  \n",
       "0                                                          \n",
       "1                                                          \n",
       "2                                                          \n",
       "3                                                          \n",
       "4                                                          "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build a DataFrame with text, gold label, model prediction & prob\n",
    "id2label = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "rows = []\n",
    "for i, ex in enumerate(sample_text_ds):\n",
    "    text = ex[\"review_clean_transformer\"]\n",
    "    gold = id2label[ex[\"label\"]]\n",
    "    pred_id = int(pred_labels_100[i])\n",
    "    pred_label = id2label[pred_id]\n",
    "    pred_prob = float(probs_100[i, pred_id])\n",
    "\n",
    "    rows.append({\n",
    "        \"sample_index\": indices_100[i],\n",
    "        \"text\": text,\n",
    "        \"gold_label\": gold,\n",
    "        \"model_label\": pred_label,\n",
    "        \"model_confidence\": pred_prob,\n",
    "        # columns for human annotators to fill in:\n",
    "        \"human_label_annotator1\": \"\",\n",
    "        \"human_label_annotator2\": \"\",\n",
    "        \"human_confidence_annotator1\": \"\",\n",
    "        \"human_confidence_annotator2\": \"\",\n",
    "    })\n",
    "\n",
    "df_100 = pd.DataFrame(rows)\n",
    "df_100.to_csv(\"human_eval_100.csv\", index=False)\n",
    "\n",
    "df_100.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a2cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotated = pd.read_csv(\"human_eval_100.csv\")\n",
    "\n",
    "# Convert human labels to ids (e.g. using annotator 1)\n",
    "label2id = {\"negative\": 0, \"positive\": 1}\n",
    "\n",
    "human_labels_1 = df_annotated[\"human_label_annotator1\"].map(label2id)\n",
    "gold_labels = df_annotated[\"gold_label\"].map(label2id)\n",
    "model_labels = df_annotated[\"model_label\"].map(label2id)\n",
    "\n",
    "# Human vs gold\n",
    "acc_human_vs_gold = accuracy_score(gold_labels, human_labels_1)\n",
    "f1_human_vs_gold = f1_score(gold_labels, human_labels_1, average=\"macro\")\n",
    "\n",
    "# Model vs gold (should match acc_100/f1_100)\n",
    "acc_model_vs_gold = accuracy_score(gold_labels, model_labels)\n",
    "f1_model_vs_gold = f1_score(gold_labels, model_labels, average=\"macro\")\n",
    "\n",
    "# Agreement between human and model\n",
    "acc_model_vs_human = accuracy_score(human_labels_1, model_labels)\n",
    "f1_model_vs_human = f1_score(human_labels_1, model_labels, average=\"macro\")\n",
    "\n",
    "print(\"Human vs gold:   acc={:.3f}, macro-F1={:.3f}\".format(acc_human_vs_gold, f1_human_vs_gold))\n",
    "print(\"Model vs gold:   acc={:.3f}, macro-F1={:.3f}\".format(acc_model_vs_gold, f1_model_vs_gold))\n",
    "print(\"Model vs human:  acc={:.3f}, macro-F1={:.3f}\".format(acc_model_vs_human, f1_model_vs_human))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b90d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "disagree_idx = df_annotated[gold_labels != human_labels_1].index\n",
    "df_annotated.loc[disagree_idx, [\"text\", \"gold_label\", \"human_label_annotator1\", \"model_label\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25955950",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_rows = []\n",
    "for i, row in df_100.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    model_label = row[\"model_label\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a strict sentiment annotator.\n",
    "\n",
    "Read the following movie review and then answer with a single word:\n",
    "either \"positive\" or \"negative\".\n",
    "\n",
    "Review:\n",
    "{text}\n",
    "\n",
    "The current model predicted: {model_label}\n",
    "\n",
    "What is the correct overall sentiment of this review? Answer with only one word: positive or negative.\n",
    "    \"\"\".strip()\n",
    "\n",
    "    judge_rows.append({\n",
    "        \"sample_index\": row[\"sample_index\"],\n",
    "        \"text\": text,\n",
    "        \"gold_label\": row[\"gold_label\"],\n",
    "        \"model_label\": model_label,\n",
    "        \"judge_prompt\": prompt,\n",
    "        \"judge_label\": \"\"  # to fill with LLM output\n",
    "    })\n",
    "\n",
    "df_judge = pd.DataFrame(judge_rows)\n",
    "df_judge.to_csv(\"llm_judge_100_template.csv\", index=False)\n",
    "df_judge.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b494847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import pandas as pd\n",
    "\n",
    "JUDGE_MODEL_NAME = \"your/local/llama-or-mistral\"  # e.g. \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "df_judge = pd.read_csv(\"llm_judge_100_template.csv\")\n",
    "\n",
    "tokenizer_judge = AutoTokenizer.from_pretrained(JUDGE_MODEL_NAME)\n",
    "model_judge = AutoModelForCausalLM.from_pretrained(JUDGE_MODEL_NAME)\n",
    "pipe_judge = pipeline(\"text-generation\", model=model_judge, tokenizer=tokenizer_judge, device=0)\n",
    "\n",
    "def extract_label_from_response(text):\n",
    "    text_low = text.lower()\n",
    "    if \"positive\" in text_low and \"negative\" not in text_low:\n",
    "        return \"positive\"\n",
    "    if \"negative\" in text_low and \"positive\" not in text_low:\n",
    "        return \"negative\"\n",
    "    # fallback (you can improve this)\n",
    "    return \"positive\" if \"positive\" in text_low else \"negative\"\n",
    "\n",
    "judge_labels = []\n",
    "for prompt in df_judge[\"judge_prompt\"]:\n",
    "    out = pipe_judge(prompt, max_new_tokens=10, do_sample=False)[0][\"generated_text\"]\n",
    "    label = extract_label_from_response(out)\n",
    "    judge_labels.append(label)\n",
    "\n",
    "df_judge[\"judge_label\"] = judge_labels\n",
    "df_judge.to_csv(\"llm_judge_100_annotated.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_judge_annotated = pd.read_csv(\"llm_judge_100_annotated.csv\")\n",
    "\n",
    "judge_labels = df_judge_annotated[\"judge_label\"].map(label2id)\n",
    "gold_labels  = df_judge_annotated[\"gold_label\"].map(label2id)\n",
    "model_labels = df_judge_annotated[\"model_label\"].map(label2id)\n",
    "\n",
    "# Judge vs gold\n",
    "acc_judge_vs_gold = accuracy_score(gold_labels, judge_labels)\n",
    "f1_judge_vs_gold = f1_score(gold_labels, judge_labels, average=\"macro\")\n",
    "\n",
    "# Judge vs model\n",
    "acc_judge_vs_model = accuracy_score(model_labels, judge_labels)\n",
    "f1_judge_vs_model = f1_score(model_labels, judge_labels, average=\"macro\")\n",
    "\n",
    "print(\"LLM-judge vs gold:   acc={:.3f}, macro-F1={:.3f}\".format(acc_judge_vs_gold, f1_judge_vs_gold))\n",
    "print(\"LLM-judge vs model:  acc={:.3f}, macro-F1={:.3f}\".format(acc_judge_vs_model, f1_judge_vs_model))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
