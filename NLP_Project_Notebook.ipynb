{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Preprocessing and Modeling Pipeline\n",
    "This notebook consolidates the preprocessing pipeline and three language models: Bigram, Trigram, and Neural LSTM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading necessary NLTK resources...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vogle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vogle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\vogle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\vogle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import html\n",
    "import string\n",
    "from typing import List, Optional, Union\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "import numpy as np\n",
    "\n",
    "# Ensure necessary NLTK data is available\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    print(\"Downloading necessary NLTK resources...\")\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    nltk.download('punkt_tab')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessor\n",
    "The `TextPreprocessor` class handles cleaning and tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    A robust and professional preprocessing pipeline for NLP tasks.\n",
    "    Designed to handle IMDB movie reviews for Classical, Neural, and Transformer models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 remove_html: bool = True,\n",
    "                 lowercase: bool = True,\n",
    "                 remove_punctuation: bool = False,\n",
    "                 remove_stopwords: bool = False,\n",
    "                 lemmatize: bool = False,\n",
    "                 expand_contractions: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline with specific configuration flags.\n",
    "        \n",
    "        Args:\n",
    "            remove_html (bool): Strip HTML tags (e.g., <br />). Default True.\n",
    "            lowercase (bool): Convert text to lowercase. Default True.\n",
    "            remove_punctuation (bool): Remove punctuation characters.\n",
    "            remove_stopwords (bool): Remove standard English stopwords.\n",
    "            lemmatize (bool): Apply WordNet lemmatization.\n",
    "            expand_contractions (bool): Expand \"isn't\" to \"is not\".\n",
    "        \"\"\"\n",
    "        self.remove_html = remove_html\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        self.expand_contractions = expand_contractions\n",
    "\n",
    "        # Pre-load resources to optimize runtime\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stop_words.remove(\"not\")\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Simple contraction map for expansion\n",
    "        self.contractions_dict = {\n",
    "            \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
    "            \"haven't\": \"have not\", \"hasn't\": \"has not\", \"hadn't\": \"had not\", \"won't\": \"will not\",\n",
    "            \"wouldn't\": \"would not\", \"don't\": \"do not\", \"doesn't\": \"does not\", \"didn't\": \"did not\",\n",
    "            \"can't\": \"cannot\", \"couldn't\": \"could not\", \"shouldn't\": \"should not\", \"mightn't\": \"might not\",\n",
    "            \"mustn't\": \"must not\", \"i'm\": \"i am\", \"you're\": \"you are\", \"he's\": \"he is\", \"she's\": \"she is\",\n",
    "            \"it's\": \"it is\", \"we're\": \"we are\", \"they're\": \"they are\", \"i've\": \"i have\", \"you've\": \"you have\",\n",
    "            \"we've\": \"we have\", \"they've\": \"they have\", \"i'll\": \"i will\", \"you'll\": \"you will\",\n",
    "            \"he'll\": \"he will\", \"she'll\": \"she will\", \"we'll\": \"we will\", \"they'll\": \"they will\"\n",
    "        }\n",
    "        self.contractions_re = re.compile('(%s)' % '|'.join(self.contractions_dict.keys()))\n",
    "\n",
    "    def _clean_html(self, text: str) -> str:\n",
    "        \"\"\"Removes HTML tags and unescapes HTML entities.\"\"\"\n",
    "        text = html.unescape(text)\n",
    "        # Regex for HTML tags\n",
    "        clean = re.compile('<.*?>')\n",
    "        return re.sub(clean, ' ', text)\n",
    "\n",
    "    def _expand_contractions(self, text: str) -> str:\n",
    "        \"\"\"Expands common English contractions.\"\"\"\n",
    "        def replace(match):\n",
    "            return self.contractions_dict[match.group(0)]\n",
    "        return self.contractions_re.sub(replace, text)\n",
    "\n",
    "    def _remove_punct(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes punctuation by replacing it with spaces.\n",
    "        This prevents 'word,word' from becoming 'wordword'.\n",
    "        \"\"\"\n",
    "        # Replace punctuation with a space\n",
    "        return re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)\n",
    "\n",
    "    def process_text(self, text: str) -> Union[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Main execution method. Applies enabled steps in the logical order.\n",
    "        \n",
    "        Returns:\n",
    "            str: If the final output is a joined string.\n",
    "            List[str]: If the processing flow ends in tokenization without re-joining.\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or not text:\n",
    "            return \"\"\n",
    "\n",
    "        # 1. Cleaning\n",
    "        if self.remove_html:\n",
    "            text = self._clean_html(text)\n",
    "        \n",
    "        # 2. Lowercasing\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "            \n",
    "        # 3. Expansion (must be after lowercasing for simple dict matching)\n",
    "        if self.expand_contractions:\n",
    "            text = self._expand_contractions(text)\n",
    "\n",
    "        # 4. Punctuation Removal\n",
    "        if self.remove_punctuation:\n",
    "            text = self._remove_punct(text)\n",
    "\n",
    "        # 5. Tokenization\n",
    "        # We always tokenize to perform word-level operations (stopword/lemma)\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # 6. Stopword Removal\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [w for w in tokens if w not in self.stop_words]\n",
    "\n",
    "        # 7. Lemmatization\n",
    "        if self.lemmatize:\n",
    "            tokens = [self.lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "        # Return list of tokens or join back to string depending on downstream need.\n",
    "        # For this pipeline, we generally return the list of tokens for Classical models,\n",
    "        # but for compatibility, we will join them back into a clean string \n",
    "        # because Tokenizers for Transformers/LSTMs often expect string input \n",
    "        # and do their own internal splitting.\n",
    "        \n",
    "        return \" \".join(tokens)\n",
    "\n",
    "# --- Usage Example / Demonstration ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "Loading the IMDB dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Loaded 50000 reviews.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_data(path='IMDB Dataset.csv'):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(\"Dataset loaded successfully.\")\n",
    "        return df['review'].tolist()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{path} not found. Trying small dataset...\")\n",
    "        try:\n",
    "             df = pd.read_csv('IMDB Dataset_small.csv')\n",
    "             print(\"IMDB Dataset_small.csv loaded.\")\n",
    "             return df['review'].tolist()\n",
    "        except FileNotFoundError:\n",
    "             print(\"No dataset found. Using dummy data.\")\n",
    "             return [\"The movie was terrible.\", \"I loved the movie.\"]\n",
    "\n",
    "reviews = load_data()\n",
    "print(f\"Loaded {len(reviews)} reviews.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bigram Language Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the Bigram Model.\n",
    "        \n",
    "        Args:\n",
    "            alpha (float): The smoothing parameter for Laplace smoothing. \n",
    "                           Default is 0.01.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        self.total_bigrams = 0\n",
    "        self.total_unigrams = 0\n",
    "        \n",
    "        self.lambda1 = 0.3 # Unigram\n",
    "        self.lambda2 = 0.7 # Bigram\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the model on a corpus of tokenized sentences.\n",
    "        Uses the full vocabulary (no <UNK> thresholding).\n",
    "        \"\"\"\n",
    "        print(\"Training model on full vocabulary...\")\n",
    "        for sentence in corpus:\n",
    "            # Update vocabulary and unigram counts\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "                self.unigram_counts[word] += 1\n",
    "                self.total_unigrams += 1\n",
    "            \n",
    "            # Update bigram counts\n",
    "            for i in range(len(sentence) - 1):\n",
    "                w_curr = sentence[i]\n",
    "                w_next = sentence[i+1]\n",
    "                self.bigram_counts[w_curr][w_next] += 1\n",
    "                self.total_bigrams += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f\"Training complete. Vocab size: {self.vocab_size}\")\n",
    "\n",
    "    def get_probability(self, prev_word, word):\n",
    "        \"\"\"\n",
    "        Calculates the interpolated probability P(word | prev_word).\n",
    "        P = L2 * P(word|prev) + L1 * P(word)\n",
    "        \"\"\"\n",
    "        # 1. Bigram Probability\n",
    "        bigram_count = self.bigram_counts[prev_word][word]\n",
    "        unigram_count_prev = self.unigram_counts[prev_word]\n",
    "        \n",
    "        p_bi_num = bigram_count + self.alpha\n",
    "        p_bi_den = unigram_count_prev + (self.alpha * self.vocab_size)\n",
    "        p_bi = p_bi_num / p_bi_den\n",
    "        \n",
    "        # 2. Unigram Probability\n",
    "        unigram_count_word = self.unigram_counts[word]\n",
    "        p_uni_num = unigram_count_word + self.alpha\n",
    "        p_uni_den = self.total_unigrams + (self.alpha * self.vocab_size)\n",
    "        p_uni = p_uni_num / p_uni_den\n",
    "        \n",
    "        return (self.lambda2 * p_bi) + (self.lambda1 * p_uni)\n",
    "\n",
    "    def calculate_perplexity(self, test_corpus):\n",
    "        \"\"\"\n",
    "        Calculates the perplexity of the model on a test corpus.\n",
    "        \"\"\"\n",
    "        log_prob_sum = 0\n",
    "        N = 0\n",
    "        \n",
    "        for sentence in test_corpus:\n",
    "            for i in range(len(sentence) - 1):\n",
    "                w_curr = sentence[i]\n",
    "                w_next = sentence[i+1]\n",
    "                \n",
    "                # We do not replace with <UNK>. If a word is unknown,\n",
    "                # get_probability handles it via smoothing.\n",
    "                prob = self.get_probability(w_curr, w_next)\n",
    "                \n",
    "                log_prob_sum += math.log2(prob)\n",
    "                N += 1\n",
    "        \n",
    "        if N == 0: return float('inf')\n",
    "        \n",
    "        avg_log_prob = -log_prob_sum / N\n",
    "        perplexity = 2 ** avg_log_prob\n",
    "        return perplexity\n",
    "\n",
    "    def generate_sentence(self, max_length=20):\n",
    "        \"\"\"\n",
    "        Generates a random sentence.\n",
    "        \"\"\"\n",
    "        current_word = \"<s>\"\n",
    "        sentence = [current_word]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "                \n",
    "            # If current_word was never seen in training (e.g. from a user prompt),\n",
    "            # unigram_count is 0. We fallback to uniform distribution or break.\n",
    "            # Here we sample from the whole vocab if unknown, or just observed followers if known.\n",
    "            \n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            \n",
    "            if not possible_next:\n",
    "                # Dead end or unknown word. \n",
    "                # Ideally: Sample uniformly from V (or weighted by unigrams).\n",
    "                # For efficiency/simplicity here: break or pick random.\n",
    "                break \n",
    "\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            sentence.append(next_word)\n",
    "            current_word = next_word\n",
    "            \n",
    "        return \" \".join(sentence)\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        \"\"\"\n",
    "        Completes a given text prompt.\n",
    "        \"\"\"\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        \n",
    "        if not tokens:\n",
    "            current_word = \"<s>\"\n",
    "        else:\n",
    "            current_word = tokens[-1]\n",
    "            \n",
    "        # Warning: If current_word is not in self.vocab, generation will stop immediately\n",
    "        # because bigram_counts[current_word] will be empty.\n",
    "        \n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "            \n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            \n",
    "            if not possible_next:\n",
    "                break\n",
    "                \n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            generated_tokens.append(next_word)\n",
    "            current_word = next_word\n",
    "            \n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        \"\"\"\n",
    "        Completes a given text prompt using the trained model.\n",
    "        \"\"\"\n",
    "        # Preprocess the prompt to get the last token\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        \n",
    "        if not tokens:\n",
    "            current_word = \"<s>\"\n",
    "        else:\n",
    "            current_word = tokens[-1]\n",
    "            \n",
    "        # Handle OOV for the seed word\n",
    "        if current_word not in self.vocab:\n",
    "            # Optionally print a warning or fallback\n",
    "            current_word = \"<UNK>\"\n",
    "            \n",
    "        # Generate continuation\n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "            \n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            \n",
    "            if not possible_next:\n",
    "                # If we hit a dead end (should be rare with smoothing context, but possible if UNK), replace\n",
    "                current_word = \"<UNK>\"\n",
    "                possible_next = self.bigram_counts[current_word]\n",
    "\n",
    "            if not possible_next:\n",
    "                break\n",
    "                \n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            generated_tokens.append(next_word)\n",
    "            current_word = next_word\n",
    "            \n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n",
    "\n",
    "# --- Example Usage with IMDB Data ---\n",
    "\n",
    "def dummy_preprocessor(text):\n",
    "    \"\"\"\n",
    "    Placeholder for your existing pipeline.\n",
    "    Ensures <s> and </s> are added and text is tokenized.\n",
    "    \"\"\"\n",
    "    # Simple tokenization for demonstration\n",
    "    tokens = text.lower().strip().split()\n",
    "    return ['<s>'] + tokens + ['</s>']\n",
    "\n",
    "def main():\n",
    "    # 1. Load Data\n",
    "    try:\n",
    "        # Assuming the CSV is in the same directory\n",
    "        # Using the column names from your screenshot: 'review', 'sentiment'\n",
    "        df = pd.read_csv('IMDB Dataset.csv')\n",
    "        print(\"Dataset loaded successfully.\")\n",
    "        \n",
    "        # Taking a subset for demonstration speed\n",
    "        reviews = df['review'].tolist() \n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"IMDB Dataset.csv not found. Using dummy data.\")\n",
    "        reviews = [\"The movie was terrible.\", \"I loved the movie.\"]\n",
    "\n",
    "    # 2. Apply Preprocessing (Less aggressive for Language Modeling)\n",
    "    preprocessor = TextPreprocessor(\n",
    "        remove_html=True,\n",
    "        lowercase=True,\n",
    "        remove_punctuation=False, # Keep punctuation for structure\n",
    "        remove_stopwords=False,   # Keep stopwords for grammar\n",
    "        lemmatize=True,          # Keep original word forms\n",
    "        expand_contractions=True)\n",
    "        \n",
    "    tokenized_corpus = []\n",
    "    print(\"Preprocessing texts...\")\n",
    "    for r in reviews:\n",
    "        # Preprocessor returns a single string of space-separated tokens\n",
    "        cleaned_text = preprocessor.process_text(r)\n",
    "        # Split into list of tokens\n",
    "        tokens = cleaned_text.split()\n",
    "        # Add sentence boundaries\n",
    "        tokens = ['<s>'] + tokens + ['</s>']\n",
    "        tokenized_corpus.append(tokens)\n",
    "\n",
    "    # 3. Split Train/Test\n",
    "    split_idx = int(len(tokenized_corpus) * 0.8)\n",
    "    train_data = tokenized_corpus[:split_idx]\n",
    "    test_data = tokenized_corpus[split_idx:]\n",
    "\n",
    "    # 4. Initialize and Train\n",
    "    model = BigramLanguageModel(alpha=0.01) # Reduced alpha\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.train(train_data)\n",
    "    end_time = time.time()\n",
    "    print(f\"Time to build model: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "    # 5. Generate Text\n",
    "    print(\"\\n--- Generated Reviews ---\")\n",
    "    for _ in range(3):\n",
    "        print(f\"- {model.generate_sentence()}\")\n",
    "\n",
    "    # 6. Evaluate Perplexity\n",
    "    print(\"\\n--- Evaluation ---\")\n",
    "    pp = model.calculate_perplexity(test_data)\n",
    "    print(f\"Model Perplexity on Test Set: {pp:.2f}\")\n",
    "    \n",
    "    # 7. Autocomplete Demo\n",
    "    print(\"\\n--- Autocomplete Demo ---\")\n",
    "    prompts = [\n",
    "        \"The movie was\",\n",
    "        \"I really liked\",\n",
    "        \"The acting is\",\n",
    "        \"This film is a complete\"\n",
    "    ]\n",
    "    for p in prompts:\n",
    "        completed = model.autocomplete(p, preprocessor)\n",
    "        print(f\"Prompt: '{p}'\\nResult: {completed}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate Bigram Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing for Bigram Model...\n",
      "Training model on full vocabulary...\n",
      "Training complete. Vocab size: 31602\n",
      "Time to train Bigram: 0.4484 seconds\n",
      "Bigram Perplexity: 471.94\n",
      "- Generated: <s> this is an independent film that these two song written here and humanist he ha touched in mideval france contest\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuration\n",
    "preprocessor_classic = TextPreprocessor(\n",
    "    remove_html=True,\n",
    "    lowercase=True,\n",
    "    remove_punctuation=True,\n",
    "    remove_stopwords=False,\n",
    "    lemmatize=True,\n",
    "    expand_contractions=True\n",
    ")\n",
    "\n",
    "tokenized_corpus_bi = []\n",
    "print(\"Preprocessing for Bigram Model...\")\n",
    "# Using a subset for speed in notebook if needed, or full\n",
    "for r in reviews[:5000]: # Limit for demo speed\n",
    "    cleaned_text = preprocessor_classic.process_text(r)\n",
    "    tokens = cleaned_text.split()\n",
    "    tokens = ['<s>'] + tokens + ['</s>']\n",
    "    tokenized_corpus_bi.append(tokens)\n",
    "\n",
    "# Split\n",
    "split_idx = int(len(tokenized_corpus_bi) * 0.8)\n",
    "train_bi = tokenized_corpus_bi[:split_idx]\n",
    "test_bi = tokenized_corpus_bi[split_idx:]\n",
    "\n",
    "# Train\n",
    "model_bi = BigramLanguageModel(alpha=0.01)\n",
    "start_time = time.time()\n",
    "model_bi.train(train_bi)\n",
    "print(f\"Time to train Bigram: {time.time() - start_time:.4f} seconds\")\n",
    "\n",
    "# Evaluate\n",
    "pp_bi = model_bi.calculate_perplexity(test_bi)\n",
    "print(f\"Bigram Perplexity: {pp_bi:.2f}\")\n",
    "\n",
    "# Generate\n",
    "print(\"- Generated: \" + model_bi.generate_sentence())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trigram Language Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramLanguageModel:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the Trigram Model.\n",
    "        \n",
    "        Args:\n",
    "            alpha (float): The smoothing parameter for Laplace smoothing. \n",
    "                           Default is 0.01.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        # trigram_counts: count of (w1, w2, w3) aka given w1, w2, what is w3?\n",
    "        # Structure: dict[(w1, w2)] -> dict[w3] -> count\n",
    "        self.trigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        # bigram_counts: count of (w1, w2) as a history.\n",
    "        # Structure: dict[(w1, w2)] -> count\n",
    "        self.bigram_counts = defaultdict(int)\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        \n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        self.total_trigrams = 0\n",
    "        self.total_unigrams = 0\n",
    "        \n",
    "        # Interpolation weights\n",
    "        self.lambda1 = 0.1 # Unigram\n",
    "        self.lambda2 = 0.3 # Bigram\n",
    "        self.lambda3 = 0.6 # Trigram\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the model on a corpus of tokenized sentences.\n",
    "        Uses the full vocabulary (no <UNK> thresholding).\n",
    "        \"\"\"\n",
    "        print(\"Training model on full vocabulary...\")\n",
    "        for sentence in corpus:\n",
    "            # Update vocabulary and unigram counts\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "                self.unigram_counts[word] += 1\n",
    "                self.total_unigrams += 1\n",
    "            \n",
    "            # Update bigram counts (for backoff)\n",
    "            for i in range(len(sentence) - 1):\n",
    "                self.bigram_counts[(sentence[i], sentence[i+1])] += 1\n",
    "\n",
    "            # Update trigram counts\n",
    "            # Sentence is expected to be padded like ['<s>', '<s>', 'w1', ..., 'wn', '</s>']\n",
    "            for i in range(len(sentence) - 2):\n",
    "                w_1 = sentence[i]\n",
    "                w_2 = sentence[i+1]\n",
    "                w_3 = sentence[i+2]\n",
    "                \n",
    "                self.trigram_counts[(w_1, w_2)][w_3] += 1\n",
    "                self.total_trigrams += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f\"Training complete. Vocab size: {self.vocab_size}\")\n",
    "\n",
    "    def get_probability(self, w_1, w_2, w_3):\n",
    "        \"\"\"\n",
    "        Calculates the interpolated probability P(w_3 | w_1, w_2).\n",
    "        P = L3 * P(w3|w1,w2) + L2 * P(w3|w2) + L1 * P(w3)\n",
    "        \"\"\"\n",
    "        # 1. Trigram Probability\n",
    "        trigram_count = self.trigram_counts[(w_1, w_2)][w_3]\n",
    "        bigram_context_count = self.bigram_counts[(w_1, w_2)]\n",
    "        \n",
    "        p_tri_num = trigram_count + self.alpha\n",
    "        p_tri_den = bigram_context_count + (self.alpha * self.vocab_size)\n",
    "        p_tri = p_tri_num / p_tri_den\n",
    "        \n",
    "        # 2. Bigram Probability (Backoff)\n",
    "        bigram_count = self.bigram_counts[(w_2, w_3)]\n",
    "        unigram_context_count = self.unigram_counts[w_2]\n",
    "        \n",
    "        p_bi_num = bigram_count + self.alpha\n",
    "        p_bi_den = unigram_context_count + (self.alpha * self.vocab_size)\n",
    "        p_bi = p_bi_num / p_bi_den\n",
    "        \n",
    "        # 3. Unigram Probability\n",
    "        unigram_count = self.unigram_counts[w_3]\n",
    "        p_uni_num = unigram_count + self.alpha\n",
    "        p_uni_den = self.total_unigrams + (self.alpha * self.vocab_size)\n",
    "        p_uni = p_uni_num / p_uni_den\n",
    "        \n",
    "        return (self.lambda3 * p_tri) + (self.lambda2 * p_bi) + (self.lambda1 * p_uni)\n",
    "\n",
    "    def calculate_perplexity(self, test_corpus):\n",
    "        \"\"\"\n",
    "        Calculates the perplexity of the model on a test corpus.\n",
    "        \"\"\"\n",
    "        log_prob_sum = 0\n",
    "        N = 0\n",
    "        \n",
    "        for sentence in test_corpus:\n",
    "            for i in range(len(sentence) - 2):\n",
    "                w_1 = sentence[i]\n",
    "                w_2 = sentence[i+1]\n",
    "                w_3 = sentence[i+2]\n",
    "                \n",
    "                prob = self.get_probability(w_1, w_2, w_3)\n",
    "                \n",
    "                log_prob_sum += math.log2(prob)\n",
    "                N += 1\n",
    "        \n",
    "        if N == 0: return float('inf')\n",
    "        \n",
    "        avg_log_prob = -log_prob_sum / N\n",
    "        perplexity = 2 ** avg_log_prob\n",
    "        return perplexity\n",
    "\n",
    "    def generate_sentence(self, max_length=20):\n",
    "        \"\"\"\n",
    "        Generates a random sentence.\n",
    "        \"\"\"\n",
    "        # Start with two padding tokens\n",
    "        current_w1 = \"<s>\"\n",
    "        current_w2 = \"<s>\"\n",
    "        sentence = [current_w1, current_w2]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # If we generated the end token, stop\n",
    "            if current_w2 == \"</s>\":\n",
    "                break\n",
    "                \n",
    "            possible_next = self.trigram_counts[(current_w1, current_w2)]\n",
    "            \n",
    "            if not possible_next:\n",
    "                # If unknown history, we can't progress. \n",
    "                break \n",
    "\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            sentence.append(next_word)\n",
    "            current_w1 = current_w2\n",
    "            current_w2 = next_word\n",
    "            \n",
    "        # Return joined sentence, removing start tokens\n",
    "        # Typically we don't show <s> <s>\n",
    "        # The list has ['<s>', '<s>', 'word1', ... '</s>' maybe]\n",
    "        # We can strip the first two <s>\n",
    "        return \" \".join(sentence[2:])\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        \"\"\"\n",
    "        Completes a given text prompt using the trained model.\n",
    "        \"\"\"\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        \n",
    "        # Determine context words (need 2)\n",
    "        if len(tokens) >= 2:\n",
    "            current_w1 = tokens[-2]\n",
    "            current_w2 = tokens[-1]\n",
    "        elif len(tokens) == 1:\n",
    "            current_w1 = \"<s>\"\n",
    "            current_w2 = tokens[-1]\n",
    "        else:\n",
    "            current_w1 = \"<s>\"\n",
    "            current_w2 = \"<s>\"\n",
    "            \n",
    "        # Handle OOV - simplistic approach, similar to bigram fallbacks could be added, \n",
    "        # but here we rely on smoothing or break if empty.\n",
    "        \n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_w2 == \"</s>\":\n",
    "                break\n",
    "            \n",
    "            possible_next = self.trigram_counts[(current_w1, current_w2)]\n",
    "            \n",
    "            if not possible_next:\n",
    "                # If we dead end, we could maybe try fallback to bigram?\n",
    "                # But for strict trigram implementation request:\n",
    "                break\n",
    "                \n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            generated_tokens.append(next_word)\n",
    "            current_w1 = current_w2\n",
    "            current_w2 = next_word\n",
    "            \n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n",
    "\n",
    "\n",
    "# --- Example Usage with IMDB Data ---\n",
    "\n",
    "def main():\n",
    "    # 1. Load Data\n",
    "    try:\n",
    "        df = pd.read_csv('IMDB Dataset.csv')\n",
    "        print(\"Dataset loaded successfully.\")\n",
    "        reviews = df['review'].tolist() \n",
    "    except FileNotFoundError:\n",
    "        print(\"IMDB Dataset.csv not found. Using dummy data or trying small.\")\n",
    "        try:\n",
    "             df = pd.read_csv('IMDB Dataset_small.csv')\n",
    "             print(\"IMDB Dataset_small.csv loaded.\")\n",
    "             reviews = df['review'].tolist()\n",
    "        except FileNotFoundError:\n",
    "             reviews = [\"The movie was terrible.\", \"I loved the movie.\"]\n",
    "\n",
    "    # 2. Apply Preprocessing\n",
    "    preprocessor = TextPreprocessor(\n",
    "        remove_html=True,\n",
    "        lowercase=True,\n",
    "        remove_punctuation=False,\n",
    "        remove_stopwords=False,\n",
    "        lemmatize=False,\n",
    "        expand_contractions=True)\n",
    "        \n",
    "    tokenized_corpus = []\n",
    "    print(\"Preprocessing texts...\")\n",
    "    for r in reviews:\n",
    "        cleaned_text = preprocessor.process_text(r)\n",
    "        tokens = cleaned_text.split()\n",
    "        # Trigram needs two start tokens to have context for the first real word\n",
    "        tokens = ['<s>', '<s>'] + tokens + ['</s>']\n",
    "        tokenized_corpus.append(tokens)\n",
    "\n",
    "    # 3. Split Train/Test\n",
    "    split_idx = int(len(tokenized_corpus) * 0.8)\n",
    "    train_data = tokenized_corpus[:split_idx]\n",
    "    test_data = tokenized_corpus[split_idx:]\n",
    "\n",
    "    # 4. Initialize and Train\n",
    "    model = TrigramLanguageModel(alpha=0.01)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.train(train_data)\n",
    "    end_time = time.time()\n",
    "    print(f\"Time to build model: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "    # 5. Generate Text\n",
    "    print(\"\\n--- Generated Reviews ---\")\n",
    "    for _ in range(3):\n",
    "        # We might generate '</s>' at the end, which generate_sentence returns.\n",
    "        print(f\"- {model.generate_sentence()}\")\n",
    "\n",
    "    # 6. Evaluate Perplexity\n",
    "    print(\"\\n--- Evaluation ---\")\n",
    "    pp = model.calculate_perplexity(test_data)\n",
    "    print(f\"Model Perplexity on Test Set: {pp:.2f}\")\n",
    "    \n",
    "    # 7. Autocomplete Demo\n",
    "    print(\"\\n--- Autocomplete Demo ---\")\n",
    "    prompts = [\n",
    "        \"The movie was\",\n",
    "        \"I really liked\",\n",
    "        \"The acting is\",\n",
    "        \"This film is a complete\"\n",
    "    ]\n",
    "    for p in prompts:\n",
    "        completed = model.autocomplete(p, preprocessor)\n",
    "        print(f\"Prompt: '{p}'\\nResult: {completed}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate Trigram Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing for Trigram Model...\n",
      "Training model on full vocabulary...\n",
      "Training complete. Vocab size: 31602\n",
      "Time to train Trigram: 1.6140 seconds\n",
      "Trigram Perplexity: 670.17\n",
      "- Generated: i think this should forever be a good a usual loved omar epps ice cube and jennifer 8 showcased an\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reuse preprocessor but Trigram needs different start tokens usually? \n",
    "# The script logic: \"tokens = ['<s>', '<s>'] + tokens + ['</s>']\"\n",
    "\n",
    "tokenized_corpus_tri = []\n",
    "print(\"Preprocessing for Trigram Model...\")\n",
    "for r in reviews[:5000]: \n",
    "    cleaned_text = preprocessor_classic.process_text(r)\n",
    "    tokens = cleaned_text.split()\n",
    "    tokens = ['<s>', '<s>'] + tokens + ['</s>']\n",
    "    tokenized_corpus_tri.append(tokens)\n",
    "\n",
    "# Split\n",
    "split_idx = int(len(tokenized_corpus_tri) * 0.8)\n",
    "train_tri = tokenized_corpus_tri[:split_idx]\n",
    "test_tri = tokenized_corpus_tri[split_idx:]\n",
    "\n",
    "# Train\n",
    "model_tri = TrigramLanguageModel(alpha=0.01)\n",
    "start_time = time.time()\n",
    "model_tri.train(train_tri)\n",
    "print(f\"Time to train Trigram: {time.time() - start_time:.4f} seconds\")\n",
    "\n",
    "# Evaluate\n",
    "pp_tri = model_tri.calculate_perplexity(test_tri)\n",
    "print(f\"Trigram Perplexity: {pp_tri:.2f}\")\n",
    "\n",
    "# Generate\n",
    "print(\"- Generated: \" + model_tri.generate_sentence())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Neural Language Model (LSTM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBED_DIM = 64\n",
    "HIDDEN_DIM = 512\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_EPOCHS = 3 # Reduced for notebook execution speed\n",
    "MAX_SEQ_LEN = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx:\n",
    "            self.token_to_idx = token_to_idx\n",
    "        else:\n",
    "            self.token_to_idx = {\"<PAD>\": 0, \"<UNK>\": 1, \"<s>\": 2, \"</s>\": 3}\n",
    "        self.idx_to_token = {v: k for k, v in self.token_to_idx.items()}\n",
    "        \n",
    "    def build_vocab(self, sentences, min_freq=2):\n",
    "        print(\"Building vocabulary...\")\n",
    "        all_tokens = [token for sent in sentences for token in sent]\n",
    "        counts = Counter(all_tokens)\n",
    "        \n",
    "        for token, count in counts.items():\n",
    "            if count >= min_freq and token not in self.token_to_idx:\n",
    "                self.token_to_idx[token] = len(self.token_to_idx)\n",
    "                \n",
    "        self.idx_to_token = {v: k for k, v in self.token_to_idx.items()}\n",
    "        print(f\"Vocabulary size: {len(self.token_to_idx)}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "    \n",
    "    def stoi(self, token):\n",
    "        return self.token_to_idx.get(token, self.token_to_idx[\"<UNK>\"])\n",
    "        \n",
    "    def itos(self, idx):\n",
    "        return self.idx_to_token.get(idx, \"<UNK>\")\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, sentences, vocab):\n",
    "        self.sentences = sentences\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_sent = self.sentences[idx]\n",
    "        # Numericalize\n",
    "        indexed = [self.vocab.stoi(t) for t in tokenized_sent]\n",
    "        return torch.tensor(indexed, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable length sentences via padding.\n",
    "    \"\"\"\n",
    "    # batch is a list of tensors\n",
    "    # Sort by length (descending) for pack_padded_sequence\n",
    "    batch.sort(key=lambda x: len(x), reverse=True)\n",
    "    \n",
    "    # Separate source and target\n",
    "    # Source: <s> w1 w2 ... wn\n",
    "    # Target: w1 w2 ... wn </s>\n",
    "    # Actually, our sentences in 'sentences' list usually have <s> and </s> already.\n",
    "    # So we just take :-1 as input and 1: as target.\n",
    "    \n",
    "    inputs = [item[:-1] for item in batch]\n",
    "    targets = [item[1:] for item in batch]\n",
    "    \n",
    "    lengths = torch.tensor([len(x) for x in inputs], dtype=torch.long)\n",
    "    \n",
    "    # Pad sequences\n",
    "    # padding_value=0 is <PAD>\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return inputs_padded, targets_padded, lengths\n",
    "\n",
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(NeuralLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, lengths=None, hidden=None):\n",
    "        # x: (batch, seq_len)\n",
    "        embed = self.embedding(x) # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        if lengths is not None:\n",
    "            # Pack\n",
    "            packed_embed = pack_padded_sequence(embed, lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
    "            packed_out, hidden = self.lstm(packed_embed, hidden)\n",
    "            # Unpack\n",
    "            output, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        else:\n",
    "            # No packing (e.g. inference)\n",
    "            output, hidden = self.lstm(embed, hidden)\n",
    "            \n",
    "        # output: (batch, seq_len, hidden_dim) (padded where needed)\n",
    "        \n",
    "        logits = self.fc(output) # (batch, seq_len, vocab_size)\n",
    "        return logits, hidden\n",
    "\n",
    "def generate_text(model, vocab, start_prompt=\"The movie\", max_len=20, device='cpu', temperature=1.0):\n",
    "    model.eval()\n",
    "    preprocessor = TextPreprocessor(lowercase=True)\n",
    "    tokens = preprocessor.process_text(start_prompt).split()\n",
    "    \n",
    "    current_idx = [vocab.stoi(t) for t in tokens]\n",
    "    # Add start token if not present logic? \n",
    "    # The model trained on <s>... so prompt should ideally start with something logical.\n",
    "    # If we feed \"The movie\", it's mid-sentence-ish.\n",
    "    \n",
    "    input_seq = torch.tensor(current_idx, dtype=torch.long).unsqueeze(0).to(device) # (1, seq_len)\n",
    "    \n",
    "    generated = list(tokens)\n",
    "    \n",
    "    hidden = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            logits, hidden = model(input_seq, hidden=hidden)\n",
    "            \n",
    "            # Get last time step\n",
    "            last_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Apply temperature\n",
    "            if temperature != 1.0:\n",
    "                last_logits = last_logits / temperature\n",
    "                \n",
    "            probs = torch.softmax(last_logits, dim=0)\n",
    "            \n",
    "            # Sample\n",
    "            next_token_idx = torch.multinomial(probs, 1).item()\n",
    "            next_token = vocab.itos(next_token_idx)\n",
    "            \n",
    "            if next_token == \"</s>\":\n",
    "                break\n",
    "                \n",
    "            generated.append(next_token)\n",
    "            \n",
    "            # Next input is the single token we just generated (feeding back one by one)\n",
    "            # Or we could feed the whole sequence, but feeding 1 is efficient IF we keep hidden state.\n",
    "            input_seq = torch.tensor([[next_token_idx]], dtype=torch.long).to(device)\n",
    "            \n",
    "    return \" \".join(generated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Neural Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Preprocessing for Neural Model...\n",
      "Building vocabulary...\n",
      "Vocabulary size: 7619\n",
      "Epoch 1/3, Loss: 6.4789\n",
      "Epoch 2/3, Loss: 5.6727\n",
      "Epoch 3/3, Loss: 5.2935\n",
      "Generated via Neural:\n",
      "the movie was meant to death .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Preprocess for Neural\n",
    "preprocessor_neural = TextPreprocessor(\n",
    "    remove_html=True,\n",
    "    lowercase=True,\n",
    "    expand_contractions=True,\n",
    "    remove_punctuation=False\n",
    ")\n",
    "\n",
    "print(\"Preprocessing for Neural Model...\")\n",
    "tokenized_sentences_neural = []\n",
    "for r in reviews[:2000]: # Smaller subset for Neural training in notebook\n",
    "    txt = preprocessor_neural.process_text(r)\n",
    "    toks = txt.split()\n",
    "    toks = ['<s>'] + toks[:MAX_SEQ_LEN] + ['</s>']\n",
    "    tokenized_sentences_neural.append(toks)\n",
    "\n",
    "# Vocab\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(tokenized_sentences_neural, min_freq=2)\n",
    "\n",
    "# Dataset\n",
    "dataset = IMDBDataset(tokenized_sentences_neural, vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Model\n",
    "model_neural = NeuralLM(len(vocab), EMBED_DIM, HIDDEN_DIM).to(device)\n",
    "optimizer = optim.Adam(model_neural.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Loop\n",
    "model_neural.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    for inputs, targets, lengths in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model_neural(inputs, lengths)\n",
    "        loss = criterion(logits.view(-1, len(vocab)), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Generate\n",
    "print(\"Generated via Neural:\")\n",
    "print(generate_text(model_neural, vocab, \"The movie was\", device=device))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
