import json
import os

nb_path = r"c:\Users\vogle\Gitstuff\uni\NLP\NLP_Project_Notebook_Optimized.ipynb"

with open(nb_path, 'r', encoding='utf-8') as f:
    nb = json.load(f)

cells = nb['cells']

# Helper to find cell by source content
def find_cell_index(cells, distinct_string):
    for i, cell in enumerate(cells):
        source_str = "".join(cell.get('source', []))
        if distinct_string in source_str:
            return i
    return -1

# 1. Add SimpleBigram
bigram_idx = find_cell_index(cells, "class BigramLanguageModel:")
simple_bigram_code = [
    "\n",
    "class SimpleBigramLanguageModel:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        print(\"Training Simple Bigram on full vocabulary...\")\n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "                self.unigram_counts[word] += 1\n",
    "            \n",
    "            for i in range(len(sentence) - 1):\n",
    "                w_curr = sentence[i]\n",
    "                w_next = sentence[i+1]\n",
    "                self.bigram_counts[w_curr][w_next] += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f\"Simple Bigram Training complete. Vocab size: {self.vocab_size}\")\n",
    "\n",
    "    def get_probability(self, prev_word, word):\n",
    "        bigram_count = self.bigram_counts[prev_word][word]\n",
    "        unigram_count_prev = self.unigram_counts[prev_word]\n",
    "        \n",
    "        num = bigram_count + self.alpha\n",
    "        den = unigram_count_prev + (self.alpha * self.vocab_size)\n",
    "        return num / den\n",
    "\n",
    "    def calculate_perplexity(self, test_corpus):\n",
    "        log_prob_sum = 0\n",
    "        N = 0\n",
    "        \n",
    "        for sentence in test_corpus:\n",
    "            for i in range(len(sentence) - 1):\n",
    "                w_curr = sentence[i]\n",
    "                w_next = sentence[i+1]\n",
    "                prob = self.get_probability(w_curr, w_next)\n",
    "                log_prob_sum += math.log2(prob)\n",
    "                N += 1\n",
    "        \n",
    "        if N == 0: return float('inf')\n",
    "        \n",
    "        avg_log_prob = -log_prob_sum / N\n",
    "        perplexity = 2 ** avg_log_prob\n",
    "        return perplexity\n",
    "\n",
    "    def generate_sentence(self, max_length=20):\n",
    "        current_word = \"<s>\"\n",
    "        sentence = [current_word]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            if not possible_next:\n",
    "                break \n",
    "\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            sentence.append(next_word)\n",
    "            current_word = next_word\n",
    "        return \" \".join(sentence[1:])\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        if not tokens:\n",
    "            current_word = \"<s>\"\n",
    "        else:\n",
    "            current_word = tokens[-1]\n",
    "        if current_word not in self.vocab:\n",
    "            current_word = \"<UNK>\"\n",
    "            \n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            if not possible_next:\n",
    "                current_word = \"<UNK>\"\n",
    "                possible_next = self.bigram_counts[current_word]\n",
    "            if not possible_next:\n",
    "                break\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            generated_tokens.append(next_word)\n",
    "            current_word = next_word\n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n"
]

if bigram_idx != -1:
    cells[bigram_idx]['source'].extend(simple_bigram_code)

# 2. Add SimpleTrigram
trigram_idx = find_cell_index(cells, "class TrigramLanguageModel:")
simple_trigram_code = [
    "\n",
    "class SimpleTrigramLanguageModel:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.trigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.bigram_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        print(\"Training Simple Trigram on full vocabulary...\")\n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "            \n",
    "            for i in range(len(sentence) - 1):\n",
    "                self.bigram_counts[(sentence[i], sentence[i+1])] += 1\n",
    "\n",
    "            for i in range(len(sentence) - 2):\n",
    "                w_1 = sentence[i]\n",
    "                w_2 = sentence[i+1]\n",
    "                w_3 = sentence[i+2]\n",
    "                self.trigram_counts[(w_1, w_2)][w_3] += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f\"Simple Trigram Training complete. Vocab size: {self.vocab_size}\")\n",
    "\n",
    "    def get_probability(self, w_1, w_2, w_3):\n",
    "        trigram_count = self.trigram_counts[(w_1, w_2)][w_3]\n",
    "        bigram_context_count = self.bigram_counts[(w_1, w_2)]\n",
    "        \n",
    "        num = trigram_count + self.alpha\n",
    "        den = bigram_context_count + (self.alpha * self.vocab_size)\n",
    "        return num / den\n",
    "\n",
    "    def calculate_perplexity(self, test_corpus):\n",
    "        log_prob_sum = 0\n",
    "        N = 0\n",
    "        \n",
    "        for sentence in test_corpus:\n",
    "            for i in range(len(sentence) - 2):\n",
    "                w_1 = sentence[i]\n",
    "                w_2 = sentence[i+1]\n",
    "                w_3 = sentence[i+2]\n",
    "                prob = self.get_probability(w_1, w_2, w_3)\n",
    "                log_prob_sum += math.log2(prob)\n",
    "                N += 1\n",
    "        \n",
    "        if N == 0: return float('inf')\n",
    "        \n",
    "        avg_log_prob = -log_prob_sum / N\n",
    "        perplexity = 2 ** avg_log_prob\n",
    "        return perplexity\n",
    "\n",
    "    def generate_sentence(self, max_length=20):\n",
    "        current_w1 = \"<s>\"\n",
    "        current_w2 = \"<s>\"\n",
    "        sentence = [current_w1, current_w2]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            if current_w2 == \"</s>\":\n",
    "                break\n",
    "            possible_next = self.trigram_counts[(current_w1, current_w2)]\n",
    "            if not possible_next:\n",
    "                # If unknown history, break \n",
    "                break \n",
    "\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            sentence.append(next_word)\n",
    "            current_w1 = current_w2\n",
    "            current_w2 = next_word\n",
    "            \n",
    "        return \" \".join(sentence[2:])\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        if len(tokens) >= 2:\n",
    "            current_w1 = tokens[-2]\n",
    "            current_w2 = tokens[-1]\n",
    "        elif len(tokens) == 1:\n",
    "            current_w1 = \"<s>\"\n",
    "            current_w2 = tokens[-1]\n",
    "        else:\n",
    "            current_w1 = \"<s>\"\n",
    "            current_w2 = \"<s>\"\n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_w2 == \"</s>\":\n",
    "                break\n",
    "            possible_next = self.trigram_counts[(current_w1, current_w2)]\n",
    "            if not possible_next:\n",
    "                break\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            generated_tokens.append(next_word)\n",
    "            current_w1 = current_w2\n",
    "            current_w2 = next_word\n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n"
]

if trigram_idx != -1:
    cells[trigram_idx]['source'].extend(simple_trigram_code)

# 3. Update Comparison Cell
comp_idx = find_cell_index(cells, "bigram_model = BigramLanguageModel(alpha=0.01)")
new_comp_source = [
    "\n",
    "# Initialize\n",
    "bigram_model = BigramLanguageModel(alpha=0.01)\n",
    "simple_bigram_model = SimpleBigramLanguageModel(alpha=0.01)\n",
    "trigram_model = TrigramLanguageModel(alpha=0.01)\n",
    "simple_trigram_model = SimpleTrigramLanguageModel(alpha=0.01)\n",
    "\n",
    "# Train Bigram\n",
    "print(\"Training Bigram...\")\n",
    "start = time.time()\n",
    "train_bi_adapted = [s[1:] for s in train_ngram] \n",
    "test_bi_adapted = [s[1:] for s in test_ngram]\n",
    "bigram_model.train(train_bi_adapted)\n",
    "bigram_time = time.time() - start\n",
    "print(f\"Bigram Trained in {bigram_time:.4f}s\")\n",
    "\n",
    "# Train Simple Bigram\n",
    "print(\"Training Simple Bigram...\")\n",
    "start = time.time()\n",
    "simple_bigram_model.train(train_bi_adapted)\n",
    "simple_bigram_time = time.time() - start\n",
    "print(f\"Simple Bigram Trained in {simple_bigram_time:.4f}s\")\n",
    "\n",
    "# Train Trigram\n",
    "print(\"Training Trigram...\")\n",
    "start = time.time()\n",
    "trigram_model.train(train_ngram)\n",
    "trigram_time = time.time() - start\n",
    "print(f\"Trigram Trained in {trigram_time:.4f}s\")\n",
    "\n",
    "# Train Simple Trigram\n",
    "print(\"Training Simple Trigram...\")\n",
    "start = time.time()\n",
    "simple_trigram_model.train(train_ngram)\n",
    "simple_trigram_time = time.time() - start\n",
    "print(f\"Simple Trigram Trained in {simple_trigram_time:.4f}s\")\n",
    "\n",
    "# Perplexity\n",
    "pp_bi = bigram_model.calculate_perplexity(test_bi_adapted)\n",
    "pp_simple_bi = simple_bigram_model.calculate_perplexity(test_bi_adapted)\n",
    "pp_tri = trigram_model.calculate_perplexity(test_ngram)\n",
    "pp_simple_tri = simple_trigram_model.calculate_perplexity(test_ngram)\n",
    "\n",
    "print(f\"\\nBigram Perplexity: {pp_bi:.2f}\")\n",
    "print(f\"Simple Bigram Perplexity: {pp_simple_bi:.2f}\")\n",
    "print(f\"Trigram Perplexity: {pp_tri:.2f}\")\n",
    "print(f\"Simple Trigram Perplexity: {pp_simple_tri:.2f}\")\n",
    "\n",
    "# Generation Comparison\n",
    "print(\"\\n--- Generation Comparison (Unprompted) ---\")\n",
    "print(\"Bigram:\")\n",
    "for i in range(2):\n",
    "    print(f\"  {i+1}. {bigram_model.generate_sentence()}\")\n",
    "print(\"Simple Bigram:\")\n",
    "for i in range(2):\n",
    "    print(f\"  {i+1}. {simple_bigram_model.generate_sentence()}\")\n",
    "\n",
    "print(\"\\nTrigram:\")\n",
    "for i in range(2):\n",
    "    print(f\"  {i+1}. {trigram_model.generate_sentence()}\")\n",
    "print(\"Simple Trigram:\")\n",
    "for i in range(2):\n",
    "    print(f\"  {i+1}. {simple_trigram_model.generate_sentence()}\")\n",
    "\n"
]

if comp_idx != -1:
    cells[comp_idx]['source'] = new_comp_source

# 4. Update Visualization Cell
vis_idx = find_cell_index(cells, "models = ['Bigram', 'Trigram', 'LSTM']")
new_vis_source = [
    "\n",
    "models = ['Bigram', 'Simple Bi', 'Trigram', 'Simple Tri', 'LSTM']\n",
    "times = [bigram_time, simple_bigram_time, trigram_time, simple_trigram_time, neural_time]\n",
    "perplexities = [pp_bi, pp_simple_bi, pp_tri, pp_simple_tri, pp_neural]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time Plot\n",
    "sns.barplot(x=models, y=times, ax=axes[0], palette=\"viridis\")\n",
    "axes[0].set_title(\"Training Time (seconds)\")\n",
    "axes[0].set_ylabel(\"Seconds\")\n",
    "for i, v in enumerate(times):\n",
    "    axes[0].text(i, v + 0.1, f\"{v:.1f}s\", ha='center')\n",
    "\n",
    "# Perplexity Plot\n",
    "sns.barplot(x=models, y=perplexities, ax=axes[1], palette=\"magma\")\n",
    "axes[1].set_title(\"Test Perplexity (Lower is Better)\")\n",
    "axes[1].set_ylabel(\"Perplexity\")\n",
    "for i, v in enumerate(perplexities):\n",
    "    axes[1].text(i, v + 1, f\"{v:.1f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
]

if vis_idx != -1:
    cells[vis_idx]['source'] = new_vis_source

# 5. Update Qualitative Cell
qual_idx = find_cell_index(cells, "prompts = [None, \"The movie was\", \"I thought that\"]")
new_qual_source = [
    "\n",
    "prompts = [None, \"The movie was\", \"I thought that\"]\n",
    "\n",
    "def get_continuation(model_type, prompt):\n",
    "    if prompt is None:\n",
    "        if model_type == 'LSTM':\n",
    "            return generate_text(model_lstm, vocab, \"The\", device=device)\n",
    "        elif model_type == 'Bigram':\n",
    "            return bigram_model.generate_sentence()\n",
    "        elif model_type == 'Simple Bi':\n",
    "            return simple_bigram_model.generate_sentence()\n",
    "        elif model_type == 'Trigram':\n",
    "            return trigram_model.generate_sentence()\n",
    "        elif model_type == 'Simple Tri':\n",
    "            return simple_trigram_model.generate_sentence()\n",
    "    else:\n",
    "        # Prompted\n",
    "        if model_type == 'LSTM':\n",
    "            return generate_text(model_lstm, vocab, prompt, device=device)\n",
    "        elif model_type == 'Bigram':\n",
    "            return bigram_model.autocomplete(prompt, ngram_prep)\n",
    "        elif model_type == 'Simple Bi':\n",
    "            return simple_bigram_model.autocomplete(prompt, ngram_prep)\n",
    "        elif model_type == 'Trigram':\n",
    "            return trigram_model.autocomplete(prompt, ngram_prep)\n",
    "        elif model_type == 'Simple Tri':\n",
    "            return simple_trigram_model.autocomplete(prompt, ngram_prep)\n",
    "\n",
    "results = []\n",
    "for p in prompts:\n",
    "    p_label = p if p else \"Unprompted\"\n",
    "    row = {\"Prompt\": p_label}\n",
    "    row[\"Bigram\"] = get_continuation('Bigram', p)\n",
    "    row[\"Simple Bi\"] = get_continuation('Simple Bi', p)\n",
    "    row[\"Trigram\"] = get_continuation('Trigram', p)\n",
    "    row[\"Simple Tri\"] = get_continuation('Simple Tri', p)\n",
    "    row[\"LSTM\"] = get_continuation('LSTM', p)\n",
    "    results.append(row)\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(res_df)\n",
    "\n"
]

if qual_idx != -1:
    cells[qual_idx]['source'] = new_qual_source

# Save
with open(nb_path, 'w', encoding='utf-8') as f:
    json.dump(nb, f, indent=1)

print("Notebook updated successfully.")
