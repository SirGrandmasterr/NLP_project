{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project: Language Modeling Comparison\n",
    "This notebook implements and compares three language models: **Bigram**, **Trigram**, and **Neural LSTM**.\n",
    "It includes a unified preprocessing pipeline, training comparisons, and generation quality checks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import html\n",
    "import string\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Setup Visuals\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Download resources if checks fail (quietly)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    \n",
    "print(\"Libraries loaded.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessor Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    A robust and professional preprocessing pipeline for NLP tasks.\n",
    "    Designed to handle IMDB movie reviews for Classical, Neural, and Transformer models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 remove_html: bool = True,\n",
    "                 lowercase: bool = True,\n",
    "                 remove_punctuation: bool = False,\n",
    "                 remove_stopwords: bool = False,\n",
    "                 lemmatize: bool = False,\n",
    "                 expand_contractions: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline with specific configuration flags.\n",
    "        \n",
    "        Args:\n",
    "            remove_html (bool): Strip HTML tags (e.g., <br />). Default True.\n",
    "            lowercase (bool): Convert text to lowercase. Default True.\n",
    "            remove_punctuation (bool): Remove punctuation characters.\n",
    "            remove_stopwords (bool): Remove standard English stopwords.\n",
    "            lemmatize (bool): Apply WordNet lemmatization.\n",
    "            expand_contractions (bool): Expand \"isn't\" to \"is not\".\n",
    "        \"\"\"\n",
    "        self.remove_html = remove_html\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        self.expand_contractions = expand_contractions\n",
    "\n",
    "        # Pre-load resources to optimize runtime\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stop_words.remove(\"not\")\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Simple contraction map for expansion\n",
    "        self.contractions_dict = {\n",
    "            \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
    "            \"haven't\": \"have not\", \"hasn't\": \"has not\", \"hadn't\": \"had not\", \"won't\": \"will not\",\n",
    "            \"wouldn't\": \"would not\", \"don't\": \"do not\", \"doesn't\": \"does not\", \"didn't\": \"did not\",\n",
    "            \"can't\": \"cannot\", \"couldn't\": \"could not\", \"shouldn't\": \"should not\", \"mightn't\": \"might not\",\n",
    "            \"mustn't\": \"must not\", \"i'm\": \"i am\", \"you're\": \"you are\", \"he's\": \"he is\", \"she's\": \"she is\",\n",
    "            \"it's\": \"it is\", \"we're\": \"we are\", \"they're\": \"they are\", \"i've\": \"i have\", \"you've\": \"you have\",\n",
    "            \"we've\": \"we have\", \"they've\": \"they have\", \"i'll\": \"i will\", \"you'll\": \"you will\",\n",
    "            \"he'll\": \"he will\", \"she'll\": \"she will\", \"we'll\": \"we will\", \"they'll\": \"they will\"\n",
    "        }\n",
    "        self.contractions_re = re.compile('(%s)' % '|'.join(self.contractions_dict.keys()))\n",
    "\n",
    "    def _clean_html(self, text: str) -> str:\n",
    "        \"\"\"Removes HTML tags and unescapes HTML entities.\"\"\"\n",
    "        text = html.unescape(text)\n",
    "        # Regex for HTML tags\n",
    "        clean = re.compile('<.*?>')\n",
    "        return re.sub(clean, ' ', text)\n",
    "\n",
    "    def _expand_contractions(self, text: str) -> str:\n",
    "        \"\"\"Expands common English contractions.\"\"\"\n",
    "        def replace(match):\n",
    "            return self.contractions_dict[match.group(0)]\n",
    "        return self.contractions_re.sub(replace, text)\n",
    "\n",
    "    def _remove_punct(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes punctuation by replacing it with spaces.\n",
    "        This prevents 'word,word' from becoming 'wordword'.\n",
    "        \"\"\"\n",
    "        # Replace punctuation with a space\n",
    "        return re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)\n",
    "\n",
    "    def process_text(self, text: str) -> Union[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Main execution method. Applies enabled steps in the logical order.\n",
    "        \n",
    "        Returns:\n",
    "            str: If the final output is a joined string.\n",
    "            List[str]: If the processing flow ends in tokenization without re-joining.\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or not text:\n",
    "            return \"\"\n",
    "\n",
    "        # 1. Cleaning\n",
    "        if self.remove_html:\n",
    "            text = self._clean_html(text)\n",
    "        \n",
    "        # 2. Lowercasing\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "            \n",
    "        # 3. Expansion (must be after lowercasing for simple dict matching)\n",
    "        if self.expand_contractions:\n",
    "            text = self._expand_contractions(text)\n",
    "\n",
    "        # 4. Punctuation Removal\n",
    "        if self.remove_punctuation:\n",
    "            text = self._remove_punct(text)\n",
    "\n",
    "        # 5. Tokenization\n",
    "        # We always tokenize to perform word-level operations (stopword/lemma)\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # 6. Stopword Removal\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [w for w in tokens if w not in self.stop_words]\n",
    "\n",
    "        # 7. Lemmatization\n",
    "        if self.lemmatize:\n",
    "            tokens = [self.lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "        # Return list of tokens or join back to string depending on downstream need.\n",
    "        # For this pipeline, we generally return the list of tokens for Classical models,\n",
    "        # but for compatibility, we will join them back into a clean string \n",
    "        # because Tokenizers for Transformers/LSTMs often expect string input \n",
    "        # and do their own internal splitting.\n",
    "        \n",
    "        return \" \".join(tokens)\n",
    "\n",
    "# --- Usage Example / Demonstration ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Unified Preprocessing\n",
    "We pre-process the dataset once for all models to ensure fair comparison and save time.\n",
    "- **N-gram Corpus**: Lemmatized, punctuation kept (structural), contractions expanded.\n",
    "- **Neural Corpus**: Lowercase, no lemmatization (learns forms), standard tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 reviews.\n",
      "Preprocessing for N-gram models...\n",
      "Preprocessing for Neural models...\n",
      "Data ready. Train size: 8000, Test size: 2000\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "try:\n",
    "    df = pd.read_csv('IMDB Dataset.csv')\n",
    "    raw_reviews = df['review'].tolist()\n",
    "    print(f\"Loaded {len(raw_reviews)} reviews.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"IMDB Dataset.csv not found. Using dummy data.\")\n",
    "    raw_reviews = [\"The movie was terrible.\", \"I loved the film. It was great.\", \"The acting was bad.\"] * 100\n",
    "\n",
    "# Configuration\n",
    "SAMPLE_SIZE = 10000  # Adjust this for speed vs accuracy (Use len(raw_reviews) for full)\n",
    "raw_reviews = raw_reviews[:SAMPLE_SIZE]\n",
    "\n",
    "# 1. Pipeline for N-gram Models\n",
    "print(\"Preprocessing for N-gram models...\")\n",
    "ngram_prep = TextPreprocessor(\n",
    "    remove_html=True, \n",
    "    lowercase=True, \n",
    "    remove_punctuation=True, # Keep structure for n-grams\n",
    "    remove_stopwords=False, \n",
    "    lemmatize=True, \n",
    "    expand_contractions=True\n",
    ")\n",
    "\n",
    "ngram_corpus = []\n",
    "for r in raw_reviews:\n",
    "    cleaned = ngram_prep.process_text(r)\n",
    "    # Add start/end tokens. Bigram needs 1 start, Trigram needs 2. \n",
    "    # We will use 2 start tokens '<s>' '<s>' universally, Bigram can just ignore the extra one or we handle it in model.\n",
    "    # To be safe and exact to class definitions, let's store as lists.\n",
    "    tokens = cleaned.split()\n",
    "    # Bigram expects ['<s>', w1...]\n",
    "    # Trigram expects ['<s>', '<s>', w1...]\n",
    "    # We'll store the clean text list and pad per model or creating a generic '<s>' '<s>' ... '</s>'\n",
    "    # Let's create a standard list with 2 padding for maximum compatibility\n",
    "    tokens = ['<s>', '<s>'] + tokens + ['</s>']\n",
    "    ngram_corpus.append(tokens)\n",
    "\n",
    "# 2. Pipeline for Neural Model\n",
    "print(\"Preprocessing for Neural models...\")\n",
    "neural_prep = TextPreprocessor(\n",
    "    remove_html=True, \n",
    "    lowercase=True, \n",
    "    remove_punctuation=False,\n",
    "    remove_stopwords=False, \n",
    "    lemmatize=False, # Neural nets prefer original forms\n",
    "    expand_contractions=True\n",
    ")\n",
    "\n",
    "neural_corpus = []\n",
    "MAX_LEN = 100\n",
    "for r in raw_reviews:\n",
    "    cleaned = neural_prep.process_text(r)\n",
    "    tokens = cleaned.split()\n",
    "    # Neural model logic from script: ['<s>'] + tokens + ['</s>']\n",
    "    tokens = ['<s>'] + tokens[:MAX_LEN] + ['</s>']\n",
    "    neural_corpus.append(tokens)\n",
    "\n",
    "# Split (Sync split indices)\n",
    "split_idx = int(len(raw_reviews) * 0.8)\n",
    "\n",
    "train_ngram = ngram_corpus[:split_idx]\n",
    "test_ngram  = ngram_corpus[split_idx:]\n",
    "\n",
    "train_neural = neural_corpus[:split_idx]\n",
    "test_neural  = neural_corpus[split_idx:]\n",
    "\n",
    "print(f\"Data ready. Train size: {len(train_ngram)}, Test size: {len(test_ngram)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Definitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Bigram Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the Bigram Model.\n",
    "        \n",
    "        Args:\n",
    "            alpha (float): The smoothing parameter for Laplace smoothing. \n",
    "                           Default is 0.01.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        self.total_bigrams = 0\n",
    "        self.total_unigrams = 0\n",
    "        \n",
    "        self.lambda1 = 0.3 # Unigram\n",
    "        self.lambda2 = 0.7 # Bigram\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the model on a corpus of tokenized sentences.\n",
    "        Uses the full vocabulary (no <UNK> thresholding).\n",
    "        \"\"\"\n",
    "        print(\"Training model on full vocabulary...\")\n",
    "        for sentence in corpus:\n",
    "            # Update vocabulary and unigram counts\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "                self.unigram_counts[word] += 1\n",
    "                self.total_unigrams += 1\n",
    "            \n",
    "            # Update bigram counts\n",
    "            for i in range(len(sentence) - 1):\n",
    "                w_curr = sentence[i]\n",
    "                w_next = sentence[i+1]\n",
    "                self.bigram_counts[w_curr][w_next] += 1\n",
    "                self.total_bigrams += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f\"Training complete. Vocab size: {self.vocab_size}\")\n",
    "\n",
    "    def get_probability(self, prev_word, word):\n",
    "        \"\"\"\n",
    "        Calculates the interpolated probability P(word | prev_word).\n",
    "        P = L2 * P(word|prev) + L1 * P(word)\n",
    "        \"\"\"\n",
    "        # 1. Bigram Probability\n",
    "        bigram_count = self.bigram_counts[prev_word][word]\n",
    "        unigram_count_prev = self.unigram_counts[prev_word]\n",
    "        \n",
    "        p_bi_num = bigram_count + self.alpha\n",
    "        p_bi_den = unigram_count_prev + (self.alpha * self.vocab_size)\n",
    "        p_bi = p_bi_num / p_bi_den\n",
    "        \n",
    "        # 2. Unigram Probability\n",
    "        unigram_count_word = self.unigram_counts[word]\n",
    "        p_uni_num = unigram_count_word + self.alpha\n",
    "        p_uni_den = self.total_unigrams + (self.alpha * self.vocab_size)\n",
    "        p_uni = p_uni_num / p_uni_den\n",
    "        \n",
    "        return (self.lambda2 * p_bi) + (self.lambda1 * p_uni)\n",
    "\n",
    "    def calculate_perplexity(self, test_corpus):\n",
    "        \"\"\"\n",
    "        Calculates the perplexity of the model on a test corpus.\n",
    "        \"\"\"\n",
    "        log_prob_sum = 0\n",
    "        N = 0\n",
    "        \n",
    "        for sentence in test_corpus:\n",
    "            for i in range(len(sentence) - 1):\n",
    "                w_curr = sentence[i]\n",
    "                w_next = sentence[i+1]\n",
    "                \n",
    "                # We do not replace with <UNK>. If a word is unknown,\n",
    "                # get_probability handles it via smoothing.\n",
    "                prob = self.get_probability(w_curr, w_next)\n",
    "                \n",
    "                log_prob_sum += math.log2(prob)\n",
    "                N += 1\n",
    "        \n",
    "        if N == 0: return float('inf')\n",
    "        \n",
    "        avg_log_prob = -log_prob_sum / N\n",
    "        perplexity = 2 ** avg_log_prob\n",
    "        return perplexity\n",
    "\n",
    "    def generate_sentence(self, max_length=20):\n",
    "        \"\"\"\n",
    "        Generates a random sentence.\n",
    "        \"\"\"\n",
    "        current_word = \"<s>\"\n",
    "        sentence = [current_word]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "                \n",
    "            # If current_word was never seen in training (e.g. from a user prompt),\n",
    "            # unigram_count is 0. We fallback to uniform distribution or break.\n",
    "            # Here we sample from the whole vocab if unknown, or just observed followers if known.\n",
    "            \n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            \n",
    "            if not possible_next:\n",
    "                # Dead end or unknown word. \n",
    "                # Ideally: Sample uniformly from V (or weighted by unigrams).\n",
    "                # For efficiency/simplicity here: break or pick random.\n",
    "                break \n",
    "\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            sentence.append(next_word)\n",
    "            current_word = next_word\n",
    "            \n",
    "        return \" \".join(sentence[1:])\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        \"\"\"\n",
    "        Completes a given text prompt.\n",
    "        \"\"\"\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        \n",
    "        if not tokens:\n",
    "            current_word = \"<s>\"\n",
    "        else:\n",
    "            current_word = tokens[-1]\n",
    "            \n",
    "        # Warning: If current_word is not in self.vocab, generation will stop immediately\n",
    "        # because bigram_counts[current_word] will be empty.\n",
    "        \n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "            \n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            \n",
    "            if not possible_next:\n",
    "                break\n",
    "                \n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            generated_tokens.append(next_word)\n",
    "            current_word = next_word\n",
    "            \n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        \"\"\"\n",
    "        Completes a given text prompt using the trained model.\n",
    "        \"\"\"\n",
    "        # Preprocess the prompt to get the last token\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        \n",
    "        if not tokens:\n",
    "            current_word = \"<s>\"\n",
    "        else:\n",
    "            current_word = tokens[-1]\n",
    "            \n",
    "        # Handle OOV for the seed word\n",
    "        if current_word not in self.vocab:\n",
    "            # Optionally print a warning or fallback\n",
    "            current_word = \"<UNK>\"\n",
    "            \n",
    "        # Generate continuation\n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "            \n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            \n",
    "            if not possible_next:\n",
    "                # If we hit a dead end (should be rare with smoothing context, but possible if UNK), replace\n",
    "                current_word = \"<UNK>\"\n",
    "                possible_next = self.bigram_counts[current_word]\n",
    "\n",
    "            if not possible_next:\n",
    "                break\n",
    "                \n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            generated_tokens.append(next_word)\n",
    "            current_word = next_word\n",
    "            \n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n",
    "class SimpleBigramLanguageModel:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        print(\"Training Simple Bigram on full vocabulary...\")\n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "                self.unigram_counts[word] += 1\n",
    "            \n",
    "            for i in range(len(sentence) - 1):\n",
    "                w_curr = sentence[i]\n",
    "                w_next = sentence[i+1]\n",
    "                self.bigram_counts[w_curr][w_next] += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f\"Simple Bigram Training complete. Vocab size: {self.vocab_size}\")\n",
    "\n",
    "    def get_probability(self, prev_word, word):\n",
    "        bigram_count = self.bigram_counts[prev_word][word]\n",
    "        unigram_count_prev = self.unigram_counts[prev_word]\n",
    "        \n",
    "        num = bigram_count + self.alpha\n",
    "        den = unigram_count_prev + (self.alpha * self.vocab_size)\n",
    "        return num / den\n",
    "\n",
    "    def calculate_perplexity(self, test_corpus):\n",
    "        log_prob_sum = 0\n",
    "        N = 0\n",
    "        \n",
    "        for sentence in test_corpus:\n",
    "            for i in range(len(sentence) - 1):\n",
    "                w_curr = sentence[i]\n",
    "                w_next = sentence[i+1]\n",
    "                prob = self.get_probability(w_curr, w_next)\n",
    "                log_prob_sum += math.log2(prob)\n",
    "                N += 1\n",
    "        \n",
    "        if N == 0: return float('inf')\n",
    "        \n",
    "        avg_log_prob = -log_prob_sum / N\n",
    "        perplexity = 2 ** avg_log_prob\n",
    "        return perplexity\n",
    "\n",
    "    def generate_sentence(self, max_length=20):\n",
    "        current_word = \"<s>\"\n",
    "        sentence = [current_word]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            if not possible_next:\n",
    "                break \n",
    "\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            sentence.append(next_word)\n",
    "            current_word = next_word\n",
    "        return \" \".join(sentence[1:])\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        if not tokens:\n",
    "            current_word = \"<s>\"\n",
    "        else:\n",
    "            current_word = tokens[-1]\n",
    "        if current_word not in self.vocab:\n",
    "            current_word = \"<UNK>\"\n",
    "            \n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            if not possible_next:\n",
    "                current_word = \"<UNK>\"\n",
    "                possible_next = self.bigram_counts[current_word]\n",
    "            if not possible_next:\n",
    "                break\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            generated_tokens.append(next_word)\n",
    "            current_word = next_word\n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Trigram Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramLanguageModel:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the Trigram Model.\n",
    "        \n",
    "        Args:\n",
    "            alpha (float): The smoothing parameter for Laplace smoothing. \n",
    "                           Default is 0.01.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        # trigram_counts: count of (w1, w2, w3) aka given w1, w2, what is w3?\n",
    "        # Structure: dict[(w1, w2)] -> dict[w3] -> count\n",
    "        self.trigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        # bigram_counts: count of (w1, w2) as a history.\n",
    "        # Structure: dict[(w1, w2)] -> count\n",
    "        self.bigram_counts = defaultdict(int)\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        \n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        self.total_trigrams = 0\n",
    "        self.total_unigrams = 0\n",
    "        \n",
    "        # Interpolation weights\n",
    "        self.lambda1 = 0.1 # Unigram\n",
    "        self.lambda2 = 0.3 # Bigram\n",
    "        self.lambda3 = 0.6 # Trigram\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the model on a corpus of tokenized sentences.\n",
    "        Uses the full vocabulary (no <UNK> thresholding).\n",
    "        \"\"\"\n",
    "        print(\"Training model on full vocabulary...\")\n",
    "        for sentence in corpus:\n",
    "            # Update vocabulary and unigram counts\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "                self.unigram_counts[word] += 1\n",
    "                self.total_unigrams += 1\n",
    "            \n",
    "            # Update bigram counts (for backoff)\n",
    "            for i in range(len(sentence) - 1):\n",
    "                self.bigram_counts[(sentence[i], sentence[i+1])] += 1\n",
    "\n",
    "            # Update trigram counts\n",
    "            # Sentence is expected to be padded like ['<s>', '<s>', 'w1', ..., 'wn', '</s>']\n",
    "            for i in range(len(sentence) - 2):\n",
    "                w_1 = sentence[i]\n",
    "                w_2 = sentence[i+1]\n",
    "                w_3 = sentence[i+2]\n",
    "                \n",
    "                self.trigram_counts[(w_1, w_2)][w_3] += 1\n",
    "                self.total_trigrams += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f\"Training complete. Vocab size: {self.vocab_size}\")\n",
    "\n",
    "    def get_probability(self, w_1, w_2, w_3):\n",
    "        \"\"\"\n",
    "        Calculates the interpolated probability P(w_3 | w_1, w_2).\n",
    "        P = L3 * P(w3|w1,w2) + L2 * P(w3|w2) + L1 * P(w3)\n",
    "        \"\"\"\n",
    "        # 1. Trigram Probability\n",
    "        trigram_count = self.trigram_counts[(w_1, w_2)][w_3]\n",
    "        bigram_context_count = self.bigram_counts[(w_1, w_2)]\n",
    "        \n",
    "        p_tri_num = trigram_count + self.alpha\n",
    "        p_tri_den = bigram_context_count + (self.alpha * self.vocab_size)\n",
    "        p_tri = p_tri_num / p_tri_den\n",
    "        \n",
    "        # 2. Bigram Probability (Backoff)\n",
    "        bigram_count = self.bigram_counts[(w_2, w_3)]\n",
    "        unigram_context_count = self.unigram_counts[w_2]\n",
    "        \n",
    "        p_bi_num = bigram_count + self.alpha\n",
    "        p_bi_den = unigram_context_count + (self.alpha * self.vocab_size)\n",
    "        p_bi = p_bi_num / p_bi_den\n",
    "        \n",
    "        # 3. Unigram Probability\n",
    "        unigram_count = self.unigram_counts[w_3]\n",
    "        p_uni_num = unigram_count + self.alpha\n",
    "        p_uni_den = self.total_unigrams + (self.alpha * self.vocab_size)\n",
    "        p_uni = p_uni_num / p_uni_den\n",
    "        \n",
    "        return (self.lambda3 * p_tri) + (self.lambda2 * p_bi) + (self.lambda1 * p_uni)\n",
    "\n",
    "    def calculate_perplexity(self, test_corpus):\n",
    "        \"\"\"\n",
    "        Calculates the perplexity of the model on a test corpus.\n",
    "        \"\"\"\n",
    "        log_prob_sum = 0\n",
    "        N = 0\n",
    "        \n",
    "        for sentence in test_corpus:\n",
    "            for i in range(len(sentence) - 2):\n",
    "                w_1 = sentence[i]\n",
    "                w_2 = sentence[i+1]\n",
    "                w_3 = sentence[i+2]\n",
    "                \n",
    "                prob = self.get_probability(w_1, w_2, w_3)\n",
    "                \n",
    "                log_prob_sum += math.log2(prob)\n",
    "                N += 1\n",
    "        \n",
    "        if N == 0: return float('inf')\n",
    "        \n",
    "        avg_log_prob = -log_prob_sum / N\n",
    "        perplexity = 2 ** avg_log_prob\n",
    "        return perplexity\n",
    "\n",
    "    def generate_sentence(self, max_length=20):\n",
    "        \"\"\"\n",
    "        Generates a random sentence.\n",
    "        \"\"\"\n",
    "        # Start with two padding tokens\n",
    "        current_w1 = \"<s>\"\n",
    "        current_w2 = \"<s>\"\n",
    "        sentence = [current_w1, current_w2]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # If we generated the end token, stop\n",
    "            if current_w2 == \"</s>\":\n",
    "                break\n",
    "                \n",
    "            possible_next = self.trigram_counts[(current_w1, current_w2)]\n",
    "            \n",
    "            if not possible_next:\n",
    "                # If unknown history, we can't progress. \n",
    "                break \n",
    "\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            sentence.append(next_word)\n",
    "            current_w1 = current_w2\n",
    "            current_w2 = next_word\n",
    "            \n",
    "        # Return joined sentence, removing start tokens\n",
    "        # Typically we don't show <s> <s>\n",
    "        # The list has ['<s>', '<s>', 'word1', ... '</s>' maybe]\n",
    "        # We can strip the first two <s>\n",
    "        return \" \".join(sentence[2:])\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        \"\"\"\n",
    "        Completes a given text prompt using the trained model.\n",
    "        \"\"\"\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        \n",
    "        # Determine context words (need 2)\n",
    "        if len(tokens) >= 2:\n",
    "            current_w1 = tokens[-2]\n",
    "            current_w2 = tokens[-1]\n",
    "        elif len(tokens) == 1:\n",
    "            current_w1 = \"<s>\"\n",
    "            current_w2 = tokens[-1]\n",
    "        else:\n",
    "            current_w1 = \"<s>\"\n",
    "            current_w2 = \"<s>\"\n",
    "            \n",
    "        # Handle OOV - simplistic approach, similar to bigram fallbacks could be added, \n",
    "        # but here we rely on smoothing or break if empty.\n",
    "        \n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_w2 == \"</s>\":\n",
    "                break\n",
    "            \n",
    "            possible_next = self.trigram_counts[(current_w1, current_w2)]\n",
    "            \n",
    "            if not possible_next:\n",
    "                # If we dead end, we could maybe try fallback to bigram?\n",
    "                # But for strict trigram implementation request:\n",
    "                break\n",
    "                \n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            generated_tokens.append(next_word)\n",
    "            current_w1 = current_w2\n",
    "            current_w2 = next_word\n",
    "            \n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n",
    "class SimpleTrigramLanguageModel:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.trigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.bigram_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        print(\"Training Simple Trigram on full vocabulary...\")\n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "            \n",
    "            for i in range(len(sentence) - 1):\n",
    "                self.bigram_counts[(sentence[i], sentence[i+1])] += 1\n",
    "\n",
    "            for i in range(len(sentence) - 2):\n",
    "                w_1 = sentence[i]\n",
    "                w_2 = sentence[i+1]\n",
    "                w_3 = sentence[i+2]\n",
    "                self.trigram_counts[(w_1, w_2)][w_3] += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f\"Simple Trigram Training complete. Vocab size: {self.vocab_size}\")\n",
    "\n",
    "    def get_probability(self, w_1, w_2, w_3):\n",
    "        trigram_count = self.trigram_counts[(w_1, w_2)][w_3]\n",
    "        bigram_context_count = self.bigram_counts[(w_1, w_2)]\n",
    "        \n",
    "        num = trigram_count + self.alpha\n",
    "        den = bigram_context_count + (self.alpha * self.vocab_size)\n",
    "        return num / den\n",
    "\n",
    "    def calculate_perplexity(self, test_corpus):\n",
    "        log_prob_sum = 0\n",
    "        N = 0\n",
    "        \n",
    "        for sentence in test_corpus:\n",
    "            for i in range(len(sentence) - 2):\n",
    "                w_1 = sentence[i]\n",
    "                w_2 = sentence[i+1]\n",
    "                w_3 = sentence[i+2]\n",
    "                prob = self.get_probability(w_1, w_2, w_3)\n",
    "                log_prob_sum += math.log2(prob)\n",
    "                N += 1\n",
    "        \n",
    "        if N == 0: return float('inf')\n",
    "        \n",
    "        avg_log_prob = -log_prob_sum / N\n",
    "        perplexity = 2 ** avg_log_prob\n",
    "        return perplexity\n",
    "\n",
    "    def generate_sentence(self, max_length=20):\n",
    "        current_w1 = \"<s>\"\n",
    "        current_w2 = \"<s>\"\n",
    "        sentence = [current_w1, current_w2]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            if current_w2 == \"</s>\":\n",
    "                break\n",
    "            possible_next = self.trigram_counts[(current_w1, current_w2)]\n",
    "            if not possible_next:\n",
    "                # If unknown history, break \n",
    "                break \n",
    "\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            sentence.append(next_word)\n",
    "            current_w1 = current_w2\n",
    "            current_w2 = next_word\n",
    "            \n",
    "        return \" \".join(sentence[2:])\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        if len(tokens) >= 2:\n",
    "            current_w1 = tokens[-2]\n",
    "            current_w2 = tokens[-1]\n",
    "        elif len(tokens) == 1:\n",
    "            current_w1 = \"<s>\"\n",
    "            current_w2 = tokens[-1]\n",
    "        else:\n",
    "            current_w1 = \"<s>\"\n",
    "            current_w2 = \"<s>\"\n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_w2 == \"</s>\":\n",
    "                break\n",
    "            possible_next = self.trigram_counts[(current_w1, current_w2)]\n",
    "            if not possible_next:\n",
    "                break\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            generated_tokens.append(next_word)\n",
    "            current_w1 = current_w2\n",
    "            current_w2 = next_word\n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Neural Model (LSTM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx:\n",
    "            self.token_to_idx = token_to_idx\n",
    "        else:\n",
    "            self.token_to_idx = {\"<PAD>\": 0, \"<UNK>\": 1, \"<s>\": 2, \"</s>\": 3}\n",
    "        self.idx_to_token = {v: k for k, v in self.token_to_idx.items()}\n",
    "        \n",
    "    def build_vocab(self, sentences, min_freq=2):\n",
    "        print(\"Building vocabulary...\")\n",
    "        all_tokens = [token for sent in sentences for token in sent]\n",
    "        counts = Counter(all_tokens)\n",
    "        \n",
    "        for token, count in counts.items():\n",
    "            if count >= min_freq and token not in self.token_to_idx:\n",
    "                self.token_to_idx[token] = len(self.token_to_idx)\n",
    "                \n",
    "        self.idx_to_token = {v: k for k, v in self.token_to_idx.items()}\n",
    "        print(f\"Vocabulary size: {len(self.token_to_idx)}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "    \n",
    "    def stoi(self, token):\n",
    "        return self.token_to_idx.get(token, self.token_to_idx[\"<UNK>\"])\n",
    "        \n",
    "    def itos(self, idx):\n",
    "        return self.idx_to_token.get(idx, \"<UNK>\")\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, sentences, vocab):\n",
    "        self.sentences = sentences\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_sent = self.sentences[idx]\n",
    "        # Numericalize\n",
    "        indexed = [self.vocab.stoi(t) for t in tokenized_sent]\n",
    "        return torch.tensor(indexed, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable length sentences via padding.\n",
    "    \"\"\"\n",
    "    # batch is a list of tensors\n",
    "    # Sort by length (descending) for pack_padded_sequence\n",
    "    batch.sort(key=lambda x: len(x), reverse=True)\n",
    "    \n",
    "    # Separate source and target\n",
    "    # Source: <s> w1 w2 ... wn\n",
    "    # Target: w1 w2 ... wn </s>\n",
    "    # Actually, our sentences in 'sentences' list usually have <s> and </s> already.\n",
    "    # So we just take :-1 as input and 1: as target.\n",
    "    \n",
    "    inputs = [item[:-1] for item in batch]\n",
    "    targets = [item[1:] for item in batch]\n",
    "    \n",
    "    lengths = torch.tensor([len(x) for x in inputs], dtype=torch.long)\n",
    "    \n",
    "    # Pad sequences\n",
    "    # padding_value=0 is <PAD>\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return inputs_padded, targets_padded, lengths\n",
    "\n",
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(NeuralLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, lengths=None, hidden=None):\n",
    "        # x: (batch, seq_len)\n",
    "        embed = self.embedding(x) # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        if lengths is not None:\n",
    "            # Pack\n",
    "            packed_embed = pack_padded_sequence(embed, lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
    "            packed_out, hidden = self.lstm(packed_embed, hidden)\n",
    "            # Unpack\n",
    "            output, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        else:\n",
    "            # No packing (e.g. inference)\n",
    "            output, hidden = self.lstm(embed, hidden)\n",
    "            \n",
    "        # output: (batch, seq_len, hidden_dim) (padded where needed)\n",
    "        \n",
    "        logits = self.fc(output) # (batch, seq_len, vocab_size)\n",
    "        return logits, hidden\n",
    "\n",
    "def generate_text(model, vocab, start_prompt=\"The movie\", max_len=20, device='cpu', temperature=1.0):\n",
    "    model.eval()\n",
    "    preprocessor = TextPreprocessor(lowercase=True)\n",
    "    tokens = preprocessor.process_text(start_prompt).split()\n",
    "    \n",
    "    current_idx = [vocab.stoi(t) for t in tokens]\n",
    "    # Add start token if not present logic? \n",
    "    # The model trained on <s>... so prompt should ideally start with something logical.\n",
    "    # If we feed \"The movie\", it's mid-sentence-ish.\n",
    "    \n",
    "    input_seq = torch.tensor(current_idx, dtype=torch.long).unsqueeze(0).to(device) # (1, seq_len)\n",
    "    \n",
    "    generated = list(tokens)\n",
    "    \n",
    "    hidden = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            logits, hidden = model(input_seq, hidden=hidden)\n",
    "            \n",
    "            # Get last time step\n",
    "            last_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Apply temperature\n",
    "            if temperature != 1.0:\n",
    "                last_logits = last_logits / temperature\n",
    "                \n",
    "            probs = torch.softmax(last_logits, dim=0)\n",
    "            \n",
    "            # Sample\n",
    "            next_token_idx = torch.multinomial(probs, 1).item()\n",
    "            next_token = vocab.itos(next_token_idx)\n",
    "            \n",
    "            if next_token == \"</s>\":\n",
    "                break\n",
    "                \n",
    "            generated.append(next_token)\n",
    "            \n",
    "            # Next input is the single token we just generated (feeding back one by one)\n",
    "            # Or we could feed the whole sequence, but feeding 1 is efficient IF we keep hidden state.\n",
    "            input_seq = torch.tensor([[next_token_idx]], dtype=torch.long).to(device)\n",
    "            \n",
    "    return \" \".join(generated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison: Bigram vs Trigram\n",
    "We compare the perplexity and generation quality of the statistical models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bigram...\n",
      "Training model on full vocabulary...\n",
      "Training complete. Vocab size: 43007\n",
      "Bigram Trained in 1.5050s\n",
      "Training Simple Bigram...\n",
      "Training Simple Bigram on full vocabulary...\n",
      "Simple Bigram Training complete. Vocab size: 43007\n",
      "Simple Bigram Trained in 1.1786s\n",
      "Training Trigram...\n",
      "Training model on full vocabulary...\n",
      "Training complete. Vocab size: 43007\n",
      "Trigram Trained in 4.5728s\n",
      "Training Simple Trigram...\n",
      "Training Simple Trigram on full vocabulary...\n",
      "Simple Trigram Training complete. Vocab size: 43007\n",
      "Simple Trigram Trained in 3.3734s\n",
      "\n",
      "Bigram Perplexity: 441.69\n",
      "Simple Bigram Perplexity: 647.06\n",
      "Trigram Perplexity: 614.51\n",
      "Simple Trigram Perplexity: 4621.62\n",
      "\n",
      "--- Generation Comparison (Unprompted) ---\n",
      "Bigram:\n",
      "  1. i say at the tv movie it is a significant number one of time but what wendy that he wanted\n",
      "  2. this is very clear in the funny near the cute she is trying so we thought the earlier against the\n",
      "Simple Bigram:\n",
      "  1. mann must a folklore but that cheered by contrast everything that the new era where melville s fun eurotrash movie\n",
      "  2. this is so if you are given some way you are excessive heinous and cowardly it ha had to see\n",
      "\n",
      "Trigram:\n",
      "  1. john cassavetes is running a marathon with little more post production bad special effect that were painted over which he\n",
      "  2. what is terribly obvious of how low can your acting career go figure and it wa not a fine example\n",
      "Simple Trigram:\n",
      "  1. ardh satya s premise is clearly influenced by a wonderful film wonderfully and finally forgiving all the acting leaf a\n",
      "  2. i am not a predictable denouement and crashing into a snow covered field of scotland yard and even the fact\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize\n",
    "bigram_model = BigramLanguageModel(alpha=0.01)\n",
    "simple_bigram_model = SimpleBigramLanguageModel(alpha=0.01)\n",
    "trigram_model = TrigramLanguageModel(alpha=0.01)\n",
    "simple_trigram_model = SimpleTrigramLanguageModel(alpha=0.01)\n",
    "\n",
    "# Train Bigram\n",
    "print(\"Training Bigram...\")\n",
    "start = time.time()\n",
    "train_bi_adapted = [s[1:] for s in train_ngram] \n",
    "test_bi_adapted = [s[1:] for s in test_ngram]\n",
    "bigram_model.train(train_bi_adapted)\n",
    "bigram_time = time.time() - start\n",
    "print(f\"Bigram Trained in {bigram_time:.4f}s\")\n",
    "\n",
    "# Train Simple Bigram\n",
    "print(\"Training Simple Bigram...\")\n",
    "start = time.time()\n",
    "simple_bigram_model.train(train_bi_adapted)\n",
    "simple_bigram_time = time.time() - start\n",
    "print(f\"Simple Bigram Trained in {simple_bigram_time:.4f}s\")\n",
    "\n",
    "# Train Trigram\n",
    "print(\"Training Trigram...\")\n",
    "start = time.time()\n",
    "trigram_model.train(train_ngram)\n",
    "trigram_time = time.time() - start\n",
    "print(f\"Trigram Trained in {trigram_time:.4f}s\")\n",
    "\n",
    "# Train Simple Trigram\n",
    "print(\"Training Simple Trigram...\")\n",
    "start = time.time()\n",
    "simple_trigram_model.train(train_ngram)\n",
    "simple_trigram_time = time.time() - start\n",
    "print(f\"Simple Trigram Trained in {simple_trigram_time:.4f}s\")\n",
    "\n",
    "# Perplexity\n",
    "pp_bi = bigram_model.calculate_perplexity(test_bi_adapted)\n",
    "pp_simple_bi = simple_bigram_model.calculate_perplexity(test_bi_adapted)\n",
    "pp_tri = trigram_model.calculate_perplexity(test_ngram)\n",
    "pp_simple_tri = simple_trigram_model.calculate_perplexity(test_ngram)\n",
    "\n",
    "print(f\"\\nBigram Perplexity: {pp_bi:.2f}\")\n",
    "print(f\"Simple Bigram Perplexity: {pp_simple_bi:.2f}\")\n",
    "print(f\"Trigram Perplexity: {pp_tri:.2f}\")\n",
    "print(f\"Simple Trigram Perplexity: {pp_simple_tri:.2f}\")\n",
    "\n",
    "# Generation Comparison\n",
    "print(\"\\n--- Generation Comparison (Unprompted) ---\")\n",
    "print(\"Bigram:\")\n",
    "for i in range(2):\n",
    "    print(f\"  {i+1}. {bigram_model.generate_sentence()}\")\n",
    "print(\"Simple Bigram:\")\n",
    "for i in range(2):\n",
    "    print(f\"  {i+1}. {simple_bigram_model.generate_sentence()}\")\n",
    "\n",
    "print(\"\\nTrigram:\")\n",
    "for i in range(2):\n",
    "    print(f\"  {i+1}. {trigram_model.generate_sentence()}\")\n",
    "print(\"Simple Trigram:\")\n",
    "for i in range(2):\n",
    "    print(f\"  {i+1}. {simple_trigram_model.generate_sentence()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Comparison (LSTM vs N-grams)\n",
    "Comparing Logic, Training Time, and Perplexity across all three architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Neural Model on cuda...\n",
      "Building vocabulary...\n",
      "Vocabulary size: 17694\n",
      "Epoch 1: Loss 6.1651\n",
      "Epoch 2: Loss 5.2750\n",
      "Epoch 3: Loss 4.8787\n",
      "Epoch 4: Loss 4.5596\n",
      "Epoch 5: Loss 4.2802\n",
      "Epoch 6: Loss 4.0283\n",
      "Neural Trained in 29.5541s\n",
      "Neural Perplexity: 158.07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train Neural Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training Neural Model on {device}...\")\n",
    "\n",
    "# Config\n",
    "EMBED_DIM = 64\n",
    "HIDDEN_DIM = 512\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_EPOCHS = 6 # Kept low for notebook speed\n",
    "\n",
    "# Setup\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(train_neural, min_freq=2)\n",
    "dataset = IMDBDataset(train_neural, vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model_lstm = NeuralLM(len(vocab), EMBED_DIM, HIDDEN_DIM).to(device)\n",
    "optimizer = optim.Adam(model_lstm.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Training Loop\n",
    "start = time.time()\n",
    "model_lstm.train()\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for inputs, targets, lengths in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model_lstm(inputs, lengths)\n",
    "        loss = criterion(logits.view(-1, len(vocab)), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    navg = epoch_loss / len(dataloader)\n",
    "    loss_history.append(navg)\n",
    "    print(f\"Epoch {epoch+1}: Loss {navg:.4f}\")\n",
    "\n",
    "neural_time = time.time() - start\n",
    "print(f\"Neural Trained in {neural_time:.4f}s\")\n",
    "\n",
    "# Neural Perplexity (Approximate on Test Set)\n",
    "# Note: Exact perplexity for neural nets requires exponentiating the CrossEntropyLoss\n",
    "test_ds = IMDBDataset(test_neural, vocab)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "model_lstm.eval()\n",
    "total_n_loss = 0\n",
    "total_batches = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets, lengths in test_dl:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        logits, _ = model_lstm(inputs, lengths)\n",
    "        # Loss per batch\n",
    "        loss = criterion(logits.view(-1, len(vocab)), targets.view(-1))\n",
    "        total_n_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "avg_test_loss = total_n_loss / total_batches if total_batches > 0 else 999\n",
    "pp_neural = math.exp(avg_test_loss)\n",
    "print(f\"Neural Perplexity: {pp_neural:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Performance Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vogle\\AppData\\Local\\Temp\\ipykernel_24996\\999618731.py:8: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=models, y=times, ax=axes[0], palette=\"viridis\")\n",
      "C:\\Users\\vogle\\AppData\\Local\\Temp\\ipykernel_24996\\999618731.py:15: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=models, y=perplexities, ax=axes[1], palette=\"magma\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "models = ['Bigram', 'Simple Bi', 'Trigram', 'Simple Tri', 'LSTM']\n",
    "times = [bigram_time, simple_bigram_time, trigram_time, simple_trigram_time, neural_time]\n",
    "perplexities = [pp_bi, pp_simple_bi, pp_tri, pp_simple_tri, pp_neural]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time Plot\n",
    "sns.barplot(x=models, y=times, ax=axes[0], palette=\"viridis\")\n",
    "axes[0].set_title(\"Training Time (seconds)\")\n",
    "axes[0].set_ylabel(\"Seconds\")\n",
    "for i, v in enumerate(times):\n",
    "    axes[0].text(i, v + 0.1, f\"{v:.1f}s\", ha='center')\n",
    "\n",
    "# Perplexity Plot\n",
    "sns.barplot(x=models, y=perplexities, ax=axes[1], palette=\"magma\")\n",
    "axes[1].set_title(\"Test Perplexity (Lower is Better)\")\n",
    "axes[1].set_ylabel(\"Perplexity\")\n",
    "for i, v in enumerate(perplexities):\n",
    "    axes[1].text(i, v + 1, f\"{v:.1f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Qualitative Comparison: Text Generation\n",
    "Comparing outputs with and without prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Bigram</th>\n",
       "      <th>Simple Bi</th>\n",
       "      <th>Trigram</th>\n",
       "      <th>Simple Tri</th>\n",
       "      <th>LSTM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unprompted</td>\n",
       "      <td>john milius he look at his highly paid for their original and originality and leave the story could do is</td>\n",
       "      <td>all day it is the long that the famous group of approach doe jeff zimbalist and jonny lee burke who</td>\n",
       "      <td>plot in a truly brilliant foundation to base a hollywood s battle with the grandkids someday when their body explode</td>\n",
       "      <td>an amazing lost piece of junk ha none of the movie seems to be a flop first any play on</td>\n",
       "      <td>the role turns out on scene . not to mention the same movie as dominic , do not expect anything in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The movie was</td>\n",
       "      <td>The movie was simply ingenious idea the rest embedded here is not charm and from the remains sad or entertaining and depression america</td>\n",
       "      <td>The movie was humorous remark is in face that mommy dearest would have seen in comparison musicly or a it seems a wood</td>\n",
       "      <td>The movie was made for those of you knowing what s a closed caption version and i am not surprised that more of</td>\n",
       "      <td>The movie was the first time capture a movie perhaps a satire and silliness &lt;/s&gt;</td>\n",
       "      <td>the movie was actually meant for the film . the movie itself could be funny and brings sharp the music which they were</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought that</td>\n",
       "      <td>I thought that the reign of the sex or jack rhiana griffith melanie griffith appears in his throat of it should ve bet</td>\n",
       "      <td>I thought that sort of the girl having not it did not expect in family s paying for those film never end of</td>\n",
       "      <td>I thought that it based on a business merger to go with m konkana sen and well written and all seemed abstract sciamma</td>\n",
       "      <td>I thought that this movie some of the video store and asked them why wa dana andrew seemed to have caught his dad</td>\n",
       "      <td>i thought that made me want to &lt;UNK&gt; the movie . it truly even love . this film a totally one of my</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Prompt  \\\n",
       "0      Unprompted   \n",
       "1   The movie was   \n",
       "2  I thought that   \n",
       "\n",
       "                                                                                                                                    Bigram  \\\n",
       "0                                john milius he look at his highly paid for their original and originality and leave the story could do is   \n",
       "1  The movie was simply ingenious idea the rest embedded here is not charm and from the remains sad or entertaining and depression america   \n",
       "2                   I thought that the reign of the sex or jack rhiana griffith melanie griffith appears in his throat of it should ve bet   \n",
       "\n",
       "                                                                                                                Simple Bi  \\\n",
       "0                     all day it is the long that the famous group of approach doe jeff zimbalist and jonny lee burke who   \n",
       "1  The movie was humorous remark is in face that mommy dearest would have seen in comparison musicly or a it seems a wood   \n",
       "2             I thought that sort of the girl having not it did not expect in family s paying for those film never end of   \n",
       "\n",
       "                                                                                                                  Trigram  \\\n",
       "0    plot in a truly brilliant foundation to base a hollywood s battle with the grandkids someday when their body explode   \n",
       "1         The movie was made for those of you knowing what s a closed caption version and i am not surprised that more of   \n",
       "2  I thought that it based on a business merger to go with m konkana sen and well written and all seemed abstract sciamma   \n",
       "\n",
       "                                                                                                          Simple Tri  \\\n",
       "0                            an amazing lost piece of junk ha none of the movie seems to be a flop first any play on   \n",
       "1                                   The movie was the first time capture a movie perhaps a satire and silliness </s>   \n",
       "2  I thought that this movie some of the video store and asked them why wa dana andrew seemed to have caught his dad   \n",
       "\n",
       "                                                                                                                     LSTM  \n",
       "0                      the role turns out on scene . not to mention the same movie as dominic , do not expect anything in  \n",
       "1  the movie was actually meant for the film . the movie itself could be funny and brings sharp the music which they were  \n",
       "2                     i thought that made me want to <UNK> the movie . it truly even love . this film a totally one of my  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "prompts = [None, \"The movie was\", \"I thought that\"]\n",
    "\n",
    "def get_continuation(model_type, prompt):\n",
    "    if prompt is None:\n",
    "        if model_type == 'LSTM':\n",
    "            return generate_text(model_lstm, vocab, \"The\", device=device)\n",
    "        elif model_type == 'Bigram':\n",
    "            return bigram_model.generate_sentence()\n",
    "        elif model_type == 'Simple Bi':\n",
    "            return simple_bigram_model.generate_sentence()\n",
    "        elif model_type == 'Trigram':\n",
    "            return trigram_model.generate_sentence()\n",
    "        elif model_type == 'Simple Tri':\n",
    "            return simple_trigram_model.generate_sentence()\n",
    "    else:\n",
    "        # Prompted\n",
    "        if model_type == 'LSTM':\n",
    "            return generate_text(model_lstm, vocab, prompt, device=device)\n",
    "        elif model_type == 'Bigram':\n",
    "            return bigram_model.autocomplete(prompt, ngram_prep)\n",
    "        elif model_type == 'Simple Bi':\n",
    "            return simple_bigram_model.autocomplete(prompt, ngram_prep)\n",
    "        elif model_type == 'Trigram':\n",
    "            return trigram_model.autocomplete(prompt, ngram_prep)\n",
    "        elif model_type == 'Simple Tri':\n",
    "            return simple_trigram_model.autocomplete(prompt, ngram_prep)\n",
    "\n",
    "results = []\n",
    "for p in prompts:\n",
    "    p_label = p if p else \"Unprompted\"\n",
    "    row = {\"Prompt\": p_label}\n",
    "    row[\"Bigram\"] = get_continuation('Bigram', p)\n",
    "    row[\"Simple Bi\"] = get_continuation('Simple Bi', p)\n",
    "    row[\"Trigram\"] = get_continuation('Trigram', p)\n",
    "    row[\"Simple Tri\"] = get_continuation('Simple Tri', p)\n",
    "    row[\"LSTM\"] = get_continuation('LSTM', p)\n",
    "    results.append(row)\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(res_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
