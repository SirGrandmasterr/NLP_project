{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project: Language Modeling Comparison\n",
    "This notebook implements and compares three language models: **Bigram**, **Trigram**, and **Neural LSTM**.\n",
    "It includes a unified preprocessing pipeline, training comparisons, and generation quality checks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import html\n",
    "import string\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Setup Visuals\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Download resources if checks fail (quietly)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    \n",
    "print(\"Libraries loaded.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessor Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    A robust and professional preprocessing pipeline for NLP tasks.\n",
    "    Designed to handle IMDB movie reviews for Classical, Neural, and Transformer models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 remove_html: bool = True,\n",
    "                 lowercase: bool = True,\n",
    "                 remove_punctuation: bool = False,\n",
    "                 remove_stopwords: bool = False,\n",
    "                 lemmatize: bool = False,\n",
    "                 expand_contractions: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline with specific configuration flags.\n",
    "        \n",
    "        Args:\n",
    "            remove_html (bool): Strip HTML tags (e.g., <br />). Default True.\n",
    "            lowercase (bool): Convert text to lowercase. Default True.\n",
    "            remove_punctuation (bool): Remove punctuation characters.\n",
    "            remove_stopwords (bool): Remove standard English stopwords.\n",
    "            lemmatize (bool): Apply WordNet lemmatization.\n",
    "            expand_contractions (bool): Expand \"isn't\" to \"is not\".\n",
    "        \"\"\"\n",
    "        self.remove_html = remove_html\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        self.expand_contractions = expand_contractions\n",
    "\n",
    "        # Pre-load resources to optimize runtime\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stop_words.remove(\"not\")\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Simple contraction map for expansion\n",
    "        self.contractions_dict = {\n",
    "            \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
    "            \"haven't\": \"have not\", \"hasn't\": \"has not\", \"hadn't\": \"had not\", \"won't\": \"will not\",\n",
    "            \"wouldn't\": \"would not\", \"don't\": \"do not\", \"doesn't\": \"does not\", \"didn't\": \"did not\",\n",
    "            \"can't\": \"cannot\", \"couldn't\": \"could not\", \"shouldn't\": \"should not\", \"mightn't\": \"might not\",\n",
    "            \"mustn't\": \"must not\", \"i'm\": \"i am\", \"you're\": \"you are\", \"he's\": \"he is\", \"she's\": \"she is\",\n",
    "            \"it's\": \"it is\", \"we're\": \"we are\", \"they're\": \"they are\", \"i've\": \"i have\", \"you've\": \"you have\",\n",
    "            \"we've\": \"we have\", \"they've\": \"they have\", \"i'll\": \"i will\", \"you'll\": \"you will\",\n",
    "            \"he'll\": \"he will\", \"she'll\": \"she will\", \"we'll\": \"we will\", \"they'll\": \"they will\"\n",
    "        }\n",
    "        self.contractions_re = re.compile('(%s)' % '|'.join(self.contractions_dict.keys()))\n",
    "\n",
    "    def _clean_html(self, text: str) -> str:\n",
    "        \"\"\"Removes HTML tags and unescapes HTML entities.\"\"\"\n",
    "        text = html.unescape(text)\n",
    "        # Regex for HTML tags\n",
    "        clean = re.compile('<.*?>')\n",
    "        return re.sub(clean, ' ', text)\n",
    "\n",
    "    def _expand_contractions(self, text: str) -> str:\n",
    "        \"\"\"Expands common English contractions.\"\"\"\n",
    "        def replace(match):\n",
    "            return self.contractions_dict[match.group(0)]\n",
    "        return self.contractions_re.sub(replace, text)\n",
    "\n",
    "    def _remove_punct(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes punctuation by replacing it with spaces.\n",
    "        This prevents 'word,word' from becoming 'wordword'.\n",
    "        \"\"\"\n",
    "        # Replace punctuation with a space\n",
    "        return re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)\n",
    "\n",
    "    def process_text(self, text: str) -> Union[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Main execution method. Applies enabled steps in the logical order.\n",
    "        \n",
    "        Returns:\n",
    "            str: If the final output is a joined string.\n",
    "            List[str]: If the processing flow ends in tokenization without re-joining.\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or not text:\n",
    "            return \"\"\n",
    "\n",
    "        # 1. Cleaning\n",
    "        if self.remove_html:\n",
    "            text = self._clean_html(text)\n",
    "        \n",
    "        # 2. Lowercasing\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "            \n",
    "        # 3. Expansion (must be after lowercasing for simple dict matching)\n",
    "        if self.expand_contractions:\n",
    "            text = self._expand_contractions(text)\n",
    "\n",
    "        # 4. Punctuation Removal\n",
    "        if self.remove_punctuation:\n",
    "            text = self._remove_punct(text)\n",
    "\n",
    "        # 5. Tokenization\n",
    "        # We always tokenize to perform word-level operations (stopword/lemma)\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # 6. Stopword Removal\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [w for w in tokens if w not in self.stop_words]\n",
    "\n",
    "        # 7. Lemmatization\n",
    "        if self.lemmatize:\n",
    "            tokens = [self.lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "        \n",
    "        return \" \".join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Unified Preprocessing\n",
    "We pre-process the dataset once for all models to ensure fair comparison and save time.\n",
    "- **N-gram Corpus**: Lemmatized, punctuation kept (structural), contractions expanded.\n",
    "- **Neural Corpus**: Lowercase, no lemmatization (learns forms), standard tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 reviews.\n",
      "Preprocessing for N-gram models...\n",
      "Preprocessing for Neural models...\n",
      "Data ready. Train size: 40000, Test size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "try:\n",
    "    df = pd.read_csv('IMDB Dataset.csv')\n",
    "    raw_reviews = df['review'].tolist()\n",
    "    print(f\"Loaded {len(raw_reviews)} reviews.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"IMDB Dataset.csv not found. Using dummy data.\")\n",
    "    raw_reviews = [\"The movie was terrible.\", \"I loved the film. It was great.\", \"The acting was bad.\"] * 100\n",
    "\n",
    "# Configuration\n",
    "SAMPLE_SIZE = len(raw_reviews)  \n",
    "raw_reviews = raw_reviews[:SAMPLE_SIZE]\n",
    "\n",
    "# 1. Pipeline for N-gram Models\n",
    "print(\"Preprocessing for N-gram models...\")\n",
    "ngram_prep = TextPreprocessor(\n",
    "    remove_html=True, \n",
    "    lowercase=True, \n",
    "    remove_punctuation=True, # Keep structure for n-grams\n",
    "    remove_stopwords=False, \n",
    "    lemmatize=True, \n",
    "    expand_contractions=True\n",
    ")\n",
    "\n",
    "ngram_corpus = []\n",
    "for r in raw_reviews:\n",
    "    cleaned = ngram_prep.process_text(r)\n",
    "    # Add start/end tokens. Bigram needs 1 start, Trigram needs 2. \n",
    "    # We will use 2 start tokens '<s>' '<s>' universally, Bigram can just ignore the extra one or we handle it in model.\n",
    "    # To be safe and exact to class definitions, let's store as lists.\n",
    "    tokens = cleaned.split()\n",
    "    # Bigram expects ['<s>', w1...]\n",
    "    # Trigram expects ['<s>', '<s>', w1...]\n",
    "    # We'll store the clean text list and pad per model or creating a generic '<s>' '<s>' ... '</s>'\n",
    "    tokens = ['<s>', '<s>'] + tokens + ['</s>']\n",
    "    ngram_corpus.append(tokens)\n",
    "\n",
    "# 2. Pipeline for Neural Model\n",
    "print(\"Preprocessing for Neural models...\")\n",
    "neural_prep = TextPreprocessor(\n",
    "    remove_html=True, \n",
    "    lowercase=True, \n",
    "    remove_punctuation=False,\n",
    "    remove_stopwords=False, \n",
    "    lemmatize=False,\n",
    "    expand_contractions=True\n",
    ")\n",
    "\n",
    "neural_corpus = []\n",
    "MAX_LEN = 100\n",
    "for r in raw_reviews:\n",
    "    cleaned = neural_prep.process_text(r)\n",
    "    tokens = cleaned.split()\n",
    "    # Neural model logic from script: ['<s>'] + tokens + ['</s>']\n",
    "    tokens = ['<s>'] + tokens[:MAX_LEN] + ['</s>']\n",
    "    neural_corpus.append(tokens)\n",
    "\n",
    "# Split (Sync split indices)\n",
    "split_idx = int(len(raw_reviews) * 0.8)\n",
    "\n",
    "train_ngram = ngram_corpus[:split_idx]\n",
    "test_ngram  = ngram_corpus[split_idx:]\n",
    "\n",
    "train_neural = neural_corpus[:split_idx]\n",
    "test_neural  = neural_corpus[split_idx:]\n",
    "\n",
    "print(f\"Data ready. Train size: {len(train_ngram)}, Test size: {len(test_ngram)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Definitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Bigram Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the Bigram Model.\n",
    "        \n",
    "        Args:\n",
    "            alpha (float): The smoothing parameter for Laplace smoothing. \n",
    "                           Default is 0.01.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        self.total_bigrams = 0\n",
    "        self.total_unigrams = 0\n",
    "        \n",
    "        self.lambda1 = 0.3 # Unigram\n",
    "        self.lambda2 = 0.7 # Bigram\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the model on a corpus of tokenized sentences.\n",
    "        Uses the full vocabulary (no <UNK> thresholding).\n",
    "        \"\"\"\n",
    "        print(\"Training model on full vocabulary...\")\n",
    "        for sentence in corpus:\n",
    "            # Update vocabulary and unigram counts\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "                self.unigram_counts[word] += 1\n",
    "                self.total_unigrams += 1\n",
    "            \n",
    "            # Update bigram counts\n",
    "            for i in range(len(sentence) - 1):\n",
    "                w_curr = sentence[i]\n",
    "                w_next = sentence[i+1]\n",
    "                self.bigram_counts[w_curr][w_next] += 1\n",
    "                self.total_bigrams += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f\"Training complete. Vocab size: {self.vocab_size}\")\n",
    "\n",
    "    def get_probability(self, prev_word, word):\n",
    "        \"\"\"\n",
    "        Calculates the interpolated probability P(word | prev_word).\n",
    "        P = L2 * P(word|prev) + L1 * P(word)\n",
    "        \"\"\"\n",
    "        # 1. Bigram Probability\n",
    "        bigram_count = self.bigram_counts[prev_word][word]\n",
    "        unigram_count_prev = self.unigram_counts[prev_word]\n",
    "        \n",
    "        p_bi_num = bigram_count + self.alpha\n",
    "        p_bi_den = unigram_count_prev + (self.alpha * self.vocab_size)\n",
    "        p_bi = p_bi_num / p_bi_den\n",
    "        \n",
    "        # 2. Unigram Probability\n",
    "        unigram_count_word = self.unigram_counts[word]\n",
    "        p_uni_num = unigram_count_word + self.alpha\n",
    "        p_uni_den = self.total_unigrams + (self.alpha * self.vocab_size)\n",
    "        p_uni = p_uni_num / p_uni_den\n",
    "        \n",
    "        return (self.lambda2 * p_bi) + (self.lambda1 * p_uni)\n",
    "\n",
    "    def calculate_perplexity(self, test_corpus):\n",
    "        \"\"\"\n",
    "        Calculates the perplexity of the model on a test corpus.\n",
    "        \"\"\"\n",
    "        log_prob_sum = 0\n",
    "        N = 0\n",
    "        \n",
    "        for sentence in test_corpus:\n",
    "            for i in range(len(sentence) - 1):\n",
    "                w_curr = sentence[i]\n",
    "                w_next = sentence[i+1]\n",
    "                \n",
    "                # We do not replace with <UNK>. If a word is unknown,\n",
    "                # get_probability handles it via smoothing.\n",
    "                prob = self.get_probability(w_curr, w_next)\n",
    "                \n",
    "                log_prob_sum += math.log2(prob)\n",
    "                N += 1\n",
    "        \n",
    "        if N == 0: return float('inf')\n",
    "        \n",
    "        avg_log_prob = -log_prob_sum / N\n",
    "        perplexity = 2 ** avg_log_prob\n",
    "        return perplexity\n",
    "\n",
    "    def generate_sentence(self, max_length=20):\n",
    "        \"\"\"\n",
    "        Generates a random sentence.\n",
    "        \"\"\"\n",
    "        current_word = \"<s>\"\n",
    "        sentence = [current_word]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "                \n",
    "            # If current_word was never seen in training (e.g. from a user prompt),\n",
    "            # unigram_count is 0. We fallback to uniform distribution or break.\n",
    "            # Here we sample from the whole vocab if unknown, or just observed followers if known.\n",
    "            \n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            \n",
    "            if not possible_next:\n",
    "                # Dead end or unknown word. \n",
    "                # Ideally: Sample uniformly from V (or weighted by unigrams).\n",
    "                # For efficiency/simplicity here: break or pick random.\n",
    "                break \n",
    "\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            sentence.append(next_word)\n",
    "            current_word = next_word\n",
    "            \n",
    "        return \" \".join(sentence[1:])\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        \"\"\"\n",
    "        Completes a given text prompt.\n",
    "        \"\"\"\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        \n",
    "        if not tokens:\n",
    "            current_word = \"<s>\"\n",
    "        else:\n",
    "            current_word = tokens[-1]\n",
    "            \n",
    "        # Warning: If current_word is not in self.vocab, generation will stop immediately\n",
    "        # because bigram_counts[current_word] will be empty.\n",
    "        \n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "            \n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            \n",
    "            if not possible_next:\n",
    "                break\n",
    "                \n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            generated_tokens.append(next_word)\n",
    "            current_word = next_word\n",
    "            \n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        \"\"\"\n",
    "        Completes a given text prompt using the trained model.\n",
    "        \"\"\"\n",
    "        # Preprocess the prompt to get the last token\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        \n",
    "        if not tokens:\n",
    "            current_word = \"<s>\"\n",
    "        else:\n",
    "            current_word = tokens[-1]\n",
    "            \n",
    "        # Handle OOV for the seed word\n",
    "        if current_word not in self.vocab:\n",
    "            # Optionally print a warning or fallback\n",
    "            current_word = \"<UNK>\"\n",
    "            \n",
    "        # Generate continuation\n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "            \n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            \n",
    "            if not possible_next:\n",
    "                # If we hit a dead end (should be rare with smoothing context, but possible if UNK), replace\n",
    "                current_word = \"<UNK>\"\n",
    "                possible_next = self.bigram_counts[current_word]\n",
    "\n",
    "            if not possible_next:\n",
    "                break\n",
    "                \n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            generated_tokens.append(next_word)\n",
    "            current_word = next_word\n",
    "            \n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n",
    "class SimpleBigramLanguageModel:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        print(\"Training Simple Bigram on full vocabulary...\")\n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "                self.unigram_counts[word] += 1\n",
    "            \n",
    "            for i in range(len(sentence) - 1):\n",
    "                w_curr = sentence[i]\n",
    "                w_next = sentence[i+1]\n",
    "                self.bigram_counts[w_curr][w_next] += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f\"Simple Bigram Training complete. Vocab size: {self.vocab_size}\")\n",
    "\n",
    "    def get_probability(self, prev_word, word):\n",
    "        bigram_count = self.bigram_counts[prev_word][word]\n",
    "        unigram_count_prev = self.unigram_counts[prev_word]\n",
    "        \n",
    "        num = bigram_count + self.alpha\n",
    "        den = unigram_count_prev + (self.alpha * self.vocab_size)\n",
    "        return num / den\n",
    "\n",
    "    def calculate_perplexity(self, test_corpus):\n",
    "        log_prob_sum = 0\n",
    "        N = 0\n",
    "        \n",
    "        for sentence in test_corpus:\n",
    "            for i in range(len(sentence) - 1):\n",
    "                w_curr = sentence[i]\n",
    "                w_next = sentence[i+1]\n",
    "                prob = self.get_probability(w_curr, w_next)\n",
    "                log_prob_sum += math.log2(prob)\n",
    "                N += 1\n",
    "        \n",
    "        if N == 0: return float('inf')\n",
    "        \n",
    "        avg_log_prob = -log_prob_sum / N\n",
    "        perplexity = 2 ** avg_log_prob\n",
    "        return perplexity\n",
    "\n",
    "    def generate_sentence(self, max_length=20):\n",
    "        current_word = \"<s>\"\n",
    "        sentence = [current_word]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            if not possible_next:\n",
    "                break \n",
    "\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            sentence.append(next_word)\n",
    "            current_word = next_word\n",
    "        return \" \".join(sentence[1:])\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        if not tokens:\n",
    "            current_word = \"<s>\"\n",
    "        else:\n",
    "            current_word = tokens[-1]\n",
    "        if current_word not in self.vocab:\n",
    "            current_word = \"<UNK>\"\n",
    "            \n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            if not possible_next:\n",
    "                current_word = \"<UNK>\"\n",
    "                possible_next = self.bigram_counts[current_word]\n",
    "            if not possible_next:\n",
    "                break\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            generated_tokens.append(next_word)\n",
    "            current_word = next_word\n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Trigram Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramLanguageModel:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the Trigram Model.\n",
    "        \n",
    "        Args:\n",
    "            alpha (float): The smoothing parameter for Laplace smoothing. \n",
    "                           Default is 0.01.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        # trigram_counts: count of (w1, w2, w3) aka given w1, w2, what is w3?\n",
    "        # Structure: dict[(w1, w2)] -> dict[w3] -> count\n",
    "        self.trigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        # bigram_counts: count of (w1, w2) as a history.\n",
    "        # Structure: dict[(w1, w2)] -> count\n",
    "        self.bigram_counts = defaultdict(int)\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        \n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        self.total_trigrams = 0\n",
    "        self.total_unigrams = 0\n",
    "        \n",
    "        # Interpolation weights\n",
    "        self.lambda1 = 0.1 # Unigram\n",
    "        self.lambda2 = 0.3 # Bigram\n",
    "        self.lambda3 = 0.6 # Trigram\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the model on a corpus of tokenized sentences.\n",
    "        \"\"\"\n",
    "        print(\"Training model on full vocabulary...\")\n",
    "        for sentence in corpus:\n",
    "            # Update vocabulary and unigram counts\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "                self.unigram_counts[word] += 1\n",
    "                self.total_unigrams += 1\n",
    "            \n",
    "            # Update bigram counts (for backoff)\n",
    "            for i in range(len(sentence) - 1):\n",
    "                self.bigram_counts[(sentence[i], sentence[i+1])] += 1\n",
    "\n",
    "            # Update trigram counts\n",
    "            # Sentence is expected to be padded like ['<s>', '<s>', 'w1', ..., 'wn', '</s>']\n",
    "            for i in range(len(sentence) - 2):\n",
    "                w_1 = sentence[i]\n",
    "                w_2 = sentence[i+1]\n",
    "                w_3 = sentence[i+2]\n",
    "                \n",
    "                self.trigram_counts[(w_1, w_2)][w_3] += 1\n",
    "                self.total_trigrams += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f\"Training complete. Vocab size: {self.vocab_size}\")\n",
    "\n",
    "    def get_probability(self, w_1, w_2, w_3):\n",
    "        \"\"\"\n",
    "        Calculates the interpolated probability P(w_3 | w_1, w_2).\n",
    "        P = L3 * P(w3|w1,w2) + L2 * P(w3|w2) + L1 * P(w3)\n",
    "        \"\"\"\n",
    "        # 1. Trigram Probability\n",
    "        trigram_count = self.trigram_counts[(w_1, w_2)][w_3]\n",
    "        bigram_context_count = self.bigram_counts[(w_1, w_2)]\n",
    "        \n",
    "        p_tri_num = trigram_count + self.alpha\n",
    "        p_tri_den = bigram_context_count + (self.alpha * self.vocab_size)\n",
    "        p_tri = p_tri_num / p_tri_den\n",
    "        \n",
    "        # 2. Bigram Probability (Backoff)\n",
    "        bigram_count = self.bigram_counts[(w_2, w_3)]\n",
    "        unigram_context_count = self.unigram_counts[w_2]\n",
    "        \n",
    "        p_bi_num = bigram_count + self.alpha\n",
    "        p_bi_den = unigram_context_count + (self.alpha * self.vocab_size)\n",
    "        p_bi = p_bi_num / p_bi_den\n",
    "        \n",
    "        # 3. Unigram Probability\n",
    "        unigram_count = self.unigram_counts[w_3]\n",
    "        p_uni_num = unigram_count + self.alpha\n",
    "        p_uni_den = self.total_unigrams + (self.alpha * self.vocab_size)\n",
    "        p_uni = p_uni_num / p_uni_den\n",
    "        \n",
    "        return (self.lambda3 * p_tri) + (self.lambda2 * p_bi) + (self.lambda1 * p_uni)\n",
    "\n",
    "    def calculate_perplexity(self, test_corpus):\n",
    "        \"\"\"\n",
    "        Calculates the perplexity of the model on a test corpus.\n",
    "        \"\"\"\n",
    "        log_prob_sum = 0\n",
    "        N = 0\n",
    "        \n",
    "        for sentence in test_corpus:\n",
    "            for i in range(len(sentence) - 2):\n",
    "                w_1 = sentence[i]\n",
    "                w_2 = sentence[i+1]\n",
    "                w_3 = sentence[i+2]\n",
    "                \n",
    "                prob = self.get_probability(w_1, w_2, w_3)\n",
    "                \n",
    "                log_prob_sum += math.log2(prob)\n",
    "                N += 1\n",
    "        \n",
    "        if N == 0: return float('inf')\n",
    "        \n",
    "        avg_log_prob = -log_prob_sum / N\n",
    "        perplexity = 2 ** avg_log_prob\n",
    "        return perplexity\n",
    "\n",
    "    def generate_sentence(self, max_length=20):\n",
    "        \"\"\"\n",
    "        Generates a random sentence.\n",
    "        \"\"\"\n",
    "        # Start with two padding tokens\n",
    "        current_w1 = \"<s>\"\n",
    "        current_w2 = \"<s>\"\n",
    "        sentence = [current_w1, current_w2]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # If we generated the end token, stop\n",
    "            if current_w2 == \"</s>\":\n",
    "                break\n",
    "                \n",
    "            possible_next = self.trigram_counts[(current_w1, current_w2)]\n",
    "            \n",
    "            if not possible_next:\n",
    "                # If unknown history, we can't progress. \n",
    "                break \n",
    "\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            sentence.append(next_word)\n",
    "            current_w1 = current_w2\n",
    "            current_w2 = next_word\n",
    "            \n",
    "        # Return joined sentence, removing start tokens\n",
    "        # Typically we don't show <s> <s>\n",
    "        # The list has ['<s>', '<s>', 'word1', ... '</s>' maybe]\n",
    "        # We can strip the first two <s>\n",
    "        return \" \".join(sentence[2:])\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        \"\"\"\n",
    "        Completes a given text prompt using the trained model.\n",
    "        \"\"\"\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        \n",
    "        # Determine context words (need 2)\n",
    "        if len(tokens) >= 2:\n",
    "            current_w1 = tokens[-2]\n",
    "            current_w2 = tokens[-1]\n",
    "        elif len(tokens) == 1:\n",
    "            current_w1 = \"<s>\"\n",
    "            current_w2 = tokens[-1]\n",
    "        else:\n",
    "            current_w1 = \"<s>\"\n",
    "            current_w2 = \"<s>\"\n",
    "            \n",
    "        # Handle OOV - simplistic approach, similar to bigram fallbacks could be added, \n",
    "        # but here we rely on smoothing or break if empty.\n",
    "        \n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_w2 == \"</s>\":\n",
    "                break\n",
    "            \n",
    "            possible_next = self.trigram_counts[(current_w1, current_w2)]\n",
    "            \n",
    "            if not possible_next:\n",
    "                # If we dead end, we could maybe try fallback to bigram?\n",
    "                # But for strict trigram implementation request:\n",
    "                break\n",
    "                \n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            generated_tokens.append(next_word)\n",
    "            current_w1 = current_w2\n",
    "            current_w2 = next_word\n",
    "            \n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n",
    "class SimpleTrigramLanguageModel:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.trigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.bigram_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        print(\"Training Simple Trigram on full vocabulary...\")\n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "            \n",
    "            for i in range(len(sentence) - 1):\n",
    "                self.bigram_counts[(sentence[i], sentence[i+1])] += 1\n",
    "\n",
    "            for i in range(len(sentence) - 2):\n",
    "                w_1 = sentence[i]\n",
    "                w_2 = sentence[i+1]\n",
    "                w_3 = sentence[i+2]\n",
    "                self.trigram_counts[(w_1, w_2)][w_3] += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f\"Simple Trigram Training complete. Vocab size: {self.vocab_size}\")\n",
    "\n",
    "    def get_probability(self, w_1, w_2, w_3):\n",
    "        trigram_count = self.trigram_counts[(w_1, w_2)][w_3]\n",
    "        bigram_context_count = self.bigram_counts[(w_1, w_2)]\n",
    "        \n",
    "        num = trigram_count + self.alpha\n",
    "        den = bigram_context_count + (self.alpha * self.vocab_size)\n",
    "        return num / den\n",
    "\n",
    "    def calculate_perplexity(self, test_corpus):\n",
    "        log_prob_sum = 0\n",
    "        N = 0\n",
    "        \n",
    "        for sentence in test_corpus:\n",
    "            for i in range(len(sentence) - 2):\n",
    "                w_1 = sentence[i]\n",
    "                w_2 = sentence[i+1]\n",
    "                w_3 = sentence[i+2]\n",
    "                prob = self.get_probability(w_1, w_2, w_3)\n",
    "                log_prob_sum += math.log2(prob)\n",
    "                N += 1\n",
    "        \n",
    "        if N == 0: return float('inf')\n",
    "        \n",
    "        avg_log_prob = -log_prob_sum / N\n",
    "        perplexity = 2 ** avg_log_prob\n",
    "        return perplexity\n",
    "\n",
    "    def generate_sentence(self, max_length=20):\n",
    "        current_w1 = \"<s>\"\n",
    "        current_w2 = \"<s>\"\n",
    "        sentence = [current_w1, current_w2]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            if current_w2 == \"</s>\":\n",
    "                break\n",
    "            possible_next = self.trigram_counts[(current_w1, current_w2)]\n",
    "            if not possible_next:\n",
    "                # If unknown history, break \n",
    "                break \n",
    "\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            sentence.append(next_word)\n",
    "            current_w1 = current_w2\n",
    "            current_w2 = next_word\n",
    "            \n",
    "        return \" \".join(sentence[2:])\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        if len(tokens) >= 2:\n",
    "            current_w1 = tokens[-2]\n",
    "            current_w2 = tokens[-1]\n",
    "        elif len(tokens) == 1:\n",
    "            current_w1 = \"<s>\"\n",
    "            current_w2 = tokens[-1]\n",
    "        else:\n",
    "            current_w1 = \"<s>\"\n",
    "            current_w2 = \"<s>\"\n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_w2 == \"</s>\":\n",
    "                break\n",
    "            possible_next = self.trigram_counts[(current_w1, current_w2)]\n",
    "            if not possible_next:\n",
    "                break\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            generated_tokens.append(next_word)\n",
    "            current_w1 = current_w2\n",
    "            current_w2 = next_word\n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Neural Model (LSTM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx:\n",
    "            self.token_to_idx = token_to_idx\n",
    "        else:\n",
    "            self.token_to_idx = {\"<PAD>\": 0, \"<UNK>\": 1, \"<s>\": 2, \"</s>\": 3}\n",
    "        self.idx_to_token = {v: k for k, v in self.token_to_idx.items()}\n",
    "        \n",
    "    def build_vocab(self, sentences, min_freq=2):\n",
    "        print(\"Building vocabulary...\")\n",
    "        all_tokens = [token for sent in sentences for token in sent]\n",
    "        counts = Counter(all_tokens)\n",
    "        \n",
    "        for token, count in counts.items():\n",
    "            if count >= min_freq and token not in self.token_to_idx:\n",
    "                self.token_to_idx[token] = len(self.token_to_idx)\n",
    "                \n",
    "        self.idx_to_token = {v: k for k, v in self.token_to_idx.items()}\n",
    "        print(f\"Vocabulary size: {len(self.token_to_idx)}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "    \n",
    "    def stoi(self, token):\n",
    "        return self.token_to_idx.get(token, self.token_to_idx[\"<UNK>\"])\n",
    "        \n",
    "    def itos(self, idx):\n",
    "        return self.idx_to_token.get(idx, \"<UNK>\")\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, sentences, vocab):\n",
    "        self.sentences = sentences\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_sent = self.sentences[idx]\n",
    "        # Numericalize\n",
    "        indexed = [self.vocab.stoi(t) for t in tokenized_sent]\n",
    "        return torch.tensor(indexed, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable length sentences via padding.\n",
    "    \"\"\"\n",
    "    # batch is a list of tensors\n",
    "    # Sort by length (descending) for pack_padded_sequence\n",
    "    batch.sort(key=lambda x: len(x), reverse=True)\n",
    "    \n",
    "    # Separate source and target\n",
    "    # Source: <s> w1 w2 ... wn\n",
    "    # Target: w1 w2 ... wn </s>\n",
    "    \n",
    "    inputs = [item[:-1] for item in batch]\n",
    "    targets = [item[1:] for item in batch]\n",
    "    \n",
    "    lengths = torch.tensor([len(x) for x in inputs], dtype=torch.long)\n",
    "    \n",
    "    # Pad sequences\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return inputs_padded, targets_padded, lengths\n",
    "\n",
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(NeuralLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, lengths=None, hidden=None):\n",
    "        # x: (batch, seq_len)\n",
    "        embed = self.embedding(x) # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        if lengths is not None:\n",
    "            # Pack\n",
    "            packed_embed = pack_padded_sequence(embed, lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
    "            packed_out, hidden = self.lstm(packed_embed, hidden)\n",
    "            # Unpack\n",
    "            output, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        else:\n",
    "            # No packing (e.g. inference)\n",
    "            output, hidden = self.lstm(embed, hidden)\n",
    "            \n",
    "        # output: (batch, seq_len, hidden_dim) (padded where needed)\n",
    "        \n",
    "        logits = self.fc(output) # (batch, seq_len, vocab_size)\n",
    "        return logits, hidden\n",
    "\n",
    "def generate_text(model, vocab, start_prompt=\"The movie\", max_len=20, device='cpu', temperature=1.0):\n",
    "    model.eval()\n",
    "    preprocessor = TextPreprocessor(lowercase=True)\n",
    "    tokens = preprocessor.process_text(start_prompt).split()\n",
    "    \n",
    "    current_idx = [vocab.stoi(t) for t in tokens]\n",
    "    # Add start token if not present logic? \n",
    "    # The model trained on <s>... so prompt should ideally start with something logical.\n",
    "    # If we feed \"The movie\", it's mid-sentence-ish.\n",
    "    \n",
    "    input_seq = torch.tensor(current_idx, dtype=torch.long).unsqueeze(0).to(device) # (1, seq_len)\n",
    "    \n",
    "    generated = list(tokens)\n",
    "    \n",
    "    hidden = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            logits, hidden = model(input_seq, hidden=hidden)\n",
    "            \n",
    "            # Get last time step\n",
    "            last_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Apply temperature\n",
    "            if temperature != 1.0:\n",
    "                last_logits = last_logits / temperature\n",
    "                \n",
    "            probs = torch.softmax(last_logits, dim=0)\n",
    "            \n",
    "            # Sample\n",
    "            next_token_idx = torch.multinomial(probs, 1).item()\n",
    "            next_token = vocab.itos(next_token_idx)\n",
    "            \n",
    "            if next_token == \"</s>\":\n",
    "                break\n",
    "                \n",
    "            generated.append(next_token)\n",
    "            \n",
    "            # Next input is the single token we just generated (feeding back one by one)\n",
    "            # Or we could feed the whole sequence, but feeding 1 is efficient IF we keep hidden state.\n",
    "            input_seq = torch.tensor([[next_token_idx]], dtype=torch.long).to(device)\n",
    "            \n",
    "    return \" \".join(generated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison: Bigram vs Trigram\n",
    "We compare the perplexity and generation quality of the statistical models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bigram...\n",
      "Training model on full vocabulary...\n",
      "Training complete. Vocab size: 84589\n",
      "Bigram Trained in 11.1138s\n",
      "Training Simple Bigram...\n",
      "Training Simple Bigram on full vocabulary...\n",
      "Simple Bigram Training complete. Vocab size: 84589\n",
      "Simple Bigram Trained in 8.6424s\n",
      "Training Trigram...\n",
      "Training model on full vocabulary...\n",
      "Training complete. Vocab size: 84589\n",
      "Trigram Trained in 28.7869s\n",
      "Training Simple Trigram...\n",
      "Training Simple Trigram on full vocabulary...\n",
      "Simple Trigram Training complete. Vocab size: 84589\n",
      "Simple Trigram Trained in 23.9890s\n",
      "\n",
      "Bigram Perplexity: 363.91\n",
      "Simple Bigram Perplexity: 450.83\n",
      "Trigram Perplexity: 465.57\n",
      "Simple Trigram Perplexity: 2985.33\n",
      "\n",
      "--- Generation Comparison (Unprompted) ---\n",
      "Bigram:\n",
      "  1. with sharky look impressive avant garde crowd scene and can superman in another problem giving up for columbia whose sexual\n",
      "  2. this show of time this because i did some thing one of those who us the unwittingly aided and often\n",
      "Simple Bigram:\n",
      "  1. this movie to get in society then he understands why perfectly paced insight into watching scary too bad very fast\n",
      "  2. ok a vhs tape </s>\n",
      "\n",
      "Trigram:\n",
      "  1. a very ambitious with the british movie but the intricate plot the asinine event of 1943 and made it different\n",
      "  2. warning there are at personal risk in french cinema just doe not notice it when they were both kid and\n",
      "Simple Trigram:\n",
      "  1. not being able to create a feel good about clearly unfortunately it had only done by crowhurst and his wife\n",
      "  2. although i have ever seen but definitely great saturday afternoon let s just hope there ll be on a true\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize\n",
    "bigram_model = BigramLanguageModel(alpha=0.01)\n",
    "simple_bigram_model = SimpleBigramLanguageModel(alpha=0.01)\n",
    "trigram_model = TrigramLanguageModel(alpha=0.01)\n",
    "simple_trigram_model = SimpleTrigramLanguageModel(alpha=0.01)\n",
    "\n",
    "# Train Bigram\n",
    "print(\"Training Bigram...\")\n",
    "start = time.time()\n",
    "train_bi_adapted = [s[1:] for s in train_ngram] \n",
    "test_bi_adapted = [s[1:] for s in test_ngram]\n",
    "bigram_model.train(train_bi_adapted)\n",
    "bigram_time = time.time() - start\n",
    "print(f\"Bigram Trained in {bigram_time:.4f}s\")\n",
    "\n",
    "# Train Simple Bigram\n",
    "print(\"Training Simple Bigram...\")\n",
    "start = time.time()\n",
    "simple_bigram_model.train(train_bi_adapted)\n",
    "simple_bigram_time = time.time() - start\n",
    "print(f\"Simple Bigram Trained in {simple_bigram_time:.4f}s\")\n",
    "\n",
    "# Train Trigram\n",
    "print(\"Training Trigram...\")\n",
    "start = time.time()\n",
    "trigram_model.train(train_ngram)\n",
    "trigram_time = time.time() - start\n",
    "print(f\"Trigram Trained in {trigram_time:.4f}s\")\n",
    "\n",
    "# Train Simple Trigram\n",
    "print(\"Training Simple Trigram...\")\n",
    "start = time.time()\n",
    "simple_trigram_model.train(train_ngram)\n",
    "simple_trigram_time = time.time() - start\n",
    "print(f\"Simple Trigram Trained in {simple_trigram_time:.4f}s\")\n",
    "\n",
    "# Perplexity\n",
    "pp_bi = bigram_model.calculate_perplexity(test_bi_adapted)\n",
    "pp_simple_bi = simple_bigram_model.calculate_perplexity(test_bi_adapted)\n",
    "pp_tri = trigram_model.calculate_perplexity(test_ngram)\n",
    "pp_simple_tri = simple_trigram_model.calculate_perplexity(test_ngram)\n",
    "\n",
    "print(f\"\\nBigram Perplexity: {pp_bi:.2f}\")\n",
    "print(f\"Simple Bigram Perplexity: {pp_simple_bi:.2f}\")\n",
    "print(f\"Trigram Perplexity: {pp_tri:.2f}\")\n",
    "print(f\"Simple Trigram Perplexity: {pp_simple_tri:.2f}\")\n",
    "\n",
    "# Generation Comparison\n",
    "print(\"\\n--- Generation Comparison (Unprompted) ---\")\n",
    "print(\"Bigram:\")\n",
    "for i in range(2):\n",
    "    print(f\"  {i+1}. {bigram_model.generate_sentence()}\")\n",
    "print(\"Simple Bigram:\")\n",
    "for i in range(2):\n",
    "    print(f\"  {i+1}. {simple_bigram_model.generate_sentence()}\")\n",
    "\n",
    "print(\"\\nTrigram:\")\n",
    "for i in range(2):\n",
    "    print(f\"  {i+1}. {trigram_model.generate_sentence()}\")\n",
    "print(\"Simple Trigram:\")\n",
    "for i in range(2):\n",
    "    print(f\"  {i+1}. {simple_trigram_model.generate_sentence()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Comparison (LSTM vs N-grams)\n",
    "Comparing Logic, Training Time, and Perplexity across all three architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Neural Model on cuda...\n",
      "Building vocabulary...\n",
      "Vocabulary size: 40253\n",
      "Starting Training. \n",
      "Epoch 1: Loss 5.4724\n",
      "Epoch 2: Loss 4.7405\n",
      "Epoch 3: Loss 4.4291\n",
      "Epoch 4: Loss 4.2026\n",
      "Epoch 5: Loss 4.0246\n",
      "Epoch 6: Loss 3.8792\n",
      "Neural Trained in 350.6895s\n",
      "Neural Perplexity: 136.08\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train Neural Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training Neural Model on {device}...\")\n",
    "\n",
    "# Config\n",
    "EMBED_DIM = 64\n",
    "HIDDEN_DIM = 512\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_EPOCHS = 6 \n",
    "\n",
    "# Setup\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(train_neural, min_freq=2) # => lowers perplexity and decreases train time\n",
    "dataset = IMDBDataset(train_neural, vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model_lstm = NeuralLM(len(vocab), EMBED_DIM, HIDDEN_DIM).to(device)\n",
    "optimizer = optim.Adam(model_lstm.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Training Loop\n",
    "start = time.time()\n",
    "model_lstm.train()\n",
    "loss_history = []\n",
    "\n",
    "print(f\"Starting Training. \")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for inputs, targets, lengths in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model_lstm(inputs, lengths)\n",
    "        loss = criterion(logits.view(-1, len(vocab)), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    navg = epoch_loss / len(dataloader)\n",
    "    loss_history.append(navg)\n",
    "    print(f\"Epoch {epoch+1}: Loss {navg:.4f}\")\n",
    "\n",
    "neural_time = time.time() - start\n",
    "print(f\"Neural Trained in {neural_time:.4f}s\")\n",
    "\n",
    "# Neural Perplexity (Approximate on Test Set)\n",
    "test_ds = IMDBDataset(test_neural, vocab)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "model_lstm.eval()\n",
    "total_n_loss = 0\n",
    "total_batches = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets, lengths in test_dl:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        logits, _ = model_lstm(inputs, lengths)\n",
    "        # Loss per batch\n",
    "        loss = criterion(logits.view(-1, len(vocab)), targets.view(-1))\n",
    "        total_n_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "avg_test_loss = total_n_loss / total_batches if total_batches > 0 else 999\n",
    "pp_neural = math.exp(avg_test_loss)\n",
    "print(f\"Neural Perplexity: {pp_neural:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Performance Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vogle\\AppData\\Local\\Temp\\ipykernel_38088\\999618731.py:8: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=models, y=times, ax=axes[0], palette=\"viridis\")\n",
      "C:\\Users\\vogle\\AppData\\Local\\Temp\\ipykernel_38088\\999618731.py:15: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=models, y=perplexities, ax=axes[1], palette=\"magma\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "models = ['Bigram', 'Simple Bi', 'Trigram', 'Simple Tri', 'LSTM']\n",
    "times = [bigram_time, simple_bigram_time, trigram_time, simple_trigram_time, neural_time]\n",
    "perplexities = [pp_bi, pp_simple_bi, pp_tri, pp_simple_tri, pp_neural]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time Plot\n",
    "sns.barplot(x=models, y=times, ax=axes[0], palette=\"viridis\")\n",
    "axes[0].set_title(\"Training Time (seconds)\")\n",
    "axes[0].set_ylabel(\"Seconds\")\n",
    "for i, v in enumerate(times):\n",
    "    axes[0].text(i, v + 0.1, f\"{v:.1f}s\", ha='center')\n",
    "\n",
    "# Perplexity Plot\n",
    "sns.barplot(x=models, y=perplexities, ax=axes[1], palette=\"magma\")\n",
    "axes[1].set_title(\"Test Perplexity (Lower is Better)\")\n",
    "axes[1].set_ylabel(\"Perplexity\")\n",
    "for i, v in enumerate(perplexities):\n",
    "    axes[1].text(i, v + 1, f\"{v:.1f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Qualitative Comparison: Text Generation\n",
    "Comparing outputs with and without prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Bigram</th>\n",
       "      <th>Simple Bi</th>\n",
       "      <th>Trigram</th>\n",
       "      <th>Simple Tri</th>\n",
       "      <th>LSTM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unprompted</td>\n",
       "      <td>the story about the movie where she transforms the selling movie could not dehumanize and hong sang to go by</td>\n",
       "      <td>you flirting with plot it back after somebody would rather than spinal tap i have any other film wa written</td>\n",
       "      <td>i have always enjoyed film like this show is the big screen and about real event very well the local</td>\n",
       "      <td>if there would have had in recapturing the humor is great love story bird and a bunch of garbage thomas</td>\n",
       "      <td>the phone rings best was dubbed and did</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The movie was</td>\n",
       "      <td>The movie was everybody on the same time watching it a tough guy lost originally going back to russell is a total lack</td>\n",
       "      <td>The movie was lucky luke wilson meet plump balding william phipps a a the script to dance in bringing an awesome in the</td>\n",
       "      <td>The movie was a pretty poor &lt;/s&gt;</td>\n",
       "      <td>The movie was being torn by war and went without any muscle power although he quietly nail the actual job while hank plan</td>\n",
       "      <td>the movie was so empty , and inane scene where chinese cop eddy presents bear down the past few years of the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought that</td>\n",
       "      <td>I thought that go check it suck up the series at the u s show the first section of the only one of</td>\n",
       "      <td>I thought that but is a movie nobody involved in 1992 and crook who knew he is due to death star animated it</td>\n",
       "      <td>I thought that myrna loy fred derry upon returning the film wa frank sinatra and he just say it can get arthritis in</td>\n",
       "      <td>I thought that tim curry and ernie three rich youth who have actually been a lifetime you have the choice of actor who</td>\n",
       "      <td>i thought that this might be a fine example of what jackie chan has given him career</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Prompt  \\\n",
       "0      Unprompted   \n",
       "1   The movie was   \n",
       "2  I thought that   \n",
       "\n",
       "                                                                                                                   Bigram  \\\n",
       "0            the story about the movie where she transforms the selling movie could not dehumanize and hong sang to go by   \n",
       "1  The movie was everybody on the same time watching it a tough guy lost originally going back to russell is a total lack   \n",
       "2                      I thought that go check it suck up the series at the u s show the first section of the only one of   \n",
       "\n",
       "                                                                                                                 Simple Bi  \\\n",
       "0              you flirting with plot it back after somebody would rather than spinal tap i have any other film wa written   \n",
       "1  The movie was lucky luke wilson meet plump balding william phipps a a the script to dance in bringing an awesome in the   \n",
       "2             I thought that but is a movie nobody involved in 1992 and crook who knew he is due to death star animated it   \n",
       "\n",
       "                                                                                                                Trigram  \\\n",
       "0                  i have always enjoyed film like this show is the big screen and about real event very well the local   \n",
       "1                                                                                      The movie was a pretty poor </s>   \n",
       "2  I thought that myrna loy fred derry upon returning the film wa frank sinatra and he just say it can get arthritis in   \n",
       "\n",
       "                                                                                                                  Simple Tri  \\\n",
       "0                    if there would have had in recapturing the humor is great love story bird and a bunch of garbage thomas   \n",
       "1  The movie was being torn by war and went without any muscle power although he quietly nail the actual job while hank plan   \n",
       "2     I thought that tim curry and ernie three rich youth who have actually been a lifetime you have the choice of actor who   \n",
       "\n",
       "                                                                                                           LSTM  \n",
       "0                                                                       the phone rings best was dubbed and did  \n",
       "1  the movie was so empty , and inane scene where chinese cop eddy presents bear down the past few years of the  \n",
       "2                          i thought that this might be a fine example of what jackie chan has given him career  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "prompts = [None, \"The movie was\", \"I thought that\"]\n",
    "\n",
    "def get_continuation(model_type, prompt):\n",
    "    if prompt is None:\n",
    "        if model_type == 'LSTM':\n",
    "            return generate_text(model_lstm, vocab, \"The\", device=device)\n",
    "        elif model_type == 'Bigram':\n",
    "            return bigram_model.generate_sentence()\n",
    "        elif model_type == 'Simple Bi':\n",
    "            return simple_bigram_model.generate_sentence()\n",
    "        elif model_type == 'Trigram':\n",
    "            return trigram_model.generate_sentence()\n",
    "        elif model_type == 'Simple Tri':\n",
    "            return simple_trigram_model.generate_sentence()\n",
    "    else:\n",
    "        # Prompted\n",
    "        if model_type == 'LSTM':\n",
    "            return generate_text(model_lstm, vocab, prompt, device=device)\n",
    "        elif model_type == 'Bigram':\n",
    "            return bigram_model.autocomplete(prompt, ngram_prep)\n",
    "        elif model_type == 'Simple Bi':\n",
    "            return simple_bigram_model.autocomplete(prompt, ngram_prep)\n",
    "        elif model_type == 'Trigram':\n",
    "            return trigram_model.autocomplete(prompt, ngram_prep)\n",
    "        elif model_type == 'Simple Tri':\n",
    "            return simple_trigram_model.autocomplete(prompt, ngram_prep)\n",
    "\n",
    "results = []\n",
    "for p in prompts:\n",
    "    p_label = p if p else \"Unprompted\"\n",
    "    row = {\"Prompt\": p_label}\n",
    "    row[\"Bigram\"] = get_continuation('Bigram', p)\n",
    "    row[\"Simple Bi\"] = get_continuation('Simple Bi', p)\n",
    "    row[\"Trigram\"] = get_continuation('Trigram', p)\n",
    "    row[\"Simple Tri\"] = get_continuation('Simple Tri', p)\n",
    "    row[\"LSTM\"] = get_continuation('LSTM', p)\n",
    "    results.append(row)\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(res_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
