{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project: Language Modeling Comparison\n",
    "This notebook implements and compares three language models: **Bigram**, **Trigram**, and **Neural LSTM**.\n",
    "It includes a unified preprocessing pipeline, training comparisons, and generation quality checks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import html\n",
    "import string\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Setup Visuals\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Download resources if checks fail (quietly)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    \n",
    "print(\"Libraries loaded.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessor Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    A robust and professional preprocessing pipeline for NLP tasks.\n",
    "    Designed to handle IMDB movie reviews for Classical, Neural, and Transformer models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 remove_html: bool = True,\n",
    "                 lowercase: bool = True,\n",
    "                 remove_punctuation: bool = False,\n",
    "                 remove_stopwords: bool = False,\n",
    "                 lemmatize: bool = False,\n",
    "                 expand_contractions: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline with specific configuration flags.\n",
    "        \n",
    "        Args:\n",
    "            remove_html (bool): Strip HTML tags (e.g., <br />). Default True.\n",
    "            lowercase (bool): Convert text to lowercase. Default True.\n",
    "            remove_punctuation (bool): Remove punctuation characters.\n",
    "            remove_stopwords (bool): Remove standard English stopwords.\n",
    "            lemmatize (bool): Apply WordNet lemmatization.\n",
    "            expand_contractions (bool): Expand \"isn't\" to \"is not\".\n",
    "        \"\"\"\n",
    "        self.remove_html = remove_html\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        self.expand_contractions = expand_contractions\n",
    "\n",
    "        # Pre-load resources to optimize runtime\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stop_words.remove(\"not\")\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Simple contraction map for expansion\n",
    "        self.contractions_dict = {\n",
    "            \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
    "            \"haven't\": \"have not\", \"hasn't\": \"has not\", \"hadn't\": \"had not\", \"won't\": \"will not\",\n",
    "            \"wouldn't\": \"would not\", \"don't\": \"do not\", \"doesn't\": \"does not\", \"didn't\": \"did not\",\n",
    "            \"can't\": \"cannot\", \"couldn't\": \"could not\", \"shouldn't\": \"should not\", \"mightn't\": \"might not\",\n",
    "            \"mustn't\": \"must not\", \"i'm\": \"i am\", \"you're\": \"you are\", \"he's\": \"he is\", \"she's\": \"she is\",\n",
    "            \"it's\": \"it is\", \"we're\": \"we are\", \"they're\": \"they are\", \"i've\": \"i have\", \"you've\": \"you have\",\n",
    "            \"we've\": \"we have\", \"they've\": \"they have\", \"i'll\": \"i will\", \"you'll\": \"you will\",\n",
    "            \"he'll\": \"he will\", \"she'll\": \"she will\", \"we'll\": \"we will\", \"they'll\": \"they will\"\n",
    "        }\n",
    "        self.contractions_re = re.compile('(%s)' % '|'.join(self.contractions_dict.keys()))\n",
    "\n",
    "    def _clean_html(self, text: str) -> str:\n",
    "        \"\"\"Removes HTML tags and unescapes HTML entities.\"\"\"\n",
    "        text = html.unescape(text)\n",
    "        # Regex for HTML tags\n",
    "        clean = re.compile('<.*?>')\n",
    "        return re.sub(clean, ' ', text)\n",
    "\n",
    "    def _expand_contractions(self, text: str) -> str:\n",
    "        \"\"\"Expands common English contractions.\"\"\"\n",
    "        def replace(match):\n",
    "            return self.contractions_dict[match.group(0)]\n",
    "        return self.contractions_re.sub(replace, text)\n",
    "\n",
    "    def _remove_punct(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes punctuation by replacing it with spaces.\n",
    "        This prevents 'word,word' from becoming 'wordword'.\n",
    "        \"\"\"\n",
    "        # Replace punctuation with a space\n",
    "        return re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)\n",
    "\n",
    "    def process_text(self, text: str) -> Union[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Main execution method. Applies enabled steps in the logical order.\n",
    "        \n",
    "        Returns:\n",
    "            str: If the final output is a joined string.\n",
    "            List[str]: If the processing flow ends in tokenization without re-joining.\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or not text:\n",
    "            return \"\"\n",
    "\n",
    "        # 1. Cleaning\n",
    "        if self.remove_html:\n",
    "            text = self._clean_html(text)\n",
    "        \n",
    "        # 2. Lowercasing\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "            \n",
    "        # 3. Expansion (must be after lowercasing for simple dict matching)\n",
    "        if self.expand_contractions:\n",
    "            text = self._expand_contractions(text)\n",
    "\n",
    "        # 4. Punctuation Removal\n",
    "        if self.remove_punctuation:\n",
    "            text = self._remove_punct(text)\n",
    "\n",
    "        # 5. Tokenization\n",
    "        # We always tokenize to perform word-level operations (stopword/lemma)\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # 6. Stopword Removal\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [w for w in tokens if w not in self.stop_words]\n",
    "\n",
    "        # 7. Lemmatization\n",
    "        if self.lemmatize:\n",
    "            tokens = [self.lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "        # Return list of tokens or join back to string depending on downstream need.\n",
    "        # For this pipeline, we generally return the list of tokens for Classical models,\n",
    "        # but for compatibility, we will join them back into a clean string \n",
    "        # because Tokenizers for Transformers/LSTMs often expect string input \n",
    "        # and do their own internal splitting.\n",
    "        \n",
    "        return \" \".join(tokens)\n",
    "\n",
    "# --- Usage Example / Demonstration ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Unified Preprocessing\n",
    "We pre-process the dataset once for all models to ensure fair comparison and save time.\n",
    "- **N-gram Corpus**: Lemmatized, punctuation kept (structural), contractions expanded.\n",
    "- **Neural Corpus**: Lowercase, no lemmatization (learns forms), standard tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 reviews.\n",
      "Preprocessing for N-gram models...\n",
      "Preprocessing for Neural models...\n",
      "Data ready. Train size: 8000, Test size: 2000\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "try:\n",
    "    df = pd.read_csv('IMDB Dataset.csv')\n",
    "    raw_reviews = df['review'].tolist()\n",
    "    print(f\"Loaded {len(raw_reviews)} reviews.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"IMDB Dataset.csv not found. Using dummy data.\")\n",
    "    raw_reviews = [\"The movie was terrible.\", \"I loved the film. It was great.\", \"The acting was bad.\"] * 100\n",
    "\n",
    "# Configuration\n",
    "SAMPLE_SIZE = 10000  # Adjust this for speed vs accuracy (Use len(raw_reviews) for full)\n",
    "raw_reviews = raw_reviews[:SAMPLE_SIZE]\n",
    "\n",
    "# 1. Pipeline for N-gram Models\n",
    "print(\"Preprocessing for N-gram models...\")\n",
    "ngram_prep = TextPreprocessor(\n",
    "    remove_html=True, \n",
    "    lowercase=True, \n",
    "    remove_punctuation=True, # Keep structure for n-grams\n",
    "    remove_stopwords=False, \n",
    "    lemmatize=True, \n",
    "    expand_contractions=True\n",
    ")\n",
    "\n",
    "ngram_corpus = []\n",
    "for r in raw_reviews:\n",
    "    cleaned = ngram_prep.process_text(r)\n",
    "    # Add start/end tokens. Bigram needs 1 start, Trigram needs 2. \n",
    "    # We will use 2 start tokens '<s>' '<s>' universally, Bigram can just ignore the extra one or we handle it in model.\n",
    "    # To be safe and exact to class definitions, let's store as lists.\n",
    "    tokens = cleaned.split()\n",
    "    # Bigram expects ['<s>', w1...]\n",
    "    # Trigram expects ['<s>', '<s>', w1...]\n",
    "    # We'll store the clean text list and pad per model or creating a generic '<s>' '<s>' ... '</s>'\n",
    "    # Let's create a standard list with 2 padding for maximum compatibility\n",
    "    tokens = ['<s>', '<s>'] + tokens + ['</s>']\n",
    "    ngram_corpus.append(tokens)\n",
    "\n",
    "# 2. Pipeline for Neural Model\n",
    "print(\"Preprocessing for Neural models...\")\n",
    "neural_prep = TextPreprocessor(\n",
    "    remove_html=True, \n",
    "    lowercase=True, \n",
    "    remove_punctuation=False,\n",
    "    remove_stopwords=False, \n",
    "    lemmatize=False, # Neural nets prefer original forms\n",
    "    expand_contractions=True\n",
    ")\n",
    "\n",
    "neural_corpus = []\n",
    "MAX_LEN = 100\n",
    "for r in raw_reviews:\n",
    "    cleaned = neural_prep.process_text(r)\n",
    "    tokens = cleaned.split()\n",
    "    # Neural model logic from script: ['<s>'] + tokens + ['</s>']\n",
    "    tokens = ['<s>'] + tokens[:MAX_LEN] + ['</s>']\n",
    "    neural_corpus.append(tokens)\n",
    "\n",
    "# Split (Sync split indices)\n",
    "split_idx = int(len(raw_reviews) * 0.8)\n",
    "\n",
    "train_ngram = ngram_corpus[:split_idx]\n",
    "test_ngram  = ngram_corpus[split_idx:]\n",
    "\n",
    "train_neural = neural_corpus[:split_idx]\n",
    "test_neural  = neural_corpus[split_idx:]\n",
    "\n",
    "print(f\"Data ready. Train size: {len(train_ngram)}, Test size: {len(test_ngram)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Definitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Bigram Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the Bigram Model.\n",
    "        \n",
    "        Args:\n",
    "            alpha (float): The smoothing parameter for Laplace smoothing. \n",
    "                           Default is 0.01.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        self.total_bigrams = 0\n",
    "        self.total_unigrams = 0\n",
    "        \n",
    "        self.lambda1 = 0.3 # Unigram\n",
    "        self.lambda2 = 0.7 # Bigram\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the model on a corpus of tokenized sentences.\n",
    "        Uses the full vocabulary (no <UNK> thresholding).\n",
    "        \"\"\"\n",
    "        print(\"Training model on full vocabulary...\")\n",
    "        for sentence in corpus:\n",
    "            # Update vocabulary and unigram counts\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "                self.unigram_counts[word] += 1\n",
    "                self.total_unigrams += 1\n",
    "            \n",
    "            # Update bigram counts\n",
    "            for i in range(len(sentence) - 1):\n",
    "                w_curr = sentence[i]\n",
    "                w_next = sentence[i+1]\n",
    "                self.bigram_counts[w_curr][w_next] += 1\n",
    "                self.total_bigrams += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f\"Training complete. Vocab size: {self.vocab_size}\")\n",
    "\n",
    "    def get_probability(self, prev_word, word):\n",
    "        \"\"\"\n",
    "        Calculates the interpolated probability P(word | prev_word).\n",
    "        P = L2 * P(word|prev) + L1 * P(word)\n",
    "        \"\"\"\n",
    "        # 1. Bigram Probability\n",
    "        bigram_count = self.bigram_counts[prev_word][word]\n",
    "        unigram_count_prev = self.unigram_counts[prev_word]\n",
    "        \n",
    "        p_bi_num = bigram_count + self.alpha\n",
    "        p_bi_den = unigram_count_prev + (self.alpha * self.vocab_size)\n",
    "        p_bi = p_bi_num / p_bi_den\n",
    "        \n",
    "        # 2. Unigram Probability\n",
    "        unigram_count_word = self.unigram_counts[word]\n",
    "        p_uni_num = unigram_count_word + self.alpha\n",
    "        p_uni_den = self.total_unigrams + (self.alpha * self.vocab_size)\n",
    "        p_uni = p_uni_num / p_uni_den\n",
    "        \n",
    "        return (self.lambda2 * p_bi) + (self.lambda1 * p_uni)\n",
    "\n",
    "    def calculate_perplexity(self, test_corpus):\n",
    "        \"\"\"\n",
    "        Calculates the perplexity of the model on a test corpus.\n",
    "        \"\"\"\n",
    "        log_prob_sum = 0\n",
    "        N = 0\n",
    "        \n",
    "        for sentence in test_corpus:\n",
    "            for i in range(len(sentence) - 1):\n",
    "                w_curr = sentence[i]\n",
    "                w_next = sentence[i+1]\n",
    "                \n",
    "                # We do not replace with <UNK>. If a word is unknown,\n",
    "                # get_probability handles it via smoothing.\n",
    "                prob = self.get_probability(w_curr, w_next)\n",
    "                \n",
    "                log_prob_sum += math.log2(prob)\n",
    "                N += 1\n",
    "        \n",
    "        if N == 0: return float('inf')\n",
    "        \n",
    "        avg_log_prob = -log_prob_sum / N\n",
    "        perplexity = 2 ** avg_log_prob\n",
    "        return perplexity\n",
    "\n",
    "    def generate_sentence(self, max_length=20):\n",
    "        \"\"\"\n",
    "        Generates a random sentence.\n",
    "        \"\"\"\n",
    "        current_word = \"<s>\"\n",
    "        sentence = [current_word]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "                \n",
    "            # If current_word was never seen in training (e.g. from a user prompt),\n",
    "            # unigram_count is 0. We fallback to uniform distribution or break.\n",
    "            # Here we sample from the whole vocab if unknown, or just observed followers if known.\n",
    "            \n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            \n",
    "            if not possible_next:\n",
    "                # Dead end or unknown word. \n",
    "                # Ideally: Sample uniformly from V (or weighted by unigrams).\n",
    "                # For efficiency/simplicity here: break or pick random.\n",
    "                break \n",
    "\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            sentence.append(next_word)\n",
    "            current_word = next_word\n",
    "            \n",
    "        return \" \".join(sentence[1:])\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        \"\"\"\n",
    "        Completes a given text prompt.\n",
    "        \"\"\"\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        \n",
    "        if not tokens:\n",
    "            current_word = \"<s>\"\n",
    "        else:\n",
    "            current_word = tokens[-1]\n",
    "            \n",
    "        # Warning: If current_word is not in self.vocab, generation will stop immediately\n",
    "        # because bigram_counts[current_word] will be empty.\n",
    "        \n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "            \n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            \n",
    "            if not possible_next:\n",
    "                break\n",
    "                \n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            generated_tokens.append(next_word)\n",
    "            current_word = next_word\n",
    "            \n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        \"\"\"\n",
    "        Completes a given text prompt using the trained model.\n",
    "        \"\"\"\n",
    "        # Preprocess the prompt to get the last token\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        \n",
    "        if not tokens:\n",
    "            current_word = \"<s>\"\n",
    "        else:\n",
    "            current_word = tokens[-1]\n",
    "            \n",
    "        # Handle OOV for the seed word\n",
    "        if current_word not in self.vocab:\n",
    "            # Optionally print a warning or fallback\n",
    "            current_word = \"<UNK>\"\n",
    "            \n",
    "        # Generate continuation\n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "            \n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            \n",
    "            if not possible_next:\n",
    "                # If we hit a dead end (should be rare with smoothing context, but possible if UNK), replace\n",
    "                current_word = \"<UNK>\"\n",
    "                possible_next = self.bigram_counts[current_word]\n",
    "\n",
    "            if not possible_next:\n",
    "                break\n",
    "                \n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            generated_tokens.append(next_word)\n",
    "            current_word = next_word\n",
    "            \n",
    "        return prompt + \" \" + \" \".join(generated_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Trigram Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def main():\\n    # 1. Load Data\\n    try:\\n        df = pd.read_csv(\\'IMDB Dataset.csv\\')\\n        print(\"Dataset loaded successfully.\")\\n        reviews = df[\\'review\\'].tolist() \\n    except FileNotFoundError:\\n        print(\"IMDB Dataset.csv not found. Using dummy data or trying small.\")\\n        try:\\n             df = pd.read_csv(\\'IMDB Dataset_small.csv\\')\\n             print(\"IMDB Dataset_small.csv loaded.\")\\n             reviews = df[\\'review\\'].tolist()\\n        except FileNotFoundError:\\n             reviews = [\"The movie was terrible.\", \"I loved the movie.\"]\\n\\n    # 2. Apply Preprocessing\\n    preprocessor = TextPreprocessor(\\n        remove_html=True,\\n        lowercase=True,\\n        remove_punctuation=False,\\n        remove_stopwords=False,\\n        lemmatize=False,\\n        expand_contractions=True)\\n\\n    tokenized_corpus = []\\n    print(\"Preprocessing texts...\")\\n    for r in reviews:\\n        cleaned_text = preprocessor.process_text(r)\\n        tokens = cleaned_text.split()\\n        # Trigram needs two start tokens to have context for the first real word\\n        tokens = [\\'<s>\\', \\'<s>\\'] + tokens + [\\'</s>\\']\\n        tokenized_corpus.append(tokens)\\n\\n    # 3. Split Train/Test\\n    split_idx = int(len(tokenized_corpus) * 0.8)\\n    train_data = tokenized_corpus[:split_idx]\\n    test_data = tokenized_corpus[split_idx:]\\n\\n    # 4. Initialize and Train\\n    model = TrigramLanguageModel(alpha=0.01)\\n\\n    start_time = time.time()\\n    model.train(train_data)\\n    end_time = time.time()\\n    print(f\"Time to build model: {end_time - start_time:.4f} seconds\")\\n\\n    # 5. Generate Text\\n    print(\"\\n--- Generated Reviews ---\")\\n    for _ in range(3):\\n        # We might generate \\'</s>\\' at the end, which generate_sentence returns.\\n        print(f\"- {model.generate_sentence()}\")\\n\\n    # 6. Evaluate Perplexity\\n    print(\"\\n--- Evaluation ---\")\\n    pp = model.calculate_perplexity(test_data)\\n    print(f\"Model Perplexity on Test Set: {pp:.2f}\")\\n\\n    # 7. Autocomplete Demo\\n    print(\"\\n--- Autocomplete Demo ---\")\\n    prompts = [\\n        \"The movie was\",\\n        \"I really liked\",\\n        \"The acting is\",\\n        \"This film is a complete\"\\n    ]\\n    for p in prompts:\\n        completed = model.autocomplete(p, preprocessor)\\n        print(f\"Prompt: \\'{p}\\'\\nResult: {completed}\\n\")'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TrigramLanguageModel:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the Trigram Model.\n",
    "        \n",
    "        Args:\n",
    "            alpha (float): The smoothing parameter for Laplace smoothing. \n",
    "                           Default is 0.01.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        # trigram_counts: count of (w1, w2, w3) aka given w1, w2, what is w3?\n",
    "        # Structure: dict[(w1, w2)] -> dict[w3] -> count\n",
    "        self.trigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        # bigram_counts: count of (w1, w2) as a history.\n",
    "        # Structure: dict[(w1, w2)] -> count\n",
    "        self.bigram_counts = defaultdict(int)\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        \n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        self.total_trigrams = 0\n",
    "        self.total_unigrams = 0\n",
    "        \n",
    "        # Interpolation weights\n",
    "        self.lambda1 = 0.1 # Unigram\n",
    "        self.lambda2 = 0.3 # Bigram\n",
    "        self.lambda3 = 0.6 # Trigram\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the model on a corpus of tokenized sentences.\n",
    "        Uses the full vocabulary (no <UNK> thresholding).\n",
    "        \"\"\"\n",
    "        print(\"Training model on full vocabulary...\")\n",
    "        for sentence in corpus:\n",
    "            # Update vocabulary and unigram counts\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "                self.unigram_counts[word] += 1\n",
    "                self.total_unigrams += 1\n",
    "            \n",
    "            # Update bigram counts (for backoff)\n",
    "            for i in range(len(sentence) - 1):\n",
    "                self.bigram_counts[(sentence[i], sentence[i+1])] += 1\n",
    "\n",
    "            # Update trigram counts\n",
    "            # Sentence is expected to be padded like ['<s>', '<s>', 'w1', ..., 'wn', '</s>']\n",
    "            for i in range(len(sentence) - 2):\n",
    "                w_1 = sentence[i]\n",
    "                w_2 = sentence[i+1]\n",
    "                w_3 = sentence[i+2]\n",
    "                \n",
    "                self.trigram_counts[(w_1, w_2)][w_3] += 1\n",
    "                self.total_trigrams += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f\"Training complete. Vocab size: {self.vocab_size}\")\n",
    "\n",
    "    def get_probability(self, w_1, w_2, w_3):\n",
    "        \"\"\"\n",
    "        Calculates the interpolated probability P(w_3 | w_1, w_2).\n",
    "        P = L3 * P(w3|w1,w2) + L2 * P(w3|w2) + L1 * P(w3)\n",
    "        \"\"\"\n",
    "        # 1. Trigram Probability\n",
    "        trigram_count = self.trigram_counts[(w_1, w_2)][w_3]\n",
    "        bigram_context_count = self.bigram_counts[(w_1, w_2)]\n",
    "        \n",
    "        p_tri_num = trigram_count + self.alpha\n",
    "        p_tri_den = bigram_context_count + (self.alpha * self.vocab_size)\n",
    "        p_tri = p_tri_num / p_tri_den\n",
    "        \n",
    "        # 2. Bigram Probability (Backoff)\n",
    "        bigram_count = self.bigram_counts[(w_2, w_3)]\n",
    "        unigram_context_count = self.unigram_counts[w_2]\n",
    "        \n",
    "        p_bi_num = bigram_count + self.alpha\n",
    "        p_bi_den = unigram_context_count + (self.alpha * self.vocab_size)\n",
    "        p_bi = p_bi_num / p_bi_den\n",
    "        \n",
    "        # 3. Unigram Probability\n",
    "        unigram_count = self.unigram_counts[w_3]\n",
    "        p_uni_num = unigram_count + self.alpha\n",
    "        p_uni_den = self.total_unigrams + (self.alpha * self.vocab_size)\n",
    "        p_uni = p_uni_num / p_uni_den\n",
    "        \n",
    "        return (self.lambda3 * p_tri) + (self.lambda2 * p_bi) + (self.lambda1 * p_uni)\n",
    "\n",
    "    def calculate_perplexity(self, test_corpus):\n",
    "        \"\"\"\n",
    "        Calculates the perplexity of the model on a test corpus.\n",
    "        \"\"\"\n",
    "        log_prob_sum = 0\n",
    "        N = 0\n",
    "        \n",
    "        for sentence in test_corpus:\n",
    "            for i in range(len(sentence) - 2):\n",
    "                w_1 = sentence[i]\n",
    "                w_2 = sentence[i+1]\n",
    "                w_3 = sentence[i+2]\n",
    "                \n",
    "                prob = self.get_probability(w_1, w_2, w_3)\n",
    "                \n",
    "                log_prob_sum += math.log2(prob)\n",
    "                N += 1\n",
    "        \n",
    "        if N == 0: return float('inf')\n",
    "        \n",
    "        avg_log_prob = -log_prob_sum / N\n",
    "        perplexity = 2 ** avg_log_prob\n",
    "        return perplexity\n",
    "\n",
    "    def generate_sentence(self, max_length=20):\n",
    "        \"\"\"\n",
    "        Generates a random sentence.\n",
    "        \"\"\"\n",
    "        # Start with two padding tokens\n",
    "        current_w1 = \"<s>\"\n",
    "        current_w2 = \"<s>\"\n",
    "        sentence = [current_w1, current_w2]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # If we generated the end token, stop\n",
    "            if current_w2 == \"</s>\":\n",
    "                break\n",
    "                \n",
    "            possible_next = self.trigram_counts[(current_w1, current_w2)]\n",
    "            \n",
    "            if not possible_next:\n",
    "                # If unknown history, we can't progress. \n",
    "                break \n",
    "\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            sentence.append(next_word)\n",
    "            current_w1 = current_w2\n",
    "            current_w2 = next_word\n",
    "            \n",
    "        # Return joined sentence, removing start tokens\n",
    "        # Typically we don't show <s> <s>\n",
    "        # The list has ['<s>', '<s>', 'word1', ... '</s>' maybe]\n",
    "        # We can strip the first two <s>\n",
    "        return \" \".join(sentence[2:])\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        \"\"\"\n",
    "        Completes a given text prompt using the trained model.\n",
    "        \"\"\"\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        \n",
    "        # Determine context words (need 2)\n",
    "        if len(tokens) >= 2:\n",
    "            current_w1 = tokens[-2]\n",
    "            current_w2 = tokens[-1]\n",
    "        elif len(tokens) == 1:\n",
    "            current_w1 = \"<s>\"\n",
    "            current_w2 = tokens[-1]\n",
    "        else:\n",
    "            current_w1 = \"<s>\"\n",
    "            current_w2 = \"<s>\"\n",
    "            \n",
    "        # Handle OOV - simplistic approach, similar to bigram fallbacks could be added, \n",
    "        # but here we rely on smoothing or break if empty.\n",
    "        \n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_w2 == \"</s>\":\n",
    "                break\n",
    "            \n",
    "            possible_next = self.trigram_counts[(current_w1, current_w2)]\n",
    "            \n",
    "            if not possible_next:\n",
    "                # If we dead end, we could maybe try fallback to bigram?\n",
    "                # But for strict trigram implementation request:\n",
    "                break\n",
    "                \n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            generated_tokens.append(next_word)\n",
    "            current_w1 = current_w2\n",
    "            current_w2 = next_word\n",
    "            \n",
    "        return prompt + \" \" + \" \".join(generated_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Neural Model (LSTM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx:\n",
    "            self.token_to_idx = token_to_idx\n",
    "        else:\n",
    "            self.token_to_idx = {\"<PAD>\": 0, \"<UNK>\": 1, \"<s>\": 2, \"</s>\": 3}\n",
    "        self.idx_to_token = {v: k for k, v in self.token_to_idx.items()}\n",
    "        \n",
    "    def build_vocab(self, sentences, min_freq=2):\n",
    "        print(\"Building vocabulary...\")\n",
    "        all_tokens = [token for sent in sentences for token in sent]\n",
    "        counts = Counter(all_tokens)\n",
    "        \n",
    "        for token, count in counts.items():\n",
    "            if count >= min_freq and token not in self.token_to_idx:\n",
    "                self.token_to_idx[token] = len(self.token_to_idx)\n",
    "                \n",
    "        self.idx_to_token = {v: k for k, v in self.token_to_idx.items()}\n",
    "        print(f\"Vocabulary size: {len(self.token_to_idx)}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "    \n",
    "    def stoi(self, token):\n",
    "        return self.token_to_idx.get(token, self.token_to_idx[\"<UNK>\"])\n",
    "        \n",
    "    def itos(self, idx):\n",
    "        return self.idx_to_token.get(idx, \"<UNK>\")\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, sentences, vocab):\n",
    "        self.sentences = sentences\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_sent = self.sentences[idx]\n",
    "        # Numericalize\n",
    "        indexed = [self.vocab.stoi(t) for t in tokenized_sent]\n",
    "        return torch.tensor(indexed, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable length sentences via padding.\n",
    "    \"\"\"\n",
    "    # batch is a list of tensors\n",
    "    # Sort by length (descending) for pack_padded_sequence\n",
    "    batch.sort(key=lambda x: len(x), reverse=True)\n",
    "    \n",
    "    # Separate source and target\n",
    "    # Source: <s> w1 w2 ... wn\n",
    "    # Target: w1 w2 ... wn </s>\n",
    "    # Actually, our sentences in 'sentences' list usually have <s> and </s> already.\n",
    "    # So we just take :-1 as input and 1: as target.\n",
    "    \n",
    "    inputs = [item[:-1] for item in batch]\n",
    "    targets = [item[1:] for item in batch]\n",
    "    \n",
    "    lengths = torch.tensor([len(x) for x in inputs], dtype=torch.long)\n",
    "    \n",
    "    # Pad sequences\n",
    "    # padding_value=0 is <PAD>\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return inputs_padded, targets_padded, lengths\n",
    "\n",
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(NeuralLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, lengths=None, hidden=None):\n",
    "        # x: (batch, seq_len)\n",
    "        embed = self.embedding(x) # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        if lengths is not None:\n",
    "            # Pack\n",
    "            packed_embed = pack_padded_sequence(embed, lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
    "            packed_out, hidden = self.lstm(packed_embed, hidden)\n",
    "            # Unpack\n",
    "            output, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        else:\n",
    "            # No packing (e.g. inference)\n",
    "            output, hidden = self.lstm(embed, hidden)\n",
    "            \n",
    "        # output: (batch, seq_len, hidden_dim) (padded where needed)\n",
    "        \n",
    "        logits = self.fc(output) # (batch, seq_len, vocab_size)\n",
    "        return logits, hidden\n",
    "\n",
    "def generate_text(model, vocab, start_prompt=\"The movie\", max_len=20, device='cpu', temperature=1.0):\n",
    "    model.eval()\n",
    "    preprocessor = TextPreprocessor(lowercase=True)\n",
    "    tokens = preprocessor.process_text(start_prompt).split()\n",
    "    \n",
    "    current_idx = [vocab.stoi(t) for t in tokens]\n",
    "    # Add start token if not present logic? \n",
    "    # The model trained on <s>... so prompt should ideally start with something logical.\n",
    "    # If we feed \"The movie\", it's mid-sentence-ish.\n",
    "    \n",
    "    input_seq = torch.tensor(current_idx, dtype=torch.long).unsqueeze(0).to(device) # (1, seq_len)\n",
    "    \n",
    "    generated = list(tokens)\n",
    "    \n",
    "    hidden = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            logits, hidden = model(input_seq, hidden=hidden)\n",
    "            \n",
    "            # Get last time step\n",
    "            last_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Apply temperature\n",
    "            if temperature != 1.0:\n",
    "                last_logits = last_logits / temperature\n",
    "                \n",
    "            probs = torch.softmax(last_logits, dim=0)\n",
    "            \n",
    "            # Sample\n",
    "            next_token_idx = torch.multinomial(probs, 1).item()\n",
    "            next_token = vocab.itos(next_token_idx)\n",
    "            \n",
    "            if next_token == \"</s>\":\n",
    "                break\n",
    "                \n",
    "            generated.append(next_token)\n",
    "            \n",
    "            # Next input is the single token we just generated (feeding back one by one)\n",
    "            # Or we could feed the whole sequence, but feeding 1 is efficient IF we keep hidden state.\n",
    "            input_seq = torch.tensor([[next_token_idx]], dtype=torch.long).to(device)\n",
    "            \n",
    "    return \" \".join(generated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison: Bigram vs Trigram\n",
    "We compare the perplexity and generation quality of the statistical models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bigram...\n",
      "Training model on full vocabulary...\n",
      "Training complete. Vocab size: 43007\n",
      "Bigram Trained in 0.9154s\n",
      "Training Trigram...\n",
      "Training model on full vocabulary...\n",
      "Training complete. Vocab size: 43007\n",
      "Trigram Trained in 3.3294s\n",
      "\n",
      "Bigram Perplexity: 441.69\n",
      "Trigram Perplexity: 614.51\n",
      "\n",
      "--- Generation Comparison (Unprompted) ---\n",
      "Bigram:\n",
      "  1. i say south africa she audition cassie did the total bore you will just intertwined with max star s ex\n",
      "  2. i m anderson is it really enjoyable the scene where s music and future episode from a local ha been\n",
      "  3. wow it ha become a mess and the cloud thing that is released by that surprising for it is very\n",
      "\n",
      "Trigram:\n",
      "  1. might contain spoiler but anyway any woman that he is probably a representation of the attention to the subtle way\n",
      "  2. this is available on u try it on camera is refreshing and honest district attorney kate capshaw if you like\n",
      "  3. there can be proud of it all off i do wonder for her and eventually expelled from england they were\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize\n",
    "bigram_model = BigramLanguageModel(alpha=0.01)\n",
    "trigram_model = TrigramLanguageModel(alpha=0.01)\n",
    "\n",
    "# Train Bigram\n",
    "print(\"Training Bigram...\")\n",
    "start = time.time()\n",
    "# Bigram expects 1 start token. Our corpus has 2. \n",
    "# Adaptation: Removing one <s> for Bigram training data\n",
    "train_bi_adapted = [s[1:] for s in train_ngram] \n",
    "test_bi_adapted = [s[1:] for s in test_ngram]\n",
    "\n",
    "bigram_model.train(train_bi_adapted)\n",
    "bigram_time = time.time() - start\n",
    "print(f\"Bigram Trained in {bigram_time:.4f}s\")\n",
    "\n",
    "# Train Trigram\n",
    "print(\"Training Trigram...\")\n",
    "start = time.time()\n",
    "# Trigram expects 2 start tokens. Our corpus has 2. Perfect.\n",
    "trigram_model.train(train_ngram)\n",
    "trigram_time = time.time() - start\n",
    "print(f\"Trigram Trained in {trigram_time:.4f}s\")\n",
    "\n",
    "# Perplexity\n",
    "pp_bi = bigram_model.calculate_perplexity(test_bi_adapted)\n",
    "pp_tri = trigram_model.calculate_perplexity(test_ngram)\n",
    "\n",
    "print(f\"\\nBigram Perplexity: {pp_bi:.2f}\")\n",
    "print(f\"Trigram Perplexity: {pp_tri:.2f}\")\n",
    "\n",
    "# Generation Comparison\n",
    "print(\"\\n--- Generation Comparison (Unprompted) ---\")\n",
    "print(\"Bigram:\")\n",
    "for i in range(3):\n",
    "    print(f\"  {i+1}. {bigram_model.generate_sentence()}\")\n",
    "\n",
    "print(\"\\nTrigram:\")\n",
    "for i in range(3):\n",
    "    print(f\"  {i+1}. {trigram_model.generate_sentence()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Comparison (LSTM vs N-grams)\n",
    "Comparing Logic, Training Time, and Perplexity across all three architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Neural Model on cuda...\n",
      "Building vocabulary...\n",
      "Vocabulary size: 17694\n",
      "Epoch 1: Loss 6.0839\n",
      "Epoch 2: Loss 5.2240\n",
      "Epoch 3: Loss 4.8302\n",
      "Epoch 4: Loss 4.5075\n",
      "Epoch 5: Loss 4.2198\n",
      "Epoch 6: Loss 3.9599\n",
      "Neural Trained in 28.0832s\n",
      "Neural Perplexity: 160.76\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train Neural Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training Neural Model on {device}...\")\n",
    "\n",
    "# Config\n",
    "EMBED_DIM = 64\n",
    "HIDDEN_DIM = 512\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_EPOCHS = 6 # Kept low for notebook speed\n",
    "\n",
    "# Setup\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(train_neural, min_freq=2)\n",
    "dataset = IMDBDataset(train_neural, vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model_lstm = NeuralLM(len(vocab), EMBED_DIM, HIDDEN_DIM).to(device)\n",
    "optimizer = optim.Adam(model_lstm.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Training Loop\n",
    "start = time.time()\n",
    "model_lstm.train()\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for inputs, targets, lengths in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model_lstm(inputs, lengths)\n",
    "        loss = criterion(logits.view(-1, len(vocab)), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    navg = epoch_loss / len(dataloader)\n",
    "    loss_history.append(navg)\n",
    "    print(f\"Epoch {epoch+1}: Loss {navg:.4f}\")\n",
    "\n",
    "neural_time = time.time() - start\n",
    "print(f\"Neural Trained in {neural_time:.4f}s\")\n",
    "\n",
    "# Neural Perplexity (Approximate on Test Set)\n",
    "# Note: Exact perplexity for neural nets requires exponentiating the CrossEntropyLoss\n",
    "test_ds = IMDBDataset(test_neural, vocab)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "model_lstm.eval()\n",
    "total_n_loss = 0\n",
    "total_batches = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets, lengths in test_dl:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        logits, _ = model_lstm(inputs, lengths)\n",
    "        # Loss per batch\n",
    "        loss = criterion(logits.view(-1, len(vocab)), targets.view(-1))\n",
    "        total_n_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "avg_test_loss = total_n_loss / total_batches if total_batches > 0 else 999\n",
    "pp_neural = math.exp(avg_test_loss)\n",
    "print(f\"Neural Perplexity: {pp_neural:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Performance Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vogle\\AppData\\Local\\Temp\\ipykernel_21700\\3279862187.py:8: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=models, y=times, ax=axes[0], palette=\"viridis\")\n",
      "C:\\Users\\vogle\\AppData\\Local\\Temp\\ipykernel_21700\\3279862187.py:15: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=models, y=perplexities, ax=axes[1], palette=\"magma\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "models = ['Bigram', 'Trigram', 'LSTM']\n",
    "times = [bigram_time, trigram_time, neural_time]\n",
    "perplexities = [pp_bi, pp_tri, pp_neural]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time Plot\n",
    "sns.barplot(x=models, y=times, ax=axes[0], palette=\"viridis\")\n",
    "axes[0].set_title(\"Training Time (seconds)\")\n",
    "axes[0].set_ylabel(\"Seconds\")\n",
    "for i, v in enumerate(times):\n",
    "    axes[0].text(i, v + 0.1, f\"{v:.1f}s\", ha='center')\n",
    "\n",
    "# Perplexity Plot\n",
    "sns.barplot(x=models, y=perplexities, ax=axes[1], palette=\"magma\")\n",
    "axes[1].set_title(\"Test Perplexity (Lower is Better)\")\n",
    "axes[1].set_ylabel(\"Perplexity\")\n",
    "for i, v in enumerate(perplexities):\n",
    "    axes[1].text(i, v + 1, f\"{v:.1f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Qualitative Comparison: Text Generation\n",
    "Comparing outputs with and without prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Bigram</th>\n",
       "      <th>Trigram</th>\n",
       "      <th>LSTM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unprompted</td>\n",
       "      <td>what happened that were rewarded for the acting bad &lt;/s&gt;</td>\n",
       "      <td>with so much disdain for female beauty the earth would be that explicit it is so bad for the scenery</td>\n",
       "      <td>the way saving grace of the box office &lt;UNK&gt; . to the point that the different focus film under the film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The movie was</td>\n",
       "      <td>The movie was particularly fetching beauty she about film really good death every sunday who out the thing a an oft used in</td>\n",
       "      <td>The movie was made solely to be either the alternative comedy and i thought it wa her first  not a single millimeter</td>\n",
       "      <td>the movie was shot on a strange story . exciting as quickly was also out his neighborhood film . i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought that</td>\n",
       "      <td>I thought that several year back and outfit the north that they they are no audience if not be the game but somehow</td>\n",
       "      <td>I thought that this is my favorite part of the housekeeper nanny determined to retrieve a ruby s deadbeat father ray winstone nicolas</td>\n",
       "      <td>i thought that the characters ! ! of this movie .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Prompt  \\\n",
       "0      Unprompted   \n",
       "1   The movie was   \n",
       "2  I thought that   \n",
       "\n",
       "                                                                                                                        Bigram  \\\n",
       "0                                                                     what happened that were rewarded for the acting bad </s>   \n",
       "1  The movie was particularly fetching beauty she about film really good death every sunday who out the thing a an oft used in   \n",
       "2          I thought that several year back and outfit the north that they they are no audience if not be the game but somehow   \n",
       "\n",
       "                                                                                                                                 Trigram  \\\n",
       "0                                   with so much disdain for female beauty the earth would be that explicit it is so bad for the scenery   \n",
       "1                  The movie was made solely to be either the alternative comedy and i thought it wa her first  not a single millimeter   \n",
       "2  I thought that this is my favorite part of the housekeeper nanny determined to retrieve a ruby s deadbeat father ray winstone nicolas   \n",
       "\n",
       "                                                                                                       LSTM  \n",
       "0  the way saving grace of the box office <UNK> . to the point that the different focus film under the film  \n",
       "1        the movie was shot on a strange story . exciting as quickly was also out his neighborhood film . i  \n",
       "2                                                         i thought that the characters ! ! of this movie .  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "prompts = [None, \"The movie was\", \"I thought that\"]\n",
    "\n",
    "def get_continuation(model_type, prompt):\n",
    "    if prompt is None:\n",
    "        if model_type == 'LSTM':\n",
    "            # Neural needs a start token usually, let's give it <s> implicitly in function or empty\n",
    "            return generate_text(model_lstm, vocab, \"The\", device=device) # Start with generic\n",
    "        elif model_type == 'Bigram':\n",
    "            return bigram_model.generate_sentence()\n",
    "        else:\n",
    "            return trigram_model.generate_sentence()\n",
    "    else:\n",
    "        # Prompted\n",
    "        if model_type == 'LSTM':\n",
    "            return generate_text(model_lstm, vocab, prompt, device=device)\n",
    "        elif model_type == 'Bigram':\n",
    "            return bigram_model.autocomplete(prompt, ngram_prep)\n",
    "        else:\n",
    "            return trigram_model.autocomplete(prompt, ngram_prep)\n",
    "\n",
    "results = []\n",
    "for p in prompts:\n",
    "    p_label = p if p else \"Unprompted\"\n",
    "    row = {\"Prompt\": p_label}\n",
    "    row[\"Bigram\"] = get_continuation('Bigram', p)\n",
    "    row[\"Trigram\"] = get_continuation('Trigram', p)\n",
    "    row[\"LSTM\"] = get_continuation('LSTM', p)\n",
    "    results.append(row)\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(res_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
