{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project: Language Modeling Comparison\n",
    "This notebook implements and compares three language models: **Bigram**, **Trigram**, and **Neural LSTM**.\n",
    "It includes a unified preprocessing pipeline, training comparisons, and generation quality checks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import html\n",
    "import string\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Setup Visuals\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Download resources if checks fail (quietly)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    \n",
    "print(\"Libraries loaded.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessor Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    A robust and professional preprocessing pipeline for NLP tasks.\n",
    "    Designed to handle IMDB movie reviews for Classical, Neural, and Transformer models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 remove_html: bool = True,\n",
    "                 lowercase: bool = True,\n",
    "                 remove_punctuation: bool = False,\n",
    "                 remove_stopwords: bool = False,\n",
    "                 lemmatize: bool = False,\n",
    "                 expand_contractions: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline with specific configuration flags.\n",
    "        \n",
    "        Args:\n",
    "            remove_html (bool): Strip HTML tags (e.g., <br />). Default True.\n",
    "            lowercase (bool): Convert text to lowercase. Default True.\n",
    "            remove_punctuation (bool): Remove punctuation characters.\n",
    "            remove_stopwords (bool): Remove standard English stopwords.\n",
    "            lemmatize (bool): Apply WordNet lemmatization.\n",
    "            expand_contractions (bool): Expand \"isn't\" to \"is not\".\n",
    "        \"\"\"\n",
    "        self.remove_html = remove_html\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        self.expand_contractions = expand_contractions\n",
    "\n",
    "        # Pre-load resources to optimize runtime\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stop_words.remove(\"not\")\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Simple contraction map for expansion\n",
    "        self.contractions_dict = {\n",
    "            \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
    "            \"haven't\": \"have not\", \"hasn't\": \"has not\", \"hadn't\": \"had not\", \"won't\": \"will not\",\n",
    "            \"wouldn't\": \"would not\", \"don't\": \"do not\", \"doesn't\": \"does not\", \"didn't\": \"did not\",\n",
    "            \"can't\": \"cannot\", \"couldn't\": \"could not\", \"shouldn't\": \"should not\", \"mightn't\": \"might not\",\n",
    "            \"mustn't\": \"must not\", \"i'm\": \"i am\", \"you're\": \"you are\", \"he's\": \"he is\", \"she's\": \"she is\",\n",
    "            \"it's\": \"it is\", \"we're\": \"we are\", \"they're\": \"they are\", \"i've\": \"i have\", \"you've\": \"you have\",\n",
    "            \"we've\": \"we have\", \"they've\": \"they have\", \"i'll\": \"i will\", \"you'll\": \"you will\",\n",
    "            \"he'll\": \"he will\", \"she'll\": \"she will\", \"we'll\": \"we will\", \"they'll\": \"they will\"\n",
    "        }\n",
    "        self.contractions_re = re.compile('(%s)' % '|'.join(self.contractions_dict.keys()))\n",
    "\n",
    "    def _clean_html(self, text: str) -> str:\n",
    "        \"\"\"Removes HTML tags and unescapes HTML entities.\"\"\"\n",
    "        text = html.unescape(text)\n",
    "        # Regex for HTML tags\n",
    "        clean = re.compile('<.*?>')\n",
    "        return re.sub(clean, ' ', text)\n",
    "\n",
    "    def _expand_contractions(self, text: str) -> str:\n",
    "        \"\"\"Expands common English contractions.\"\"\"\n",
    "        def replace(match):\n",
    "            return self.contractions_dict[match.group(0)]\n",
    "        return self.contractions_re.sub(replace, text)\n",
    "\n",
    "    def _remove_punct(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes punctuation by replacing it with spaces.\n",
    "        This prevents 'word,word' from becoming 'wordword'.\n",
    "        \"\"\"\n",
    "        # Replace punctuation with a space\n",
    "        return re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)\n",
    "\n",
    "    def process_text(self, text: str) -> Union[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Main execution method. Applies enabled steps in the logical order.\n",
    "        \n",
    "        Returns:\n",
    "            str: If the final output is a joined string.\n",
    "            List[str]: If the processing flow ends in tokenization without re-joining.\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or not text:\n",
    "            return \"\"\n",
    "\n",
    "        # 1. Cleaning\n",
    "        if self.remove_html:\n",
    "            text = self._clean_html(text)\n",
    "        \n",
    "        # 2. Lowercasing\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "            \n",
    "        # 3. Expansion (must be after lowercasing for simple dict matching)\n",
    "        if self.expand_contractions:\n",
    "            text = self._expand_contractions(text)\n",
    "\n",
    "        # 4. Punctuation Removal\n",
    "        if self.remove_punctuation:\n",
    "            text = self._remove_punct(text)\n",
    "\n",
    "        # 5. Tokenization\n",
    "        # We always tokenize to perform word-level operations (stopword/lemma)\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # 6. Stopword Removal\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [w for w in tokens if w not in self.stop_words]\n",
    "\n",
    "        # 7. Lemmatization\n",
    "        if self.lemmatize:\n",
    "            tokens = [self.lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "        \n",
    "        return \" \".join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Unified Preprocessing\n",
    "We pre-process the dataset once for all models to ensure fair comparison and save time.\n",
    "- **N-gram Corpus**: Lemmatized, punctuation kept (structural), contractions expanded.\n",
    "- **Neural Corpus**: Lowercase, no lemmatization (learns forms), standard tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 reviews.\n",
      "Preprocessing for N-gram models...\n",
      "Preprocessing for Neural models...\n",
      "Data ready. Train size: 40000, Test size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "try:\n",
    "    df = pd.read_csv('IMDB Dataset.csv')\n",
    "    raw_reviews = df['review'].tolist()\n",
    "    print(f\"Loaded {len(raw_reviews)} reviews.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"IMDB Dataset.csv not found. Using dummy data.\")\n",
    "    raw_reviews = [\"The movie was terrible.\", \"I loved the film. It was great.\", \"The acting was bad.\"] * 100\n",
    "\n",
    "# Configuration\n",
    "SAMPLE_SIZE = len(raw_reviews)  \n",
    "raw_reviews = raw_reviews[:SAMPLE_SIZE]\n",
    "\n",
    "# 1. Pipeline for N-gram Models\n",
    "print(\"Preprocessing for N-gram models...\")\n",
    "ngram_prep = TextPreprocessor(\n",
    "    remove_html=True, \n",
    "    lowercase=True, \n",
    "    remove_punctuation=True, # Keep structure for n-grams\n",
    "    remove_stopwords=False, \n",
    "    lemmatize=True, \n",
    "    expand_contractions=True\n",
    ")\n",
    "\n",
    "ngram_corpus = []\n",
    "for r in raw_reviews:\n",
    "    cleaned = ngram_prep.process_text(r)\n",
    "    # Add start/end tokens. Bigram needs 1 start, Trigram needs 2. \n",
    "    # We will use 2 start tokens '<s>' '<s>' universally, Bigram can just ignore the extra one or we handle it in model.\n",
    "    # To be safe and exact to class definitions, let's store as lists.\n",
    "    tokens = cleaned.split()\n",
    "    # Bigram expects ['<s>', w1...]\n",
    "    # Trigram expects ['<s>', '<s>', w1...]\n",
    "    # We'll store the clean text list and pad per model or creating a generic '<s>' '<s>' ... '</s>'\n",
    "    tokens = ['<s>', '<s>'] + tokens + ['</s>']\n",
    "    ngram_corpus.append(tokens)\n",
    "\n",
    "# 2. Pipeline for Neural Model\n",
    "print(\"Preprocessing for Neural models...\")\n",
    "neural_prep = TextPreprocessor(\n",
    "    remove_html=True, \n",
    "    lowercase=True, \n",
    "    remove_punctuation=False,\n",
    "    remove_stopwords=False, \n",
    "    lemmatize=False,\n",
    "    expand_contractions=True\n",
    ")\n",
    "\n",
    "neural_corpus = []\n",
    "MAX_LEN = 100\n",
    "for r in raw_reviews:\n",
    "    cleaned = neural_prep.process_text(r)\n",
    "    tokens = cleaned.split()\n",
    "    # Neural model logic from script: ['<s>'] + tokens + ['</s>']\n",
    "    tokens = ['<s>'] + tokens[:MAX_LEN] + ['</s>']\n",
    "    neural_corpus.append(tokens)\n",
    "\n",
    "# Split (Sync split indices)\n",
    "split_idx = int(len(raw_reviews) * 0.8)\n",
    "\n",
    "train_ngram = ngram_corpus[:split_idx]\n",
    "test_ngram  = ngram_corpus[split_idx:]\n",
    "\n",
    "train_neural = neural_corpus[:split_idx]\n",
    "test_neural  = neural_corpus[split_idx:]\n",
    "\n",
    "print(f\"Data ready. Train size: {len(train_ngram)}, Test size: {len(test_ngram)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Definitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Bigram Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the Bigram Model.\n",
    "        \n",
    "        Args:\n",
    "            alpha (float): The smoothing parameter for Laplace smoothing. \n",
    "                           Default is 0.01.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        self.total_bigrams = 0\n",
    "        self.total_unigrams = 0\n",
    "        \n",
    "        self.lambda1 = 0.3 # Unigram\n",
    "        self.lambda2 = 0.7 # Bigram\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the model on a corpus of tokenized sentences.\n",
    "        Uses the full vocabulary (no <UNK> thresholding).\n",
    "        \"\"\"\n",
    "        print(\"Training model on full vocabulary...\")\n",
    "        for sentence in corpus:\n",
    "            # Update vocabulary and unigram counts\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "                self.unigram_counts[word] += 1\n",
    "                self.total_unigrams += 1\n",
    "            \n",
    "            # Update bigram counts\n",
    "            for i in range(len(sentence) - 1):\n",
    "                w_curr = sentence[i]\n",
    "                w_next = sentence[i+1]\n",
    "                self.bigram_counts[w_curr][w_next] += 1\n",
    "                self.total_bigrams += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f\"Training complete. Vocab size: {self.vocab_size}\")\n",
    "\n",
    "    def get_probability(self, prev_word, word):\n",
    "        \"\"\"\n",
    "        Calculates the interpolated probability P(word | prev_word).\n",
    "        P = L2 * P(word|prev) + L1 * P(word)\n",
    "        \"\"\"\n",
    "        # 1. Bigram Probability\n",
    "        bigram_count = self.bigram_counts[prev_word][word]\n",
    "        unigram_count_prev = self.unigram_counts[prev_word]\n",
    "        \n",
    "        p_bi_num = bigram_count + self.alpha\n",
    "        p_bi_den = unigram_count_prev + (self.alpha * self.vocab_size)\n",
    "        p_bi = p_bi_num / p_bi_den\n",
    "        \n",
    "        # 2. Unigram Probability\n",
    "        unigram_count_word = self.unigram_counts[word]\n",
    "        p_uni_num = unigram_count_word + self.alpha\n",
    "        p_uni_den = self.total_unigrams + (self.alpha * self.vocab_size)\n",
    "        p_uni = p_uni_num / p_uni_den\n",
    "        \n",
    "        return (self.lambda2 * p_bi) + (self.lambda1 * p_uni)\n",
    "\n",
    "    def calculate_perplexity(self, test_corpus):\n",
    "        \"\"\"\n",
    "        Calculates the perplexity of the model on a test corpus.\n",
    "        \"\"\"\n",
    "        log_prob_sum = 0\n",
    "        N = 0\n",
    "        \n",
    "        for sentence in test_corpus:\n",
    "            for i in range(len(sentence) - 1):\n",
    "                w_curr = sentence[i]\n",
    "                w_next = sentence[i+1]\n",
    "                \n",
    "                # We do not replace with <UNK>. If a word is unknown,\n",
    "                # get_probability handles it via smoothing.\n",
    "                prob = self.get_probability(w_curr, w_next)\n",
    "                \n",
    "                log_prob_sum += math.log2(prob)\n",
    "                N += 1\n",
    "        \n",
    "        if N == 0: return float('inf')\n",
    "        \n",
    "        avg_log_prob = -log_prob_sum / N\n",
    "        perplexity = 2 ** avg_log_prob\n",
    "        return perplexity\n",
    "\n",
    "    def generate_sentence(self, max_length=20):\n",
    "        \"\"\"\n",
    "        Generates a random sentence.\n",
    "        \"\"\"\n",
    "        current_word = \"<s>\"\n",
    "        sentence = [current_word]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "                \n",
    "            # If current_word was never seen in training (e.g. from a user prompt),\n",
    "            # unigram_count is 0. We fallback to uniform distribution or break.\n",
    "            # Here we sample from the whole vocab if unknown, or just observed followers if known.\n",
    "            \n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            \n",
    "            if not possible_next:\n",
    "                # Dead end or unknown word. \n",
    "                # Ideally: Sample uniformly from V (or weighted by unigrams).\n",
    "                # For efficiency/simplicity here: break or pick random.\n",
    "                break \n",
    "\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            sentence.append(next_word)\n",
    "            current_word = next_word\n",
    "            \n",
    "        return \" \".join(sentence[1:])\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        \"\"\"\n",
    "        Completes a given text prompt.\n",
    "        \"\"\"\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        \n",
    "        if not tokens:\n",
    "            current_word = \"<s>\"\n",
    "        else:\n",
    "            current_word = tokens[-1]\n",
    "            \n",
    "        # Warning: If current_word is not in self.vocab, generation will stop immediately\n",
    "        # because bigram_counts[current_word] will be empty.\n",
    "        \n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "            \n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            \n",
    "            if not possible_next:\n",
    "                break\n",
    "                \n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            generated_tokens.append(next_word)\n",
    "            current_word = next_word\n",
    "            \n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        \"\"\"\n",
    "        Completes a given text prompt using the trained model.\n",
    "        \"\"\"\n",
    "        # Preprocess the prompt to get the last token\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        \n",
    "        if not tokens:\n",
    "            current_word = \"<s>\"\n",
    "        else:\n",
    "            current_word = tokens[-1]\n",
    "            \n",
    "        # Handle OOV for the seed word\n",
    "        if current_word not in self.vocab:\n",
    "            # Optionally print a warning or fallback\n",
    "            current_word = \"<UNK>\"\n",
    "            \n",
    "        # Generate continuation\n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "            \n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            \n",
    "            if not possible_next:\n",
    "                # If we hit a dead end (should be rare with smoothing context, but possible if UNK), replace\n",
    "                current_word = \"<UNK>\"\n",
    "                possible_next = self.bigram_counts[current_word]\n",
    "\n",
    "            if not possible_next:\n",
    "                break\n",
    "                \n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            generated_tokens.append(next_word)\n",
    "            current_word = next_word\n",
    "            \n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n",
    "class SimpleBigramLanguageModel:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        print(\"Training Simple Bigram on full vocabulary...\")\n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "                self.unigram_counts[word] += 1\n",
    "            \n",
    "            for i in range(len(sentence) - 1):\n",
    "                w_curr = sentence[i]\n",
    "                w_next = sentence[i+1]\n",
    "                self.bigram_counts[w_curr][w_next] += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f\"Simple Bigram Training complete. Vocab size: {self.vocab_size}\")\n",
    "\n",
    "    def get_probability(self, prev_word, word):\n",
    "        bigram_count = self.bigram_counts[prev_word][word]\n",
    "        unigram_count_prev = self.unigram_counts[prev_word]\n",
    "        \n",
    "        num = bigram_count + self.alpha\n",
    "        den = unigram_count_prev + (self.alpha * self.vocab_size)\n",
    "        return num / den\n",
    "\n",
    "    def calculate_perplexity(self, test_corpus):\n",
    "        log_prob_sum = 0\n",
    "        N = 0\n",
    "        \n",
    "        for sentence in test_corpus:\n",
    "            for i in range(len(sentence) - 1):\n",
    "                w_curr = sentence[i]\n",
    "                w_next = sentence[i+1]\n",
    "                prob = self.get_probability(w_curr, w_next)\n",
    "                log_prob_sum += math.log2(prob)\n",
    "                N += 1\n",
    "        \n",
    "        if N == 0: return float('inf')\n",
    "        \n",
    "        avg_log_prob = -log_prob_sum / N\n",
    "        perplexity = 2 ** avg_log_prob\n",
    "        return perplexity\n",
    "\n",
    "    def generate_sentence(self, max_length=20):\n",
    "        current_word = \"<s>\"\n",
    "        sentence = [current_word]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            if not possible_next:\n",
    "                break \n",
    "\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            sentence.append(next_word)\n",
    "            current_word = next_word\n",
    "        return \" \".join(sentence[1:])\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        if not tokens:\n",
    "            current_word = \"<s>\"\n",
    "        else:\n",
    "            current_word = tokens[-1]\n",
    "        if current_word not in self.vocab:\n",
    "            current_word = \"<UNK>\"\n",
    "            \n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_word == \"</s>\":\n",
    "                break\n",
    "            possible_next = self.bigram_counts[current_word]\n",
    "            if not possible_next:\n",
    "                current_word = \"<UNK>\"\n",
    "                possible_next = self.bigram_counts[current_word]\n",
    "            if not possible_next:\n",
    "                break\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            generated_tokens.append(next_word)\n",
    "            current_word = next_word\n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Trigram Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramLanguageModel:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the Trigram Model.\n",
    "        \n",
    "        Args:\n",
    "            alpha (float): The smoothing parameter for Laplace smoothing. \n",
    "                           Default is 0.01.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        # trigram_counts: count of (w1, w2, w3) aka given w1, w2, what is w3?\n",
    "        # Structure: dict[(w1, w2)] -> dict[w3] -> count\n",
    "        self.trigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        # bigram_counts: count of (w1, w2) as a history.\n",
    "        # Structure: dict[(w1, w2)] -> count\n",
    "        self.bigram_counts = defaultdict(int)\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        \n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        self.total_trigrams = 0\n",
    "        self.total_unigrams = 0\n",
    "        \n",
    "        # Interpolation weights\n",
    "        self.lambda1 = 0.1 # Unigram\n",
    "        self.lambda2 = 0.3 # Bigram\n",
    "        self.lambda3 = 0.6 # Trigram\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Trains the model on a corpus of tokenized sentences.\n",
    "        \"\"\"\n",
    "        print(\"Training model on full vocabulary...\")\n",
    "        for sentence in corpus:\n",
    "            # Update vocabulary and unigram counts\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "                self.unigram_counts[word] += 1\n",
    "                self.total_unigrams += 1\n",
    "            \n",
    "            # Update bigram counts (for backoff)\n",
    "            for i in range(len(sentence) - 1):\n",
    "                self.bigram_counts[(sentence[i], sentence[i+1])] += 1\n",
    "\n",
    "            # Update trigram counts\n",
    "            # Sentence is expected to be padded like ['<s>', '<s>', 'w1', ..., 'wn', '</s>']\n",
    "            for i in range(len(sentence) - 2):\n",
    "                w_1 = sentence[i]\n",
    "                w_2 = sentence[i+1]\n",
    "                w_3 = sentence[i+2]\n",
    "                \n",
    "                self.trigram_counts[(w_1, w_2)][w_3] += 1\n",
    "                self.total_trigrams += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f\"Training complete. Vocab size: {self.vocab_size}\")\n",
    "\n",
    "    def get_probability(self, w_1, w_2, w_3):\n",
    "        \"\"\"\n",
    "        Calculates the interpolated probability P(w_3 | w_1, w_2).\n",
    "        P = L3 * P(w3|w1,w2) + L2 * P(w3|w2) + L1 * P(w3)\n",
    "        \"\"\"\n",
    "        # 1. Trigram Probability\n",
    "        trigram_count = self.trigram_counts[(w_1, w_2)][w_3]\n",
    "        bigram_context_count = self.bigram_counts[(w_1, w_2)]\n",
    "        \n",
    "        p_tri_num = trigram_count + self.alpha\n",
    "        p_tri_den = bigram_context_count + (self.alpha * self.vocab_size)\n",
    "        p_tri = p_tri_num / p_tri_den\n",
    "        \n",
    "        # 2. Bigram Probability (Backoff)\n",
    "        bigram_count = self.bigram_counts[(w_2, w_3)]\n",
    "        unigram_context_count = self.unigram_counts[w_2]\n",
    "        \n",
    "        p_bi_num = bigram_count + self.alpha\n",
    "        p_bi_den = unigram_context_count + (self.alpha * self.vocab_size)\n",
    "        p_bi = p_bi_num / p_bi_den\n",
    "        \n",
    "        # 3. Unigram Probability\n",
    "        unigram_count = self.unigram_counts[w_3]\n",
    "        p_uni_num = unigram_count + self.alpha\n",
    "        p_uni_den = self.total_unigrams + (self.alpha * self.vocab_size)\n",
    "        p_uni = p_uni_num / p_uni_den\n",
    "        \n",
    "        return (self.lambda3 * p_tri) + (self.lambda2 * p_bi) + (self.lambda1 * p_uni)\n",
    "\n",
    "    def calculate_perplexity(self, test_corpus):\n",
    "        \"\"\"\n",
    "        Calculates the perplexity of the model on a test corpus.\n",
    "        \"\"\"\n",
    "        log_prob_sum = 0\n",
    "        N = 0\n",
    "        \n",
    "        for sentence in test_corpus:\n",
    "            for i in range(len(sentence) - 2):\n",
    "                w_1 = sentence[i]\n",
    "                w_2 = sentence[i+1]\n",
    "                w_3 = sentence[i+2]\n",
    "                \n",
    "                prob = self.get_probability(w_1, w_2, w_3)\n",
    "                \n",
    "                log_prob_sum += math.log2(prob)\n",
    "                N += 1\n",
    "        \n",
    "        if N == 0: return float('inf')\n",
    "        \n",
    "        avg_log_prob = -log_prob_sum / N\n",
    "        perplexity = 2 ** avg_log_prob\n",
    "        return perplexity\n",
    "\n",
    "    def generate_sentence(self, max_length=20):\n",
    "        \"\"\"\n",
    "        Generates a random sentence.\n",
    "        \"\"\"\n",
    "        # Start with two padding tokens\n",
    "        current_w1 = \"<s>\"\n",
    "        current_w2 = \"<s>\"\n",
    "        sentence = [current_w1, current_w2]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # If we generated the end token, stop\n",
    "            if current_w2 == \"</s>\":\n",
    "                break\n",
    "                \n",
    "            possible_next = self.trigram_counts[(current_w1, current_w2)]\n",
    "            \n",
    "            if not possible_next:\n",
    "                # If unknown history, we can't progress. \n",
    "                break \n",
    "\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            sentence.append(next_word)\n",
    "            current_w1 = current_w2\n",
    "            current_w2 = next_word\n",
    "            \n",
    "        # Return joined sentence, removing start tokens\n",
    "        # Typically we don't show <s> <s>\n",
    "        # The list has ['<s>', '<s>', 'word1', ... '</s>' maybe]\n",
    "        # We can strip the first two <s>\n",
    "        return \" \".join(sentence[2:])\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        \"\"\"\n",
    "        Completes a given text prompt using the trained model.\n",
    "        \"\"\"\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        \n",
    "        # Determine context words (need 2)\n",
    "        if len(tokens) >= 2:\n",
    "            current_w1 = tokens[-2]\n",
    "            current_w2 = tokens[-1]\n",
    "        elif len(tokens) == 1:\n",
    "            current_w1 = \"<s>\"\n",
    "            current_w2 = tokens[-1]\n",
    "        else:\n",
    "            current_w1 = \"<s>\"\n",
    "            current_w2 = \"<s>\"\n",
    "            \n",
    "        # Handle OOV - simplistic approach, similar to bigram fallbacks could be added, \n",
    "        # but here we rely on smoothing or break if empty.\n",
    "        \n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_w2 == \"</s>\":\n",
    "                break\n",
    "            \n",
    "            possible_next = self.trigram_counts[(current_w1, current_w2)]\n",
    "            \n",
    "            if not possible_next:\n",
    "                # If we dead end, we could maybe try fallback to bigram?\n",
    "                # But for strict trigram implementation request:\n",
    "                break\n",
    "                \n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            \n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            \n",
    "            generated_tokens.append(next_word)\n",
    "            current_w1 = current_w2\n",
    "            current_w2 = next_word\n",
    "            \n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n",
    "class SimpleTrigramLanguageModel:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.trigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.bigram_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def train(self, corpus):\n",
    "        print(\"Training Simple Trigram on full vocabulary...\")\n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "            \n",
    "            for i in range(len(sentence) - 1):\n",
    "                self.bigram_counts[(sentence[i], sentence[i+1])] += 1\n",
    "\n",
    "            for i in range(len(sentence) - 2):\n",
    "                w_1 = sentence[i]\n",
    "                w_2 = sentence[i+1]\n",
    "                w_3 = sentence[i+2]\n",
    "                self.trigram_counts[(w_1, w_2)][w_3] += 1\n",
    "                \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f\"Simple Trigram Training complete. Vocab size: {self.vocab_size}\")\n",
    "\n",
    "    def get_probability(self, w_1, w_2, w_3):\n",
    "        trigram_count = self.trigram_counts[(w_1, w_2)][w_3]\n",
    "        bigram_context_count = self.bigram_counts[(w_1, w_2)]\n",
    "        \n",
    "        num = trigram_count + self.alpha\n",
    "        den = bigram_context_count + (self.alpha * self.vocab_size)\n",
    "        return num / den\n",
    "\n",
    "    def calculate_perplexity(self, test_corpus):\n",
    "        log_prob_sum = 0\n",
    "        N = 0\n",
    "        \n",
    "        for sentence in test_corpus:\n",
    "            for i in range(len(sentence) - 2):\n",
    "                w_1 = sentence[i]\n",
    "                w_2 = sentence[i+1]\n",
    "                w_3 = sentence[i+2]\n",
    "                prob = self.get_probability(w_1, w_2, w_3)\n",
    "                log_prob_sum += math.log2(prob)\n",
    "                N += 1\n",
    "        \n",
    "        if N == 0: return float('inf')\n",
    "        \n",
    "        avg_log_prob = -log_prob_sum / N\n",
    "        perplexity = 2 ** avg_log_prob\n",
    "        return perplexity\n",
    "\n",
    "    def generate_sentence(self, max_length=20):\n",
    "        current_w1 = \"<s>\"\n",
    "        current_w2 = \"<s>\"\n",
    "        sentence = [current_w1, current_w2]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            if current_w2 == \"</s>\":\n",
    "                break\n",
    "            possible_next = self.trigram_counts[(current_w1, current_w2)]\n",
    "            if not possible_next:\n",
    "                # If unknown history, break \n",
    "                break \n",
    "\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            sentence.append(next_word)\n",
    "            current_w1 = current_w2\n",
    "            current_w2 = next_word\n",
    "            \n",
    "        return \" \".join(sentence[2:])\n",
    "\n",
    "    def autocomplete(self, prompt, preprocessor, max_length=20):\n",
    "        cleaned_prompt = preprocessor.process_text(prompt)\n",
    "        tokens = cleaned_prompt.split()\n",
    "        if len(tokens) >= 2:\n",
    "            current_w1 = tokens[-2]\n",
    "            current_w2 = tokens[-1]\n",
    "        elif len(tokens) == 1:\n",
    "            current_w1 = \"<s>\"\n",
    "            current_w2 = tokens[-1]\n",
    "        else:\n",
    "            current_w1 = \"<s>\"\n",
    "            current_w2 = \"<s>\"\n",
    "        generated_tokens = []\n",
    "        for _ in range(max_length):\n",
    "            if current_w2 == \"</s>\":\n",
    "                break\n",
    "            possible_next = self.trigram_counts[(current_w1, current_w2)]\n",
    "            if not possible_next:\n",
    "                break\n",
    "            candidates = list(possible_next.keys())\n",
    "            counts = list(possible_next.values())\n",
    "            next_word = random.choices(candidates, weights=counts, k=1)[0]\n",
    "            generated_tokens.append(next_word)\n",
    "            current_w1 = current_w2\n",
    "            current_w2 = next_word\n",
    "        return prompt + \" \" + \" \".join(generated_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Neural Model (LSTM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx:\n",
    "            self.token_to_idx = token_to_idx\n",
    "        else:\n",
    "            self.token_to_idx = {\"<PAD>\": 0, \"<UNK>\": 1, \"<s>\": 2, \"</s>\": 3}\n",
    "        self.idx_to_token = {v: k for k, v in self.token_to_idx.items()}\n",
    "        \n",
    "    def build_vocab(self, sentences, min_freq=2):\n",
    "        print(\"Building vocabulary...\")\n",
    "        all_tokens = [token for sent in sentences for token in sent]\n",
    "        counts = Counter(all_tokens)\n",
    "        \n",
    "        for token, count in counts.items():\n",
    "            if count >= min_freq and token not in self.token_to_idx:\n",
    "                self.token_to_idx[token] = len(self.token_to_idx)\n",
    "                \n",
    "        self.idx_to_token = {v: k for k, v in self.token_to_idx.items()}\n",
    "        print(f\"Vocabulary size: {len(self.token_to_idx)}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "    \n",
    "    def stoi(self, token):\n",
    "        return self.token_to_idx.get(token, self.token_to_idx[\"<UNK>\"])\n",
    "        \n",
    "    def itos(self, idx):\n",
    "        return self.idx_to_token.get(idx, \"<UNK>\")\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, sentences, vocab):\n",
    "        self.sentences = sentences\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_sent = self.sentences[idx]\n",
    "        # Numericalize\n",
    "        indexed = [self.vocab.stoi(t) for t in tokenized_sent]\n",
    "        return torch.tensor(indexed, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable length sentences via padding.\n",
    "    \"\"\"\n",
    "    # batch is a list of tensors\n",
    "    # Sort by length (descending) for pack_padded_sequence\n",
    "    batch.sort(key=lambda x: len(x), reverse=True)\n",
    "    \n",
    "    # Separate source and target\n",
    "    # Source: <s> w1 w2 ... wn\n",
    "    # Target: w1 w2 ... wn </s>\n",
    "    \n",
    "    inputs = [item[:-1] for item in batch]\n",
    "    targets = [item[1:] for item in batch]\n",
    "    \n",
    "    lengths = torch.tensor([len(x) for x in inputs], dtype=torch.long)\n",
    "    \n",
    "    # Pad sequences\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return inputs_padded, targets_padded, lengths\n",
    "\n",
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(NeuralLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, lengths=None, hidden=None):\n",
    "        # x: (batch, seq_len)\n",
    "        embed = self.embedding(x) # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        if lengths is not None:\n",
    "            # Pack\n",
    "            packed_embed = pack_padded_sequence(embed, lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
    "            packed_out, hidden = self.lstm(packed_embed, hidden)\n",
    "            # Unpack\n",
    "            output, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        else:\n",
    "            # No packing (e.g. inference)\n",
    "            output, hidden = self.lstm(embed, hidden)\n",
    "            \n",
    "        # output: (batch, seq_len, hidden_dim) (padded where needed)\n",
    "        \n",
    "        logits = self.fc(output) # (batch, seq_len, vocab_size)\n",
    "        return logits, hidden\n",
    "\n",
    "def generate_text(model, vocab, start_prompt=\"The movie\", max_len=20, device='cpu', temperature=1.0):\n",
    "    model.eval()\n",
    "    preprocessor = TextPreprocessor(lowercase=True)\n",
    "    tokens = preprocessor.process_text(start_prompt).split()\n",
    "    \n",
    "    current_idx = [vocab.stoi(t) for t in tokens]\n",
    "    # Add start token if not present logic? \n",
    "    # The model trained on <s>... so prompt should ideally start with something logical.\n",
    "    # If we feed \"The movie\", it's mid-sentence-ish.\n",
    "    \n",
    "    input_seq = torch.tensor(current_idx, dtype=torch.long).unsqueeze(0).to(device) # (1, seq_len)\n",
    "    \n",
    "    generated = list(tokens)\n",
    "    \n",
    "    hidden = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            logits, hidden = model(input_seq, hidden=hidden)\n",
    "            \n",
    "            # Get last time step\n",
    "            last_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Apply temperature\n",
    "            if temperature != 1.0:\n",
    "                last_logits = last_logits / temperature\n",
    "                \n",
    "            probs = torch.softmax(last_logits, dim=0)\n",
    "            \n",
    "            # Sample\n",
    "            next_token_idx = torch.multinomial(probs, 1).item()\n",
    "            next_token = vocab.itos(next_token_idx)\n",
    "            \n",
    "            if next_token == \"</s>\":\n",
    "                break\n",
    "                \n",
    "            generated.append(next_token)\n",
    "            \n",
    "            # Next input is the single token we just generated (feeding back one by one)\n",
    "            # Or we could feed the whole sequence, but feeding 1 is efficient IF we keep hidden state.\n",
    "            input_seq = torch.tensor([[next_token_idx]], dtype=torch.long).to(device)\n",
    "            \n",
    "    return \" \".join(generated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison: Bigram vs Trigram\n",
    "We compare the perplexity and generation quality of the statistical models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bigram...\n",
      "Training model on full vocabulary...\n",
      "Training complete. Vocab size: 84589\n",
      "Bigram Trained in 8.9728s\n",
      "Training Simple Bigram...\n",
      "Training Simple Bigram on full vocabulary...\n",
      "Simple Bigram Training complete. Vocab size: 84589\n",
      "Simple Bigram Trained in 6.1873s\n",
      "Training Trigram...\n",
      "Training model on full vocabulary...\n",
      "Training complete. Vocab size: 84589\n",
      "Trigram Trained in 22.5765s\n",
      "Training Simple Trigram...\n",
      "Training Simple Trigram on full vocabulary...\n",
      "Simple Trigram Training complete. Vocab size: 84589\n",
      "Simple Trigram Trained in 17.3339s\n",
      "\n",
      "Bigram Perplexity: 363.91\n",
      "Simple Bigram Perplexity: 450.83\n",
      "Trigram Perplexity: 465.57\n",
      "Simple Trigram Perplexity: 2985.33\n",
      "\n",
      "--- Generation Comparison (Unprompted) ---\n",
      "Bigram:\n",
      "  1. i wa shot the course it is maybe have been caught this tedium which can only with a it is\n",
      "  2. although the visual metaphor and presently only natural response of the picture it and if the nakedness i wanted to\n",
      "Simple Bigram:\n",
      "  1. a camera zoom swoop whatever inside the first revival </s>\n",
      "  2. a other movie especially when they just a much difference he wa locked in real japanese guy simon yam louis\n",
      "\n",
      "Trigram:\n",
      "  1. at least she look great but i do recommend that you almost instantly at home half a minute clock to\n",
      "  2. aaliyah blow all of those film you get what s coming a mile a lady </s>\n",
      "Simple Trigram:\n",
      "  1. chupacabra terror is one of modern movie look like by having upbeat music i like my entertainment choice oh i\n",
      "  2. this is too dull for such talented actor like james wood f gary gray and always go the perfect hero\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize\n",
    "bigram_model = BigramLanguageModel(alpha=0.01)\n",
    "simple_bigram_model = SimpleBigramLanguageModel(alpha=0.01)\n",
    "trigram_model = TrigramLanguageModel(alpha=0.01)\n",
    "simple_trigram_model = SimpleTrigramLanguageModel(alpha=0.01)\n",
    "\n",
    "# Train Bigram\n",
    "print(\"Training Bigram...\")\n",
    "start = time.time()\n",
    "train_bi_adapted = [s[1:] for s in train_ngram] \n",
    "test_bi_adapted = [s[1:] for s in test_ngram]\n",
    "bigram_model.train(train_bi_adapted)\n",
    "bigram_time = time.time() - start\n",
    "print(f\"Bigram Trained in {bigram_time:.4f}s\")\n",
    "\n",
    "# Train Simple Bigram\n",
    "print(\"Training Simple Bigram...\")\n",
    "start = time.time()\n",
    "simple_bigram_model.train(train_bi_adapted)\n",
    "simple_bigram_time = time.time() - start\n",
    "print(f\"Simple Bigram Trained in {simple_bigram_time:.4f}s\")\n",
    "\n",
    "# Train Trigram\n",
    "print(\"Training Trigram...\")\n",
    "start = time.time()\n",
    "trigram_model.train(train_ngram)\n",
    "trigram_time = time.time() - start\n",
    "print(f\"Trigram Trained in {trigram_time:.4f}s\")\n",
    "\n",
    "# Train Simple Trigram\n",
    "print(\"Training Simple Trigram...\")\n",
    "start = time.time()\n",
    "simple_trigram_model.train(train_ngram)\n",
    "simple_trigram_time = time.time() - start\n",
    "print(f\"Simple Trigram Trained in {simple_trigram_time:.4f}s\")\n",
    "\n",
    "# Perplexity\n",
    "pp_bi = bigram_model.calculate_perplexity(test_bi_adapted)\n",
    "pp_simple_bi = simple_bigram_model.calculate_perplexity(test_bi_adapted)\n",
    "pp_tri = trigram_model.calculate_perplexity(test_ngram)\n",
    "pp_simple_tri = simple_trigram_model.calculate_perplexity(test_ngram)\n",
    "\n",
    "print(f\"\\nBigram Perplexity: {pp_bi:.2f}\")\n",
    "print(f\"Simple Bigram Perplexity: {pp_simple_bi:.2f}\")\n",
    "print(f\"Trigram Perplexity: {pp_tri:.2f}\")\n",
    "print(f\"Simple Trigram Perplexity: {pp_simple_tri:.2f}\")\n",
    "\n",
    "# Generation Comparison\n",
    "print(\"\\n--- Generation Comparison (Unprompted) ---\")\n",
    "print(\"Bigram:\")\n",
    "for i in range(2):\n",
    "    print(f\"  {i+1}. {bigram_model.generate_sentence()}\")\n",
    "print(\"Simple Bigram:\")\n",
    "for i in range(2):\n",
    "    print(f\"  {i+1}. {simple_bigram_model.generate_sentence()}\")\n",
    "\n",
    "print(\"\\nTrigram:\")\n",
    "for i in range(2):\n",
    "    print(f\"  {i+1}. {trigram_model.generate_sentence()}\")\n",
    "print(\"Simple Trigram:\")\n",
    "for i in range(2):\n",
    "    print(f\"  {i+1}. {simple_trigram_model.generate_sentence()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Comparison (LSTM vs N-grams)\n",
    "Comparing Logic, Training Time, and Perplexity across all three architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Neural Model on cuda...\n",
      "Building vocabulary...\n",
      "Vocabulary size: 79426\n",
      "Starting Training. \n",
      "Epoch 1: Loss 5.7169\n",
      "Epoch 2: Loss 4.8597\n",
      "Epoch 3: Loss 4.4907\n",
      "Epoch 4: Loss 4.2175\n",
      "Epoch 5: Loss 4.0066\n",
      "Epoch 6: Loss 3.8405\n",
      "Neural Trained in 2927.9262s\n",
      "Neural Perplexity: 180.02\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train Neural Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training Neural Model on {device}...\")\n",
    "\n",
    "# Config\n",
    "EMBED_DIM = 64\n",
    "HIDDEN_DIM = 512\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_EPOCHS = 6 \n",
    "\n",
    "# Setup\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(train_neural, min_freq=2) # => lowers perplexity and decreases train time\n",
    "dataset = IMDBDataset(train_neural, vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model_lstm = NeuralLM(len(vocab), EMBED_DIM, HIDDEN_DIM).to(device)\n",
    "optimizer = optim.Adam(model_lstm.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Training Loop\n",
    "start = time.time()\n",
    "model_lstm.train()\n",
    "loss_history = []\n",
    "\n",
    "print(f\"Starting Training. \")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for inputs, targets, lengths in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model_lstm(inputs, lengths)\n",
    "        loss = criterion(logits.view(-1, len(vocab)), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    navg = epoch_loss / len(dataloader)\n",
    "    loss_history.append(navg)\n",
    "    print(f\"Epoch {epoch+1}: Loss {navg:.4f}\")\n",
    "\n",
    "neural_time = time.time() - start\n",
    "print(f\"Neural Trained in {neural_time:.4f}s\")\n",
    "\n",
    "# Neural Perplexity (Approximate on Test Set)\n",
    "test_ds = IMDBDataset(test_neural, vocab)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "model_lstm.eval()\n",
    "total_n_loss = 0\n",
    "total_batches = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets, lengths in test_dl:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        logits, _ = model_lstm(inputs, lengths)\n",
    "        # Loss per batch\n",
    "        loss = criterion(logits.view(-1, len(vocab)), targets.view(-1))\n",
    "        total_n_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "avg_test_loss = total_n_loss / total_batches if total_batches > 0 else 999\n",
    "pp_neural = math.exp(avg_test_loss)\n",
    "print(f\"Neural Perplexity: {pp_neural:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Performance Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vogle\\AppData\\Local\\Temp\\ipykernel_38172\\999618731.py:8: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=models, y=times, ax=axes[0], palette=\"viridis\")\n",
      "C:\\Users\\vogle\\AppData\\Local\\Temp\\ipykernel_38172\\999618731.py:15: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=models, y=perplexities, ax=axes[1], palette=\"magma\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "models = ['Bigram', 'Simple Bi', 'Trigram', 'Simple Tri', 'LSTM']\n",
    "times = [bigram_time, simple_bigram_time, trigram_time, simple_trigram_time, neural_time]\n",
    "perplexities = [pp_bi, pp_simple_bi, pp_tri, pp_simple_tri, pp_neural]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time Plot\n",
    "sns.barplot(x=models, y=times, ax=axes[0], palette=\"viridis\")\n",
    "axes[0].set_title(\"Training Time (seconds)\")\n",
    "axes[0].set_ylabel(\"Seconds\")\n",
    "for i, v in enumerate(times):\n",
    "    axes[0].text(i, v + 0.1, f\"{v:.1f}s\", ha='center')\n",
    "\n",
    "# Perplexity Plot\n",
    "sns.barplot(x=models, y=perplexities, ax=axes[1], palette=\"magma\")\n",
    "axes[1].set_title(\"Test Perplexity (Lower is Better)\")\n",
    "axes[1].set_ylabel(\"Perplexity\")\n",
    "for i, v in enumerate(perplexities):\n",
    "    axes[1].text(i, v + 1, f\"{v:.1f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Qualitative Comparison: Text Generation\n",
    "Comparing outputs with and without prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Bigram</th>\n",
       "      <th>Simple Bi</th>\n",
       "      <th>Trigram</th>\n",
       "      <th>Simple Tri</th>\n",
       "      <th>LSTM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unprompted</td>\n",
       "      <td>a lord ha no exception film dip a scene our in the fascist most of movie i see unforgiven 2004</td>\n",
       "      <td>i wa the community theater i do not take the movie stylish womanizer sulky french play him beyond the set</td>\n",
       "      <td>this picture on the armed forced policy that are what they are alienated from the national film board video that</td>\n",
       "      <td>this is a brilliantly done action scene it is too bizarre to entertaining to me people this movie perhaps one</td>\n",
       "      <td>the man in the charge of schools . where to start down the drain readily got a miracle , the gooey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The movie was</td>\n",
       "      <td>The movie was just handing out how this sloppily edited i complain about him of hazing can not only manage or bungling up</td>\n",
       "      <td>The movie was so i really brings back story but my head at a new villa paranoia asserts that time you are a</td>\n",
       "      <td>The movie was right from the moment the raptor split up reunited just in respect to latter comment above i hesitated somewhat at</td>\n",
       "      <td>The movie was the clear lack of familiarity created by people who are replaced by these difficulty winning the rodeo footage some of</td>\n",
       "      <td>the movie was so lacking in how we got worried about what i will tell about the work of the theater . perhaps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought that</td>\n",
       "      <td>I thought that is a 16 and the cia s effort even if you should she really that appear twice the movie i</td>\n",
       "      <td>I thought that are geoffrey clifton kristin conner doe look for the love at one person who quite literally rolling stone is a</td>\n",
       "      <td>I thought that plan to embark upon a a bit too broad that s ok it is predictable it is that my boyfriend</td>\n",
       "      <td>I thought that it will always be a fan of zeta beta zeta the most thought provoking and a blast with the french</td>\n",
       "      <td>i thought that it was just a torrent . it was also funny because it failed in a bad feeling that the whole</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Prompt  \\\n",
       "0      Unprompted   \n",
       "1   The movie was   \n",
       "2  I thought that   \n",
       "\n",
       "                                                                                                                      Bigram  \\\n",
       "0                             a lord ha no exception film dip a scene our in the fascist most of movie i see unforgiven 2004   \n",
       "1  The movie was just handing out how this sloppily edited i complain about him of hazing can not only manage or bungling up   \n",
       "2                    I thought that is a 16 and the cia s effort even if you should she really that appear twice the movie i   \n",
       "\n",
       "                                                                                                                       Simple Bi  \\\n",
       "0                      i wa the community theater i do not take the movie stylish womanizer sulky french play him beyond the set   \n",
       "1                    The movie was so i really brings back story but my head at a new villa paranoia asserts that time you are a   \n",
       "2  I thought that are geoffrey clifton kristin conner doe look for the love at one person who quite literally rolling stone is a   \n",
       "\n",
       "                                                                                                                            Trigram  \\\n",
       "0                  this picture on the armed forced policy that are what they are alienated from the national film board video that   \n",
       "1  The movie was right from the moment the raptor split up reunited just in respect to latter comment above i hesitated somewhat at   \n",
       "2                          I thought that plan to embark upon a a bit too broad that s ok it is predictable it is that my boyfriend   \n",
       "\n",
       "                                                                                                                             Simple Tri  \\\n",
       "0                         this is a brilliantly done action scene it is too bizarre to entertaining to me people this movie perhaps one   \n",
       "1  The movie was the clear lack of familiarity created by people who are replaced by these difficulty winning the rodeo footage some of   \n",
       "2                       I thought that it will always be a fan of zeta beta zeta the most thought provoking and a blast with the french   \n",
       "\n",
       "                                                                                                            LSTM  \n",
       "0             the man in the charge of schools . where to start down the drain readily got a miracle , the gooey  \n",
       "1  the movie was so lacking in how we got worried about what i will tell about the work of the theater . perhaps  \n",
       "2     i thought that it was just a torrent . it was also funny because it failed in a bad feeling that the whole  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "prompts = [None, \"The movie was\", \"I thought that\"]\n",
    "\n",
    "def get_continuation(model_type, prompt):\n",
    "    if prompt is None:\n",
    "        if model_type == 'LSTM':\n",
    "            return generate_text(model_lstm, vocab, \"The\", device=device)\n",
    "        elif model_type == 'Bigram':\n",
    "            return bigram_model.generate_sentence()\n",
    "        elif model_type == 'Simple Bi':\n",
    "            return simple_bigram_model.generate_sentence()\n",
    "        elif model_type == 'Trigram':\n",
    "            return trigram_model.generate_sentence()\n",
    "        elif model_type == 'Simple Tri':\n",
    "            return simple_trigram_model.generate_sentence()\n",
    "    else:\n",
    "        # Prompted\n",
    "        if model_type == 'LSTM':\n",
    "            return generate_text(model_lstm, vocab, prompt, device=device)\n",
    "        elif model_type == 'Bigram':\n",
    "            return bigram_model.autocomplete(prompt, ngram_prep)\n",
    "        elif model_type == 'Simple Bi':\n",
    "            return simple_bigram_model.autocomplete(prompt, ngram_prep)\n",
    "        elif model_type == 'Trigram':\n",
    "            return trigram_model.autocomplete(prompt, ngram_prep)\n",
    "        elif model_type == 'Simple Tri':\n",
    "            return simple_trigram_model.autocomplete(prompt, ngram_prep)\n",
    "\n",
    "results = []\n",
    "for p in prompts:\n",
    "    p_label = p if p else \"Unprompted\"\n",
    "    row = {\"Prompt\": p_label}\n",
    "    row[\"Bigram\"] = get_continuation('Bigram', p)\n",
    "    row[\"Simple Bi\"] = get_continuation('Simple Bi', p)\n",
    "    row[\"Trigram\"] = get_continuation('Trigram', p)\n",
    "    row[\"Simple Tri\"] = get_continuation('Simple Tri', p)\n",
    "    row[\"LSTM\"] = get_continuation('LSTM', p)\n",
    "    results.append(row)\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(res_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
